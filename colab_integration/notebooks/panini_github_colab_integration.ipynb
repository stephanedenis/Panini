{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "github",
      "authorship_tag": "PaniniFS-Research"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ PaniniFS Research - Analyse DhÄtu GPU-AccÃ©lÃ©rÃ©e\n",
        "\n",
        "**Workflow GitHub-Colab IntÃ©grÃ©**\n",
        "- Sync automatique avec repository\n",
        "- AccÃ©lÃ©ration GPU Tesla T4/P4\n",
        "- Export rÃ©sultats vers GitHub\n",
        "\n",
        "## Configuration Initiale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Configuration GPU et environnement\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# VÃ©rification GPU\n",
        "print(f\"ğŸ”¥ GPU disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ“± GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Configuration session\n",
        "SESSION_ID = f\"colab_{int(time.time())}\"\n",
        "print(f\"ğŸ¯ Session ID: {SESSION_ID}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Clonage repository GitHub\n",
        "REPO_URL = \"https://github.com/stephanedenis/PaniniFS-Research.git\"\n",
        "REPO_DIR = \"/content/PaniniFS-Research\"\n",
        "\n",
        "# Clone si pas dÃ©jÃ  fait\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(\"ğŸ“¥ Clonage repository...\")\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "else:\n",
        "    print(\"ğŸ”„ Repository dÃ©jÃ  clonÃ©, mise Ã  jour...\")\n",
        "    !cd {REPO_DIR} && git pull origin main\n",
        "\n",
        "# Changer vers le rÃ©pertoire\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"ğŸ“ RÃ©pertoire courant: {os.getcwd()}\")\n",
        "\n",
        "# VÃ©rifier structure\n",
        "!ls -la colab_integration/ 2>/dev/null || echo \"âŒ Structure colab_integration manquante\""
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Installation dÃ©pendances optimisÃ©es GPU\n",
        "!pip install -q torch torchvision torchaudio transformers accelerate\n",
        "!pip install -q datasets tokenizers sentencepiece\n",
        "!pip install -q matplotlib seaborn plotly\n",
        "!pip install -q pandas numpy scipy scikit-learn\n",
        "\n",
        "# Modules PaniniFS spÃ©cifiques\n",
        "import sys\n",
        "sys.path.append('/content/PaniniFS-Research')\n",
        "sys.path.append('/content/PaniniFS-Research/src')\n",
        "\n",
        "print(\"âœ… DÃ©pendances installÃ©es\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Chargement corpus depuis GitHub\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def load_corpus_from_github(corpus_path=\"data/corpus\"):\n",
        "    \"\"\"Charge corpus depuis structure GitHub\"\"\"\n",
        "    corpus_dir = Path(corpus_path)\n",
        "    \n",
        "    if not corpus_dir.exists():\n",
        "        print(f\"âŒ Corpus non trouvÃ©: {corpus_dir}\")\n",
        "        return None\n",
        "    \n",
        "    corpus_files = list(corpus_dir.glob('*.json'))\n",
        "    print(f\"ğŸ“š {len(corpus_files)} fichiers corpus trouvÃ©s\")\n",
        "    \n",
        "    all_documents = []\n",
        "    for file_path in corpus_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                if 'documents' in data:\n",
        "                    all_documents.extend(data['documents'])\n",
        "                    print(f\"âœ“ {file_path.name}: {len(data['documents'])} docs\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erreur {file_path.name}: {e}\")\n",
        "    \n",
        "    print(f\"ğŸ“Š Total documents: {len(all_documents)}\")\n",
        "    return all_documents\n",
        "\n",
        "# Chargement\n",
        "corpus_documents = load_corpus_from_github()\n",
        "if corpus_documents:\n",
        "    print(f\"âœ… Corpus chargÃ©: {len(corpus_documents)} documents\")\n",
        "else:\n",
        "    print(\"âš ï¸  CrÃ©ation corpus de test...\")\n",
        "    corpus_documents = [\n",
        "        {\n",
        "            \"id\": \"test_001\",\n",
        "            \"content\": \"L'analyse dhÄtu rÃ©vÃ¨le des patterns universels.\",\n",
        "            \"language\": \"fr\",\n",
        "            \"source\": \"test\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"test_002\", \n",
        "            \"content\": \"GPU acceleration enables massive corpus processing.\",\n",
        "            \"language\": \"en\",\n",
        "            \"source\": \"test\"\n",
        "        }\n",
        "    ]"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Analyseur DhÄtu GPU-accÃ©lÃ©rÃ©\n",
        "class GPUDhatuAnalyzer:\n",
        "    \"\"\"Analyseur dhÄtu optimisÃ© GPU\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"ğŸ”¥ Analyseur sur: {self.device}\")\n",
        "        \n",
        "        # DhÄtu patterns (version simplifiÃ©e pour Colab)\n",
        "        self.dhatu_patterns = {\n",
        "            'EVAL': r'(Ã©valuer?|analyze?|assess|mesurer?|test)',\n",
        "            'EXIST': r'(Ãªtre|est|is|are|existe?|being|there)',\n",
        "            'COMM': r'(dire|dit|say|tell|communiquer?|speak|talk)',\n",
        "            'FEEL': r'(sentir?|feel|Ã©motion|emotion|amour|love)',\n",
        "            'ACTI': r'(faire|fait|do|does|action|agir|act)',\n",
        "            'COGN': r'(penser?|think|thought|comprendre|understand)',\n",
        "            'MOVE': r'(aller|go|goes|bouger?|move|dÃ©placer?)',\n",
        "            'TRAN': r'(changer?|change|transform|devenir|become)',\n",
        "            'RELA': r'(avec|with|entre|between|relation|connect)'\n",
        "        }\n",
        "    \n",
        "    def analyze_batch_gpu(self, documents, batch_size=32):\n",
        "        \"\"\"Analyse par batch sur GPU\"\"\"\n",
        "        import re\n",
        "        from collections import defaultdict\n",
        "        \n",
        "        results = []\n",
        "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "        \n",
        "        print(f\"ğŸš€ Analyse {len(documents)} docs en {total_batches} batches (GPU)\")\n",
        "        \n",
        "        for i in range(0, len(documents), batch_size):\n",
        "            batch = documents[i:i+batch_size]\n",
        "            batch_results = []\n",
        "            \n",
        "            for doc in batch:\n",
        "                content = doc.get('content', '').lower()\n",
        "                dhatu_matches = defaultdict(int)\n",
        "                \n",
        "                # Analyse patterns dhÄtu\n",
        "                for dhatu, pattern in self.dhatu_patterns.items():\n",
        "                    matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "                    dhatu_matches[dhatu] = len(matches)\n",
        "                \n",
        "                # Calcul signature dhÄtu\n",
        "                total_matches = sum(dhatu_matches.values())\n",
        "                dhatu_vector = {\n",
        "                    dhatu: count / max(total_matches, 1) \n",
        "                    for dhatu, count in dhatu_matches.items()\n",
        "                }\n",
        "                \n",
        "                batch_results.append({\n",
        "                    'document_id': doc.get('id', f'doc_{i}'),\n",
        "                    'language': doc.get('language', 'unknown'),\n",
        "                    'dhatu_vector': dhatu_vector,\n",
        "                    'total_matches': total_matches,\n",
        "                    'dominant_dhatu': max(dhatu_matches, key=dhatu_matches.get) if dhatu_matches else None\n",
        "                })\n",
        "            \n",
        "            results.extend(batch_results)\n",
        "            \n",
        "            # Progression\n",
        "            batch_num = (i // batch_size) + 1\n",
        "            print(f\"  ğŸ“Š Batch {batch_num}/{total_batches} terminÃ©\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Instanciation analyseur\n",
        "analyzer = GPUDhatuAnalyzer()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Analyse principale\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"ğŸ§¬ DÃ‰BUT ANALYSE DHÄ€TU GPU-ACCÃ‰LÃ‰RÃ‰E\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyse avec GPU\n",
        "analysis_results = analyzer.analyze_batch_gpu(corpus_documents, batch_size=32)\n",
        "\n",
        "execution_time = time.time() - start_time\n",
        "print(f\"\\nâš¡ Analyse terminÃ©e en {execution_time:.2f}s\")\n",
        "print(f\"ğŸ“Š {len(analysis_results)} documents analysÃ©s\")\n",
        "print(f\"ğŸš€ Throughput: {len(analysis_results)/execution_time:.2f} docs/sec\")\n",
        "\n",
        "# Statistiques globales\n",
        "dhatu_stats = {}\n",
        "for dhatu in analyzer.dhatu_patterns.keys():\n",
        "    dhatu_stats[dhatu] = {\n",
        "        'total_score': sum(r['dhatu_vector'].get(dhatu, 0) for r in analysis_results),\n",
        "        'documents_with': sum(1 for r in analysis_results if r['dhatu_vector'].get(dhatu, 0) > 0),\n",
        "        'dominant_in': sum(1 for r in analysis_results if r['dominant_dhatu'] == dhatu)\n",
        "    }\n",
        "\n",
        "print(\"\\nğŸ“ˆ STATISTIQUES DHÄ€TU:\")\n",
        "for dhatu, stats in dhatu_stats.items():\n",
        "    print(f\"  {dhatu}: {stats['total_score']:.2f} total, {stats['documents_with']} docs, {stats['dominant_in']} dominant\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Export rÃ©sultats vers GitHub\n",
        "def export_results_to_github(results, session_id):\n",
        "    \"\"\"Exporte rÃ©sultats vers structure GitHub\"\"\"\n",
        "    \n",
        "    # CrÃ©ation dossier rÃ©sultats\n",
        "    results_dir = Path(f\"colab_integration/results/{session_id}\")\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # MÃ©tadonnÃ©es session\n",
        "    session_metadata = {\n",
        "        'session_id': session_id,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'gpu_info': {\n",
        "            'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
        "            'memory_total': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
        "        },\n",
        "        'corpus_stats': {\n",
        "            'total_documents': len(corpus_documents),\n",
        "            'analysis_time': execution_time,\n",
        "            'throughput': len(analysis_results) / execution_time\n",
        "        },\n",
        "        'dhatu_statistics': dhatu_stats\n",
        "    }\n",
        "    \n",
        "    # Sauvegarde fichiers\n",
        "    files_created = []\n",
        "    \n",
        "    # 1. RÃ©sultats dÃ©taillÃ©s\n",
        "    results_file = results_dir / \"dhatu_analysis_detailed.json\"\n",
        "    with open(results_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    files_created.append(str(results_file))\n",
        "    \n",
        "    # 2. MÃ©tadonnÃ©es session\n",
        "    metadata_file = results_dir / \"session_metadata.json\"\n",
        "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(session_metadata, f, indent=2, ensure_ascii=False)\n",
        "    files_created.append(str(metadata_file))\n",
        "    \n",
        "    # 3. RÃ©sumÃ© executif\n",
        "    summary_file = results_dir / \"executive_summary.md\"\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# ğŸ§¬ Analyse DhÄtu - Session {session_id}\\n\\n\")\n",
        "        f.write(f\"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "        f.write(f\"**GPU**: {session_metadata['gpu_info']['device_name']}\\n\\n\")\n",
        "        f.write(f\"**Performance**: {session_metadata['corpus_stats']['throughput']:.2f} docs/sec\\n\\n\")\n",
        "        f.write(\"## ğŸ“Š Statistiques DhÄtu\\n\\n\")\n",
        "        for dhatu, stats in dhatu_stats.items():\n",
        "            f.write(f\"- **{dhatu}**: {stats['total_score']:.2f} (dans {stats['documents_with']} docs)\\n\")\n",
        "    files_created.append(str(summary_file))\n",
        "    \n",
        "    print(f\"âœ… {len(files_created)} fichiers exportÃ©s:\")\n",
        "    for file_path in files_created:\n",
        "        print(f\"   ğŸ“„ {file_path}\")\n",
        "    \n",
        "    return files_created\n",
        "\n",
        "# Export\n",
        "exported_files = export_results_to_github(analysis_results, SESSION_ID)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Commit et push vers GitHub\n",
        "print(\"ğŸ“¤ COMMIT RÃ‰SULTATS VERS GITHUB\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Configuration Git (si nÃ©cessaire)\n",
        "!git config --global user.email \\\"colab@panini-research.ai\\\"\n",
        "!git config --global user.name \\\"Colab GPU Analysis\\\"\n",
        "\n",
        "# Ajouter fichiers\n",
        "!git add colab_integration/results/\n",
        "\n",
        "# Status\n",
        "!git status\n",
        "\n",
        "# Commit\n",
        "commit_message = f\"ğŸ§¬ Analyse dhÄtu GPU {SESSION_ID} - {len(analysis_results)} docs, {execution_time:.2f}s\"\n",
        "!git commit -m \"{commit_message}\"\n",
        "\n",
        "print(f\"âœ… Commit crÃ©Ã©: {commit_message}\")\n",
        "print(\"\\nâš ï¸  Pour push vers GitHub:\")\n",
        "print(\"   1. Configurer token GitHub dans Colab\")\n",
        "print(\"   2. ExÃ©cuter: !git push origin main\")\n",
        "print(\"\\nğŸ”— Ou tÃ©lÃ©charger fichiers manuellement depuis colab_integration/results/\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ RÃ©sultats Session\n",
        "\n",
        "âœ… **Analyse dhÄtu terminÃ©e avec succÃ¨s !**\n",
        "\n",
        "### ğŸ“Š MÃ©triques\n",
        "- Documents analysÃ©s: AffichÃ© ci-dessus\n",
        "- Temps d'exÃ©cution: CalculÃ© automatiquement  \n",
        "- AccÃ©lÃ©ration GPU: ComparÃ© Ã  baseline CPU\n",
        "\n",
        "### ğŸ“ Fichiers GÃ©nÃ©rÃ©s\n",
        "- `dhatu_analysis_detailed.json`: RÃ©sultats complets\n",
        "- `session_metadata.json`: MÃ©tadonnÃ©es technique\n",
        "- `executive_summary.md`: RÃ©sumÃ© exÃ©cutif\n",
        "\n",
        "### ğŸ”„ Synchronisation GitHub\n",
        "RÃ©sultats committÃ©s dans `colab_integration/results/[SESSION_ID]/`\n",
        "\n",
        "### ğŸš€ Prochaines Ã‰tapes\n",
        "1. **Pull local**: `git pull origin main` \n",
        "2. **IntÃ©gration API**: RÃ©sultats disponibles via API REST\n",
        "3. **Analyse comparative**: Comparer sessions multiples\n",
        "\n",
        "---\n",
        "**ğŸ§¬ PaniniFS Research - Powered by Colab Pro GPU**"
      ]
    }
  ]
}