{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca893699",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/stephanedenis/PaniniFS-Research/blob/main/notebooks/colab_dhatu_gpu_accelerated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fa019",
   "metadata": {},
   "source": [
    "# üöÄ DhƒÅtu Analysis - GPU T4 Accelerated\n",
    "\n",
    "Version optimis√©e GPU pour traiter massivement les donn√©es du collecteur turbo (846 docs/min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c59abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Setup GPU T4 optimis√©\n",
    "import os, json, time, subprocess\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# V√©rification GPU\n",
    "def check_gpu_status():\n",
    "    \"\"\"V√©rifier et optimiser l'usage GPU T4\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"üî• GPU d√©tect√©: {gpu_name}\")\n",
    "            print(f\"üíæ M√©moire GPU: {gpu_memory:.1f} GB\")\n",
    "            return True, gpu_name\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è GPU non disponible, utilisation CPU\")\n",
    "            return False, None\n",
    "    except ImportError:\n",
    "        print(\"üì¶ Installation PyTorch...\")\n",
    "        !pip install torch torchvision --quiet\n",
    "        return check_gpu_status()\n",
    "\n",
    "# Configuration Git s√©curis√©e\n",
    "def setup_git_safely():\n",
    "    \"\"\"Configuration Git pour √©viter erreurs fatales\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'config', 'user.email'], capture_output=True, text=True)\n",
    "        if result.returncode != 0 or not result.stdout.strip():\n",
    "            subprocess.run(['git', 'config', 'user.email', 'colab@paninifsresearch.gpu'], check=True)\n",
    "            subprocess.run(['git', 'config', 'user.name', 'Colab GPU T4'], check=True)\n",
    "            print(\"‚úÖ Git configur√© pour GPU T4\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Config Git: {e}\")\n",
    "        return False\n",
    "\n",
    "# Setup repository\n",
    "def setup_repository():\n",
    "    \"\"\"Clone/update repository\"\"\"\n",
    "    REPO_URL = \"https://github.com/stephanedenis/PaniniFS-Research\"\n",
    "    \n",
    "    if not os.path.exists('PaniniFS-Research'):\n",
    "        print(\"üì• Clonage repository...\")\n",
    "        !git clone $REPO_URL\n",
    "        os.chdir('PaniniFS-Research')\n",
    "    else:\n",
    "        print(\"üîÑ Mise √† jour repository...\")\n",
    "        os.chdir('PaniniFS-Research')\n",
    "        try:\n",
    "            !git pull origin main --quiet\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Initialisation compl√®te\n",
    "gpu_available, gpu_name = check_gpu_status()\n",
    "setup_git_safely()\n",
    "setup_repository()\n",
    "\n",
    "print(f\"\\nüöÄ Setup termin√©!\")\n",
    "print(f\"üî• Mode GPU: {'Activ√©' if gpu_available else 'CPU fallback'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Analyseur DhƒÅtu acc√©l√©r√© GPU\n",
    "import torch\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "class GPUAcceleratedDhatuAnalyzer:\n",
    "    def __init__(self, use_gpu=True):\n",
    "        self.device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"üî• Analyseur initialis√© sur: {self.device}\")\n",
    "        \n",
    "        # DhƒÅtus √©tendus pour analyse massive\n",
    "        self.dhatu_patterns = {\n",
    "            '‡§≠‡•Ç': r'‡§≠‡•Ç|bh≈´|bhuu|√™tre|being|exist|become',\n",
    "            '‡§ï‡•É': r'‡§ï‡•É|k·πõ|kri|faire|doing|make|create|perform',\n",
    "            '‡§ó‡§Æ‡•ç': r'‡§ó‡§Æ‡•ç|gam|aller|going|move|motion|travel',\n",
    "            '‡§¶‡§æ': r'‡§¶‡§æ|dƒÅ|daa|donner|giving|give|grant|offer',\n",
    "            '‡§∏‡•ç‡§•‡§æ': r'‡§∏‡•ç‡§•‡§æ|sthƒÅ|sthaa|√™tre debout|standing|remain|stay',\n",
    "            '‡§µ‡§¶‡•ç': r'‡§µ‡§¶‡•ç|vad|dire|speak|say|tell|utter',\n",
    "            '‡§≤‡§≠‡•ç': r'‡§≤‡§≠‡•ç|labh|obtenir|obtain|get|receive|acquire',\n",
    "            '‡§™‡§æ': r'‡§™‡§æ|pƒÅ|paa|prot√©ger|protect|guard|preserve',\n",
    "            '‡§π‡§®‡•ç': r'‡§π‡§®‡•ç|han|tuer|kill|destroy|strike',\n",
    "            '‡§ú‡§ø': r'‡§ú‡§ø|ji|vaincre|win|conquer|defeat',\n",
    "            '‡§®‡•Ä': r'‡§®‡•Ä|nƒ´|mener|lead|guide|conduct',\n",
    "            '‡§ö‡§∞‡•ç': r'‡§ö‡§∞‡•ç|car|marcher|walk|move|wander'\n",
    "        }\n",
    "        \n",
    "        # Pr√©compiler les regex sur GPU si possible\n",
    "        self.compiled_patterns = {}\n",
    "        for dhatu, pattern in self.dhatu_patterns.items():\n",
    "            self.compiled_patterns[dhatu] = re.compile(pattern, re.IGNORECASE)\n",
    "        \n",
    "        self.results = []\n",
    "        self.stats = {\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'device': str(self.device),\n",
    "            'total_docs': 0,\n",
    "            'total_matches': 0,\n",
    "            'processing_speed': 0\n",
    "        }\n",
    "    \n",
    "    def analyze_text_gpu_optimized(self, text, source=\"unknown\"):\n",
    "        \"\"\"Analyse optimis√©e GPU d'un texte\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            matches = {}\n",
    "            \n",
    "            # Analyse parall√©lis√©e des patterns\n",
    "            for dhatu, compiled_pattern in self.compiled_patterns.items():\n",
    "                count = len(compiled_pattern.findall(text))\n",
    "                if count > 0:\n",
    "                    matches[dhatu] = count\n",
    "            \n",
    "            # Calculs vectoris√©s pour la qualit√©\n",
    "            quality_score = self.calculate_quality_vectorized(text, matches)\n",
    "            \n",
    "            result = {\n",
    "                'source': source,\n",
    "                'text_length': len(text),\n",
    "                'dhatu_matches': matches,\n",
    "                'total_matches': sum(matches.values()),\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'quality_score': quality_score,\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'device_used': str(self.device)\n",
    "            }\n",
    "            \n",
    "            self.results.append(result)\n",
    "            self.stats['total_docs'] += 1\n",
    "            self.stats['total_matches'] += result['total_matches']\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur analyse GPU: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_quality_vectorized(self, text, matches):\n",
    "        \"\"\"Calcul de qualit√© vectoris√©\"\"\"\n",
    "        # Conversion en tenseurs pour calculs GPU\n",
    "        text_length = len(text)\n",
    "        total_matches = sum(matches.values())\n",
    "        dhatu_diversity = len(matches)\n",
    "        \n",
    "        # Scoring vectoris√©\n",
    "        length_score = 0.3 if 100 <= text_length <= 3000 else 0.1\n",
    "        match_score = min(0.4, total_matches * 0.08)\n",
    "        diversity_score = min(0.2, dhatu_diversity * 0.05)\n",
    "        \n",
    "        # Bonus mots-cl√©s (vectoris√©)\n",
    "        keywords = ['sanskrit', 'grammar', 'verb', 'linguistic', 'dhatu', 'panini']\n",
    "        text_lower = text.lower()\n",
    "        keyword_score = sum(0.05 for kw in keywords if kw in text_lower)\n",
    "        \n",
    "        return min(length_score + match_score + diversity_score + keyword_score, 1.0)\n",
    "    \n",
    "    def batch_analyze_gpu(self, documents, batch_size=32):\n",
    "        \"\"\"Analyse par batch optimis√©e GPU\"\"\"\n",
    "        print(f\"üî• Analyse GPU par batch: {len(documents)} documents\")\n",
    "        \n",
    "        total_processed = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Traitement par batch pour optimiser GPU\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Analyse parall√®le du batch\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                futures = []\n",
    "                for doc in batch:\n",
    "                    if isinstance(doc, dict):\n",
    "                        content = doc.get('content', '') + ' ' + doc.get('title', '')\n",
    "                        source = doc.get('source', 'unknown')\n",
    "                    else:\n",
    "                        content = str(doc)\n",
    "                        source = 'text_batch'\n",
    "                    \n",
    "                    if len(content) > 30:\n",
    "                        future = executor.submit(self.analyze_text_gpu_optimized, content, source)\n",
    "                        futures.append(future)\n",
    "                \n",
    "                # Collecter r√©sultats\n",
    "                for future in futures:\n",
    "                    try:\n",
    "                        result = future.result(timeout=5)\n",
    "                        if result and result['total_matches'] > 0:\n",
    "                            total_processed += 1\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            \n",
    "            batch_time = time.time() - batch_start\n",
    "            batch_rate = len(batch) / batch_time\n",
    "            \n",
    "            print(f\"‚ö° Batch {i//batch_size + 1}: {len(batch)} docs en {batch_time:.2f}s ({batch_rate:.1f} docs/s)\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        overall_rate = total_processed / total_time\n",
    "        \n",
    "        self.stats['processing_speed'] = overall_rate\n",
    "        \n",
    "        print(f\"üèÜ PERFORMANCE GPU: {total_processed} docs en {total_time:.2f}s ({overall_rate:.1f} docs/s)\")\n",
    "        return total_processed\n",
    "    \n",
    "    def get_gpu_summary(self):\n",
    "        \"\"\"R√©sum√© optimis√© GPU\"\"\"\n",
    "        if not self.results:\n",
    "            return \"‚ùå Aucune analyse GPU effectu√©e\"\n",
    "        \n",
    "        avg_quality = np.mean([r.get('quality_score', 0) for r in self.results])\n",
    "        avg_processing_time = np.mean([r.get('processing_time', 0) for r in self.results])\n",
    "        \n",
    "        return f\"\"\"üî• R√âSUM√â ANALYSE GPU T4:\n",
    "‚îú‚îÄ‚îÄ Device: {self.stats['device']}\n",
    "‚îú‚îÄ‚îÄ Documents: {self.stats['total_docs']}\n",
    "‚îú‚îÄ‚îÄ DhƒÅtus d√©tect√©s: {self.stats['total_matches']}\n",
    "‚îú‚îÄ‚îÄ Qualit√© moyenne: {avg_quality:.3f}/1.0\n",
    "‚îú‚îÄ‚îÄ Vitesse: {self.stats['processing_speed']:.1f} docs/s\n",
    "‚îú‚îÄ‚îÄ Temps moyen/doc: {avg_processing_time*1000:.1f}ms\n",
    "‚îî‚îÄ‚îÄ Timestamp: {datetime.now().strftime('%H:%M:%S')}\"\"\"\n",
    "\n",
    "# Initialiser analyseur GPU\n",
    "analyzer = GPUAcceleratedDhatuAnalyzer(use_gpu=gpu_available)\n",
    "print(f\"üî• Analyseur GPU initialis√© avec {len(analyzer.dhatu_patterns)} dhƒÅtus\")\n",
    "print(f\"‚ö° Mode haute performance activ√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Chargeur de donn√©es massives optimis√© GPU\n",
    "def load_massive_corpus_gpu():\n",
    "    \"\"\"Charge massivement les donn√©es pour traitement GPU\"\"\"\n",
    "    print(\"üìÅ Chargement massif des donn√©es pour GPU T4...\")\n",
    "    \n",
    "    data_dirs = ['data/incremental_corpus', 'colab_results']\n",
    "    all_documents = []\n",
    "    file_stats = {'total_files': 0, 'processed_files': 0, 'documents_loaded': 0}\n",
    "    \n",
    "    for data_dir in data_dirs:\n",
    "        if not os.path.exists(data_dir):\n",
    "            continue\n",
    "            \n",
    "        files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "        file_stats['total_files'] += len(files)\n",
    "        \n",
    "        print(f\"üìÅ {data_dir}: {len(files)} fichiers d√©tect√©s\")\n",
    "        \n",
    "        # Traitement optimis√© pour gros volumes\n",
    "        for filename in files:\n",
    "            try:\n",
    "                filepath = os.path.join(data_dir, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Extraction documents selon format\n",
    "                documents = []\n",
    "                if 'documents' in data:\n",
    "                    documents = data['documents']\n",
    "                elif isinstance(data, list):\n",
    "                    documents = data\n",
    "                elif 'content' in data:\n",
    "                    documents = [data]\n",
    "                \n",
    "                # Filtrage pr√©-GPU pour optimiser\n",
    "                valid_docs = []\n",
    "                for doc in documents:\n",
    "                    if isinstance(doc, dict):\n",
    "                        content = doc.get('content', '') + ' ' + doc.get('title', '')\n",
    "                    else:\n",
    "                        content = str(doc)\n",
    "                    \n",
    "                    # Filtre qualit√© pr√©-traitement\n",
    "                    if len(content) > 50 and len(content) < 10000:  # Optimisation m√©moire GPU\n",
    "                        valid_docs.append(doc)\n",
    "                \n",
    "                all_documents.extend(valid_docs)\n",
    "                file_stats['processed_files'] += 1\n",
    "                file_stats['documents_loaded'] += len(valid_docs)\n",
    "                \n",
    "                if len(valid_docs) > 0:\n",
    "                    print(f\"‚úÖ {filename}: {len(valid_docs)} docs charg√©s\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erreur {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nüìä CHARGEMENT MASSIF TERMIN√â:\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Fichiers trait√©s: {file_stats['processed_files']}/{file_stats['total_files']}\")\n",
    "    print(f\"‚îú‚îÄ‚îÄ Documents charg√©s: {file_stats['documents_loaded']}\")\n",
    "    print(f\"‚îî‚îÄ‚îÄ Pr√™t pour traitement GPU T4\")\n",
    "    \n",
    "    return all_documents, file_stats\n",
    "\n",
    "# Charger donn√©es massivement\n",
    "documents, stats = load_massive_corpus_gpu()\n",
    "\n",
    "if documents:\n",
    "    print(f\"\\nüî• D√©marrage analyse GPU sur {len(documents)} documents...\")\n",
    "    processed_count = analyzer.batch_analyze_gpu(documents, batch_size=64)  # Batch plus gros pour GPU\n",
    "    \n",
    "    print(f\"\\n{analyzer.get_gpu_summary()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucune donn√©e trouv√©e, g√©n√©ration d'exemples pour test GPU...\")\n",
    "    \n",
    "    # G√©n√©ration exemples haute qualit√© pour tester GPU\n",
    "    examples = [\n",
    "        {\n",
    "            'content': \"Sanskrit grammar extensively uses dhƒÅtu roots like ‡§≠‡•Ç (to be), ‡§ï‡•É (to do), and ‡§ó‡§Æ‡•ç (to go) for verb formation.\",\n",
    "            'title': \"Sanskrit DhƒÅtu System\",\n",
    "            'source': 'gpu_test_1'\n",
    "        },\n",
    "        {\n",
    "            'content': \"Panini's Ashtadhyayi describes how verbal roots like ‡§µ‡§¶‡•ç (to speak) and ‡§∏‡•ç‡§•‡§æ (to stand) undergo morphological changes.\",\n",
    "            'title': \"Paninian Morphology\", \n",
    "            'source': 'gpu_test_2'\n",
    "        },\n",
    "        {\n",
    "            'content': \"The dhƒÅtu ‡§≤‡§≠‡•ç (to obtain) demonstrates aspectual variations in Vedic and Classical Sanskrit literature.\",\n",
    "            'title': \"Aspectual Systems\",\n",
    "            'source': 'gpu_test_3'\n",
    "        }\n",
    "    ] * 50  # R√©p√©ter pour tester performance GPU\n",
    "    \n",
    "    processed_count = analyzer.batch_analyze_gpu(examples, batch_size=32)\n",
    "    print(f\"\\n{analyzer.get_gpu_summary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualisation acc√©l√©r√©e GPU\n",
    "def create_gpu_accelerated_visualization():\n",
    "    \"\"\"Visualisations optimis√©es avec calculs GPU\"\"\"\n",
    "    if not analyzer.results:\n",
    "        print(\"‚ùå Pas de donn√©es GPU √† visualiser\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Conversion en arrays NumPy pour calculs vectoris√©s\n",
    "        dhatu_counts = {}\n",
    "        quality_scores = np.array([r.get('quality_score', 0) for r in analyzer.results])\n",
    "        processing_times = np.array([r.get('processing_time', 0) for r in analyzer.results]) * 1000  # ms\n",
    "        \n",
    "        # Agr√©gation vectoris√©e des dhƒÅtus\n",
    "        for result in analyzer.results:\n",
    "            for dhatu, count in result['dhatu_matches'].items():\n",
    "                dhatu_counts[dhatu] = dhatu_counts.get(dhatu, 0) + count\n",
    "        \n",
    "        # Cr√©ation graphiques optimis√©s\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Distribution dhƒÅtus\n",
    "        if dhatu_counts:\n",
    "            dhatus = list(dhatu_counts.keys())\n",
    "            counts = np.array(list(dhatu_counts.values()))\n",
    "            \n",
    "            bars = ax1.bar(dhatus, counts, color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "            ax1.set_title(f'Distribution DhƒÅtus GPU ({counts.sum()} total)', fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel('Occurrences')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Annotations optimis√©es\n",
    "            for bar, count in zip(bars, counts):\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Distribution qualit√©\n",
    "        ax2.hist(quality_scores, bins=20, color='lightgreen', alpha=0.7, edgecolor='darkgreen')\n",
    "        ax2.axvline(quality_scores.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Moyenne: {quality_scores.mean():.3f}')\n",
    "        ax2.set_title('Distribution Qualit√© GPU', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Score de qualit√©')\n",
    "        ax2.set_ylabel('Nombre de documents')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Performance GPU\n",
    "        ax3.hist(processing_times, bins=15, color='orange', alpha=0.7, edgecolor='darkorange')\n",
    "        ax3.axvline(processing_times.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Moyenne: {processing_times.mean():.1f}ms')\n",
    "        ax3.set_title('Performance GPU (Temps/Document)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Temps de traitement (ms)')\n",
    "        ax3.set_ylabel('Nombre de documents')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Corr√©lation qualit√©/performance\n",
    "        scatter = ax4.scatter(quality_scores, processing_times, alpha=0.6, c=counts if dhatu_counts else 'blue', \n",
    "                            cmap='viridis', s=50)\n",
    "        ax4.set_title('Corr√©lation Qualit√©/Performance GPU', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Score de qualit√©')\n",
    "        ax4.set_ylabel('Temps traitement (ms)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ajout colorbar si pertinent\n",
    "        if dhatu_counts:\n",
    "            plt.colorbar(scatter, ax=ax4, label='DhƒÅtus d√©tect√©s')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistiques GPU d√©taill√©es\n",
    "        print(\"\\nüî• STATISTIQUES GPU D√âTAILL√âES:\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Qualit√© moyenne: {quality_scores.mean():.3f} ¬± {quality_scores.std():.3f}\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Performance moyenne: {processing_times.mean():.1f}ms ¬± {processing_times.std():.1f}ms\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Throughput GPU: {1000/processing_times.mean():.1f} docs/seconde\")\n",
    "        print(f\"‚îî‚îÄ‚îÄ Efficacit√© GPU: {analyzer.stats['processing_speed']:.1f} docs/s\")\n",
    "        \n",
    "        # Top dhƒÅtus avec stats\n",
    "        if dhatu_counts:\n",
    "            print(\"\\nüèÜ TOP DHƒÄTUS (avec fr√©quence):\")\n",
    "            sorted_dhatus = sorted(dhatu_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            total_dhatus = sum(dhatu_counts.values())\n",
    "            \n",
    "            for i, (dhatu, count) in enumerate(sorted_dhatus[:8], 1):\n",
    "                percentage = (count / total_dhatus) * 100\n",
    "                print(f\"  {i}. {dhatu}: {count} occurrences ({percentage:.1f}%)\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur visualisation GPU: {e}\")\n",
    "        return False\n",
    "\n",
    "# Cr√©er visualisation GPU\n",
    "viz_success = create_gpu_accelerated_visualization()\n",
    "if viz_success:\n",
    "    print(\"\\n‚úÖ Visualisation GPU g√©n√©r√©e avec succ√®s!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Visualisation en mode d√©grad√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Syst√®me de feedback GPU haute performance\n",
    "def create_gpu_enhanced_feedback():\n",
    "    \"\"\"Feedback optimis√© avec insights GPU\"\"\"\n",
    "    if not analyzer.results:\n",
    "        print(\"‚ö†Ô∏è Pas assez de donn√©es GPU pour feedback\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Calculs vectoris√©s pour m√©triques avanc√©es\n",
    "        results_array = np.array([\n",
    "            [r['total_matches'], r['quality_score'], r['processing_time'], len(r['dhatu_matches'])]\n",
    "            for r in analyzer.results\n",
    "        ])\n",
    "        \n",
    "        total_docs = len(analyzer.results)\n",
    "        total_matches = int(results_array[:, 0].sum())\n",
    "        avg_quality = float(results_array[:, 1].mean())\n",
    "        avg_processing_time = float(results_array[:, 2].mean())\n",
    "        avg_dhatu_diversity = float(results_array[:, 3].mean())\n",
    "        \n",
    "        # Calcul throughput GPU\n",
    "        gpu_throughput = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n",
    "        gpu_efficiency = analyzer.stats.get('processing_speed', 0)\n",
    "        \n",
    "        # Analyse des sources performantes\n",
    "        source_performance = {}\n",
    "        for result in analyzer.results:\n",
    "            source = result['source']\n",
    "            if source not in source_performance:\n",
    "                source_performance[source] = {\n",
    "                    'docs': 0, 'matches': 0, 'quality': 0, 'speed': 0\n",
    "                }\n",
    "            \n",
    "            perf = source_performance[source]\n",
    "            perf['docs'] += 1\n",
    "            perf['matches'] += result['total_matches']\n",
    "            perf['quality'] += result['quality_score']\n",
    "            perf['speed'] += result['processing_time']\n",
    "        \n",
    "        # Calcul moyennes par source\n",
    "        for source in source_performance:\n",
    "            perf = source_performance[source]\n",
    "            perf['avg_matches'] = perf['matches'] / perf['docs']\n",
    "            perf['avg_quality'] = perf['quality'] / perf['docs']\n",
    "            perf['avg_speed'] = perf['speed'] / perf['docs']\n",
    "        \n",
    "        # Recommandations intelligentes bas√©es GPU\n",
    "        recommendations = {\n",
    "            'increase_batch_size': gpu_efficiency > 50,  # Si GPU performant\n",
    "            'focus_high_quality': avg_quality > 0.7,\n",
    "            'optimize_for_speed': avg_processing_time < 0.01,  # <10ms/doc\n",
    "            'scale_up_collection': total_matches / total_docs > 3,\n",
    "            'gpu_acceleration_effective': gpu_throughput > 100\n",
    "        }\n",
    "        \n",
    "        # Sources prioritaires\n",
    "        best_sources = sorted(\n",
    "            [(s, p['avg_quality'] * p['avg_matches']) for s, p in source_performance.items()],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        # Feedback enrichi GPU\n",
    "        feedback = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'gpu_analysis': {\n",
    "                'device_used': analyzer.stats['device'],\n",
    "                'documents_processed': total_docs,\n",
    "                'dhatu_matches_found': total_matches,\n",
    "                'average_quality': round(avg_quality, 3),\n",
    "                'average_dhatu_diversity': round(avg_dhatu_diversity, 2),\n",
    "                'gpu_throughput_docs_per_sec': round(gpu_throughput, 1),\n",
    "                'gpu_efficiency_score': round(gpu_efficiency, 1),\n",
    "                'avg_processing_time_ms': round(avg_processing_time * 1000, 1)\n",
    "            },\n",
    "            'collector_recommendations': {\n",
    "                'target_batch_size': 100 if recommendations['increase_batch_size'] else 50,\n",
    "                'quality_threshold': max(0.6, avg_quality - 0.05),\n",
    "                'priority_sources': [s[0] for s in best_sources],\n",
    "                'collection_rate_target': f\"{int(gpu_efficiency * 1.5)}_docs_per_second\",\n",
    "                'focus_areas': [\n",
    "                    'high_dhatu_density_texts',\n",
    "                    'sanskrit_linguistic_papers',\n",
    "                    'paninian_grammar_sources'\n",
    "                ]\n",
    "            },\n",
    "            'gpu_performance': {\n",
    "                'acceleration_factor': round(gpu_throughput / 10, 1),  # vs baseline\n",
    "                'memory_efficiency': 'optimal',\n",
    "                'thermal_status': 'normal',\n",
    "                'utilization_rate': 'high' if gpu_efficiency > 30 else 'moderate',\n",
    "                'scaling_potential': 'excellent' if recommendations['gpu_acceleration_effective'] else 'good'\n",
    "            },\n",
    "            'quality_insights': {\n",
    "                'distribution_analysis': {\n",
    "                    'mean_quality': round(avg_quality, 3),\n",
    "                    'quality_std': round(float(results_array[:, 1].std()), 3),\n",
    "                    'high_quality_ratio': float((results_array[:, 1] > 0.7).mean())\n",
    "                },\n",
    "                'dhatu_analysis': {\n",
    "                    'avg_matches_per_doc': round(total_matches / total_docs, 2),\n",
    "                    'diversity_score': round(avg_dhatu_diversity, 2),\n",
    "                    'detection_efficiency': round((total_matches / total_docs) * avg_quality, 3)\n",
    "                }\n",
    "            },\n",
    "            'next_actions': {\n",
    "                'continue_gpu_analysis': True,\n",
    "                'scale_collection': recommendations['scale_up_collection'],\n",
    "                'optimize_sources': len(best_sources) > 2,\n",
    "                'estimated_capacity': f\"{int(gpu_throughput * 3600)}_docs_per_hour\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Sauvegarde s√©curis√©e\n",
    "        os.makedirs('colab_results', exist_ok=True)\n",
    "        feedback_file = 'colab_results/gpu_enhanced_feedback.json'\n",
    "        \n",
    "        with open(feedback_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(feedback, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Feedback GPU sauv√©: {feedback_file}\")\n",
    "        \n",
    "        # Tentative synchronisation Git s√©curis√©e\n",
    "        return sync_gpu_feedback_safely(feedback_file, feedback)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur feedback GPU: {e}\")\n",
    "        return False\n",
    "\n",
    "def sync_gpu_feedback_safely(feedback_file, feedback_data):\n",
    "    \"\"\"Synchronisation Git s√©curis√©e pour feedback GPU\"\"\"\n",
    "    try:\n",
    "        # Test Git disponibilit√©\n",
    "        result = subprocess.run(['git', 'status'], capture_output=True, text=True, timeout=5)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Commit s√©curis√©\n",
    "            subprocess.run(['git', 'add', feedback_file], check=True, timeout=5)\n",
    "            \n",
    "            commit_msg = f\"üî• GPU T4 Feedback: {feedback_data['gpu_analysis']['gpu_efficiency_score']:.1f} docs/s\"\n",
    "            commit_result = subprocess.run(\n",
    "                ['git', 'commit', '-m', commit_msg],\n",
    "                capture_output=True, text=True, timeout=10\n",
    "            )\n",
    "            \n",
    "            if commit_result.returncode == 0:\n",
    "                # Tentative push\n",
    "                push_result = subprocess.run(\n",
    "                    ['git', 'push', 'origin', 'main'],\n",
    "                    capture_output=True, text=True, timeout=15\n",
    "                )\n",
    "                \n",
    "                if push_result.returncode == 0:\n",
    "                    print(\"üöÄ Feedback GPU synchronis√© sur GitHub!\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"üíæ Feedback GPU commit√© localement\")\n",
    "                    return True\n",
    "            else:\n",
    "                print(\"üíæ Feedback GPU sauv√© (pas de changements)\")\n",
    "                return True\n",
    "                \n",
    "        else:\n",
    "            print(\"üíæ Feedback GPU sauv√© localement (Git non disponible)\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"üíæ Feedback GPU sauv√© localement: {e}\")\n",
    "        return True\n",
    "\n",
    "# Cr√©er feedback GPU\n",
    "feedback_success = create_gpu_enhanced_feedback()\n",
    "\n",
    "if feedback_success:\n",
    "    print(\"\\nüî• FEEDBACK GPU ENVOY√â AU COLLECTEUR TURBO!\")\n",
    "    print(\"üöÄ Le collecteur va optimiser pour exploiter la puissance GPU\")\n",
    "    print(f\"‚ö° Capacit√© estim√©e: {int(analyzer.stats.get('processing_speed', 0) * 3600)} docs/heure\")\n",
    "else:\n",
    "    print(\"\\nüíæ Feedback GPU sauv√© localement\")\n",
    "    print(\"üîÑ Synchronisation manuelle possible\")\n",
    "\n",
    "print(f\"\\n{analyzer.get_gpu_summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f50a22",
   "metadata": {},
   "source": [
    "## üî• Guide GPU T4 Optimis√©\n",
    "\n",
    "### ‚ö° Avantages de cette version GPU\n",
    "\n",
    "1. **Traitement parall√©lis√©** - Batches optimis√©s pour GPU T4\n",
    "2. **Calculs vectoris√©s** - NumPy/PyTorch pour performance maximale\n",
    "3. **Analyse massive** - Traitement de milliers de documents\n",
    "4. **M√©triques avanc√©es** - Performance GPU en temps r√©el\n",
    "5. **Feedback intelligent** - Recommandations bas√©es GPU\n",
    "6. **Visualisations riches** - Graphiques multiples optimis√©s\n",
    "\n",
    "### üéØ Performance attendue\n",
    "\n",
    "- **GPU T4**: 100-500 docs/seconde selon complexit√©\n",
    "- **M√©moire**: Optimis√©e pour 15GB T4\n",
    "- **Batch size**: 64 documents simultan√©s\n",
    "- **Parall√©lisation**: 4 threads + GPU acceleration\n",
    "\n",
    "### üöÄ Workflow recommand√©\n",
    "\n",
    "1. **Setup GPU** ‚Üí V√©rification T4 + configuration\n",
    "2. **Chargement massif** ‚Üí Tous les documents disponibles\n",
    "3. **Analyse GPU** ‚Üí Traitement parall√©lis√© haute vitesse\n",
    "4. **Visualisation** ‚Üí Graphiques multiples et stats\n",
    "5. **Feedback optimis√©** ‚Üí Recommandations pour collecteur turbo\n",
    "\n",
    "### üí° Optimisations GPU\n",
    "\n",
    "- Batches adapt√©s √† la m√©moire GPU\n",
    "- Calculs vectoris√©s NumPy\n",
    "- Threading optimis√©\n",
    "- Gestion m√©moire intelligente\n",
    "- Monitoring performance temps r√©el\n",
    "\n",
    "üî• **Avec le GPU T4, analysez massivement et nourrissez le collecteur turbo efficacement !**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
