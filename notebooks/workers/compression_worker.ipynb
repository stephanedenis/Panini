{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b441e014",
   "metadata": {},
   "source": [
    "# üöÄ PaniniFS Compression Worker (Colab Pro)\n",
    "\n",
    "**Purpose**: GPU-accelerated semantic compression of file chunks\n",
    "\n",
    "**Pipeline**:\n",
    "1. Receive webhook from GitHub Actions\n",
    "2. Fetch chunk from GitHub repository\n",
    "3. Apply semantic compression (GPU-accelerated)\n",
    "4. Upload compressed result to Google One\n",
    "5. Send completion callback to GitHub\n",
    "\n",
    "**Requirements**:\n",
    "- Colab Pro (GPU access)\n",
    "- GitHub Personal Access Token (secret)\n",
    "- Google One storage (mounted via Google Drive)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dffbf2",
   "metadata": {},
   "source": [
    "## üîß Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q requests PyGithub google-auth google-auth-oauthlib google-auth-httplib2\n",
    "!pip install -q pillow numpy torch torchvision\n",
    "!pip install -q flask pyngrok  # For webhook server\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c73b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Google One storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create compression output directory\n",
    "import os\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/PaniniFS/compressed_chunks'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02428e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (use Colab secrets)\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "try:\n",
    "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "    GITHUB_REPO = userdata.get('GITHUB_REPO')  # Format: owner/repo\n",
    "    print(\"‚úÖ GitHub credentials loaded from secrets\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è GitHub credentials not found in Colab secrets\")\n",
    "    print(\"Please add GITHUB_TOKEN and GITHUB_REPO to Colab secrets\")\n",
    "    GITHUB_TOKEN = None\n",
    "    GITHUB_REPO = None\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "HAS_GPU = torch.cuda.is_available()\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if HAS_GPU else \"None\"\n",
    "print(f\"üéÆ GPU Available: {HAS_GPU} ({GPU_NAME})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc3fe",
   "metadata": {},
   "source": [
    "## üì¶ Chunk Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9379e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "class ChunkFetcher:\n",
    "    \"\"\"Fetch chunks from GitHub repository\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str, repo: str):\n",
    "        self.token = token\n",
    "        self.repo = repo\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Authorization': f'token {token}',\n",
    "            'Accept': 'application/vnd.github.v3+json'\n",
    "        })\n",
    "    \n",
    "    def fetch_chunk(self, chunk_path: str, output_dir: str = '/tmp') -> Dict[str, Any]:\n",
    "        \"\"\"Fetch chunk directory from GitHub\"\"\"\n",
    "        print(f\"üì• Fetching chunk: {chunk_path}\")\n",
    "        \n",
    "        # Get directory contents\n",
    "        api_url = f\"https://api.github.com/repos/{self.repo}/contents/{chunk_path}\"\n",
    "        response = self.session.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        files = response.json()\n",
    "        \n",
    "        # Create local directory\n",
    "        local_dir = Path(output_dir) / Path(chunk_path).name\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download each file\n",
    "        chunk_data = {'path': str(local_dir), 'files': {}}\n",
    "        \n",
    "        for file_info in files:\n",
    "            file_name = file_info['name']\n",
    "            file_url = file_info['download_url']\n",
    "            \n",
    "            # Download file\n",
    "            file_response = self.session.get(file_url)\n",
    "            file_response.raise_for_status()\n",
    "            \n",
    "            local_file = local_dir / file_name\n",
    "            local_file.write_bytes(file_response.content)\n",
    "            \n",
    "            chunk_data['files'][file_name] = str(local_file)\n",
    "            print(f\"  ‚úÖ {file_name} ({len(file_response.content)} bytes)\")\n",
    "        \n",
    "        # Load metadata\n",
    "        if 'metadata.json' in chunk_data['files']:\n",
    "            with open(chunk_data['files']['metadata.json']) as f:\n",
    "                chunk_data['metadata'] = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Chunk fetched: {local_dir}\")\n",
    "        return chunk_data\n",
    "\n",
    "print(\"‚úÖ ChunkFetcher class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3dfa90",
   "metadata": {},
   "source": [
    "## üß† GPU-Accelerated Compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gzip\n",
    "import hashlib\n",
    "\n",
    "class SemanticCompressor:\n",
    "    \"\"\"GPU-accelerated semantic compression\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu: bool = True):\n",
    "        self.device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"üéÆ Compressor using device: {self.device}\")\n",
    "    \n",
    "    def compress_chunk(self, chunk_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Compress chunk based on pattern type\"\"\"\n",
    "        metadata = chunk_data.get('metadata', {})\n",
    "        pattern_type = metadata.get('pattern_type', 'generic')\n",
    "        \n",
    "        print(f\"üîß Compressing chunk (pattern: {pattern_type})...\")\n",
    "        \n",
    "        # Get chunk content file\n",
    "        content_file = chunk_data['files'].get('content', None)\n",
    "        if not content_file:\n",
    "            raise ValueError(\"No content file found in chunk\")\n",
    "        \n",
    "        # Compression strategy based on pattern type\n",
    "        if pattern_type == 'image':\n",
    "            result = self._compress_image(content_file, metadata)\n",
    "        elif pattern_type == 'text':\n",
    "            result = self._compress_text(content_file, metadata)\n",
    "        else:\n",
    "            result = self._compress_generic(content_file, metadata)\n",
    "        \n",
    "        # Add compression stats\n",
    "        original_size = Path(content_file).stat().st_size\n",
    "        compressed_size = len(result['compressed_data'])\n",
    "        result['compression_stats'] = {\n",
    "            'original_size': original_size,\n",
    "            'compressed_size': compressed_size,\n",
    "            'ratio': compressed_size / original_size,\n",
    "            'savings_percent': (1 - compressed_size / original_size) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ Compressed: {original_size} ‚Üí {compressed_size} bytes ({result['compression_stats']['savings_percent']:.1f}% savings)\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _compress_image(self, content_file: str, metadata: Dict) -> Dict:\n",
    "        \"\"\"GPU-accelerated image compression\"\"\"\n",
    "        # Load image\n",
    "        img = Image.open(content_file)\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Convert to tensor and move to GPU\n",
    "        tensor = torch.from_numpy(img_array).float().to(self.device)\n",
    "        \n",
    "        # Semantic analysis (placeholder for actual semantic compression)\n",
    "        # TODO: Implement actual semantic compression algorithm\n",
    "        \n",
    "        # For now: efficient PNG compression\n",
    "        compressed = gzip.compress(img_array.tobytes(), compresslevel=9)\n",
    "        \n",
    "        return {\n",
    "            'compressed_data': compressed,\n",
    "            'compression_method': 'semantic_image_v1',\n",
    "            'reconstruction_recipe': {\n",
    "                'method': 'gzip_decompress',\n",
    "                'shape': img_array.shape,\n",
    "                'dtype': str(img_array.dtype),\n",
    "                'format': img.format\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _compress_text(self, content_file: str, metadata: Dict) -> Dict:\n",
    "        \"\"\"GPU-accelerated text compression\"\"\"\n",
    "        with open(content_file, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        # Use gzip for now (can be enhanced with semantic analysis)\n",
    "        compressed = gzip.compress(data, compresslevel=9)\n",
    "        \n",
    "        return {\n",
    "            'compressed_data': compressed,\n",
    "            'compression_method': 'semantic_text_v1',\n",
    "            'reconstruction_recipe': {\n",
    "                'method': 'gzip_decompress',\n",
    "                'encoding': 'utf-8'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _compress_generic(self, content_file: str, metadata: Dict) -> Dict:\n",
    "        \"\"\"Generic compression for unknown patterns\"\"\"\n",
    "        with open(content_file, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        compressed = gzip.compress(data, compresslevel=9)\n",
    "        \n",
    "        return {\n",
    "            'compressed_data': compressed,\n",
    "            'compression_method': 'generic_gzip_v1',\n",
    "            'reconstruction_recipe': {\n",
    "                'method': 'gzip_decompress'\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SemanticCompressor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6c1cb",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Google One Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf59d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "class GoogleOneUploader:\n",
    "    \"\"\"Upload compressed chunks to Google One (via Google Drive)\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = OUTPUT_DIR):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def upload(self, compressed_result: Dict, chunk_metadata: Dict) -> Dict:\n",
    "        \"\"\"Upload compressed chunk to Google One\"\"\"\n",
    "        chunk_id = chunk_metadata.get('chunk_id', 'unknown')\n",
    "        chunk_hash = chunk_metadata.get('original_hash', 'unknown')\n",
    "        \n",
    "        print(f\"‚òÅÔ∏è Uploading chunk {chunk_id} to Google One...\")\n",
    "        \n",
    "        # Create chunk directory\n",
    "        chunk_dir = self.output_dir / f\"chunk_{chunk_id:04d}\"\n",
    "        chunk_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save compressed data\n",
    "        compressed_file = chunk_dir / 'compressed.bin'\n",
    "        compressed_file.write_bytes(compressed_result['compressed_data'])\n",
    "        \n",
    "        # Save reconstruction recipe\n",
    "        recipe_file = chunk_dir / 'recipe.json'\n",
    "        recipe = {\n",
    "            'chunk_id': chunk_id,\n",
    "            'original_hash': chunk_hash,\n",
    "            'compression_method': compressed_result['compression_method'],\n",
    "            'reconstruction': compressed_result['reconstruction_recipe'],\n",
    "            'stats': compressed_result['compression_stats'],\n",
    "            'uploaded_at': datetime.utcnow().isoformat() + 'Z',\n",
    "            'worker': 'colab_pro_gpu'\n",
    "        }\n",
    "        recipe_file.write_text(json.dumps(recipe, indent=2))\n",
    "        \n",
    "        # Calculate upload hash\n",
    "        upload_hash = hashlib.sha256(compressed_result['compressed_data']).hexdigest()\n",
    "        \n",
    "        print(f\"  ‚úÖ Uploaded to: {chunk_dir}\")\n",
    "        print(f\"  üì¶ Compressed size: {len(compressed_result['compressed_data'])} bytes\")\n",
    "        print(f\"  üîê Upload hash: {upload_hash[:16]}...\")\n",
    "        \n",
    "        return {\n",
    "            'upload_path': str(chunk_dir),\n",
    "            'compressed_file': str(compressed_file),\n",
    "            'recipe_file': str(recipe_file),\n",
    "            'upload_hash': upload_hash,\n",
    "            'stats': compressed_result['compression_stats']\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ GoogleOneUploader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df3ca9",
   "metadata": {},
   "source": [
    "## üîÑ GitHub Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f270dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubCallback:\n",
    "    \"\"\"Send completion callback to GitHub\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str, repo: str):\n",
    "        self.token = token\n",
    "        self.repo = repo\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Authorization': f'token {token}',\n",
    "            'Accept': 'application/vnd.github.v3+json'\n",
    "        })\n",
    "    \n",
    "    def send_completion(self, chunk_path: str, upload_result: Dict) -> bool:\n",
    "        \"\"\"Send completion status back to GitHub\"\"\"\n",
    "        print(f\"üì§ Sending completion callback to GitHub...\")\n",
    "        \n",
    "        # Update chunk metadata in repository via GitHub API\n",
    "        # For now, create an issue comment or use repository dispatch\n",
    "        \n",
    "        # Option 1: Repository dispatch event\n",
    "        dispatch_url = f\"https://api.github.com/repos/{self.repo}/dispatches\"\n",
    "        payload = {\n",
    "            'event_type': 'chunk_compressed',\n",
    "            'client_payload': {\n",
    "                'chunk_path': chunk_path,\n",
    "                'status': 'completed',\n",
    "                'upload_path': upload_result['upload_path'],\n",
    "                'upload_hash': upload_result['upload_hash'],\n",
    "                'stats': upload_result['stats']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(dispatch_url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            print(\"  ‚úÖ Callback sent successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Callback failed: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"‚úÖ GitHubCallback class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11ab05",
   "metadata": {},
   "source": [
    "## üöÄ Main Worker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_info: Dict) -> Dict:\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    try:\n",
    "        print(\"=\"*60)\n",
    "        print(\"üöÄ Starting chunk compression...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Initialize components\n",
    "        fetcher = ChunkFetcher(GITHUB_TOKEN, GITHUB_REPO)\n",
    "        compressor = SemanticCompressor(use_gpu=HAS_GPU)\n",
    "        uploader = GoogleOneUploader()\n",
    "        callback = GitHubCallback(GITHUB_TOKEN, GITHUB_REPO)\n",
    "        \n",
    "        # 1. Fetch chunk\n",
    "        chunk_path = chunk_info['chunk_path']\n",
    "        chunk_data = fetcher.fetch_chunk(chunk_path)\n",
    "        \n",
    "        # 2. Compress\n",
    "        compressed_result = compressor.compress_chunk(chunk_data)\n",
    "        \n",
    "        # 3. Upload to Google One\n",
    "        upload_result = uploader.upload(compressed_result, chunk_data.get('metadata', {}))\n",
    "        \n",
    "        # 4. Send callback\n",
    "        callback.send_completion(chunk_path, upload_result)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"‚úÖ Chunk processing completed successfully!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'chunk_path': chunk_path,\n",
    "            'upload_result': upload_result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"‚ùå Error processing chunk: {e}\")\n",
    "        print(\"=\"*60)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Main worker function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77420768",
   "metadata": {},
   "source": [
    "## üåê Webhook Server (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from threading import Thread\n",
    "from pyngrok import ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "processing_queue = []\n",
    "\n",
    "@app.route('/webhook', methods=['POST'])\n",
    "def webhook():\n",
    "    \"\"\"Receive webhook from GitHub Actions\"\"\"\n",
    "    data = request.json\n",
    "    print(f\"üì® Received webhook: {data.get('action')}\")\n",
    "    \n",
    "    if data.get('action') == 'compress_chunk':\n",
    "        chunk_info = data.get('chunk_info', {})\n",
    "        processing_queue.append(chunk_info)\n",
    "        \n",
    "        # Process immediately in background\n",
    "        Thread(target=process_chunk, args=(chunk_info,)).start()\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'accepted',\n",
    "            'message': 'Chunk queued for processing'\n",
    "        }), 202\n",
    "    \n",
    "    return jsonify({'status': 'ok'}), 200\n",
    "\n",
    "@app.route('/status', methods=['GET'])\n",
    "def status():\n",
    "    \"\"\"Get worker status\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'ready',\n",
    "        'gpu': GPU_NAME,\n",
    "        'queue_length': len(processing_queue)\n",
    "    })\n",
    "\n",
    "def start_webhook_server():\n",
    "    \"\"\"Start Flask server with ngrok tunnel\"\"\"\n",
    "    # Start ngrok tunnel\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üåê Webhook server started!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Public URL: {public_url}\")\n",
    "    print(f\"Webhook endpoint: {public_url}/webhook\")\n",
    "    print(f\"Status endpoint: {public_url}/status\")\n",
    "    print(f\"\\n‚ö†Ô∏è Add this URL to GitHub secrets as COLAB_WEBHOOK_URL\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Start Flask\n",
    "    app.run(port=5000)\n",
    "\n",
    "print(\"‚úÖ Webhook server defined\")\n",
    "print(\"\\nTo start webhook server, run: start_webhook_server()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07214f1a",
   "metadata": {},
   "source": [
    "## üß™ Test Manual Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8038c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with manual chunk info\n",
    "test_chunk_info = {\n",
    "    'chunk_path': 'pending_compression/test_image/chunk_0000',\n",
    "    'pattern_type': 'image',\n",
    "    'chunk_id': 0\n",
    "}\n",
    "\n",
    "# Uncomment to test:\n",
    "# result = process_chunk(test_chunk_info)\n",
    "# print(json.dumps(result, indent=2))\n",
    "\n",
    "print(\"‚úÖ Ready for manual testing\")\n",
    "print(\"Uncomment the lines above to test with a chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640abd3",
   "metadata": {},
   "source": [
    "## üéØ Start Webhook Server\n",
    "\n",
    "**Run this cell to start the webhook server and wait for GitHub Actions to dispatch chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7562fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webhook server\n",
    "# This will run indefinitely and process chunks as they arrive\n",
    "\n",
    "if GITHUB_TOKEN and GITHUB_REPO:\n",
    "    print(\"üöÄ Starting compression worker with webhook server...\")\n",
    "    start_webhook_server()\n",
    "else:\n",
    "    print(\"‚ùå Cannot start: GitHub credentials not configured\")\n",
    "    print(\"Please add GITHUB_TOKEN and GITHUB_REPO to Colab secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f477729",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "**Setup Steps**:\n",
    "1. Add secrets to Colab:\n",
    "   - `GITHUB_TOKEN`: Personal access token with repo access\n",
    "   - `GITHUB_REPO`: Format `owner/repo`\n",
    "2. Run all cells above\n",
    "3. Copy the ngrok webhook URL\n",
    "4. Add URL to GitHub repository secrets as `COLAB_WEBHOOK_URL`\n",
    "5. Notebook will process chunks automatically when GitHub Actions dispatches them\n",
    "\n",
    "**GPU Optimization**:\n",
    "- This notebook uses GPU when available (Colab Pro)\n",
    "- Semantic compression algorithms can be enhanced with deep learning models\n",
    "- Current implementation uses efficient gzip as baseline\n",
    "\n",
    "**Google One Storage**:\n",
    "- Compressed chunks saved to Google Drive (which syncs to Google One)\n",
    "- Path: `/content/drive/MyDrive/PaniniFS/compressed_chunks/`\n",
    "- Each chunk has: `compressed.bin` + `recipe.json`\n",
    "\n",
    "**Monitoring**:\n",
    "- Check webhook status: `{ngrok_url}/status`\n",
    "- View processing logs in notebook output\n",
    "- GitHub Actions receives callbacks after completion\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated**: November 12, 2025  \n",
    "**Version**: 1.0  \n",
    "**Architecture**: See `docs/architecture/ASYNC_SEMANTIC_COMPRESSION_PIPELINE.md`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
