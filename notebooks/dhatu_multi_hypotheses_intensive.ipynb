{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac218b8",
   "metadata": {},
   "source": [
    "# ğŸ§¬ Analyse DhÄtu Multi-HypothÃ¨ses Intensive\n",
    "\n",
    "**PaniniFS Research - Processing Intensif GPU pour HypothÃ¨ses Multiples**\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-PaniniFS--Research-blue)](https://github.com/stephanedenis/PaniniFS-Research)\n",
    "\n",
    "## ğŸ¯ Objectifs du Traitement Intensif\n",
    "\n",
    "1. **Analyse Multi-HypothÃ¨ses** : Pour chaque document, tester multiples interprÃ©tations dhÄtu\n",
    "2. **Multi-Langues** : FranÃ§ais, Anglais, Sanskrit translittÃ©rÃ©\n",
    "3. **Variantes Aspectuelles** : Chaque dhÄtu testÃ© sous diffÃ©rents aspects temporels\n",
    "4. **Combinaisons Complexes** : Patterns dhÄtu composÃ©s et interactions\n",
    "5. **GPU Long Processing** : OptimisÃ© pour sessions de plusieurs heures\n",
    "\n",
    "---\n",
    "\n",
    "âš ï¸ **Configuration Required**: Runtime â†’ Change runtime type â†’ GPU (T4 ou V100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Configuration GPU et Environnement\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# VÃ©rification GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”¥ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   CUDA: {torch.version.cuda}\")\n",
    "\n",
    "# Session info\n",
    "session_id = f\"intensive_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"\\nğŸ§¬ Session ID: {session_id}\")\n",
    "print(f\"â±ï¸ Start Time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f824c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ Synchronisation GitHub\n",
    "!git clone https://github.com/stephanedenis/PaniniFS-Research.git\n",
    "%cd PaniniFS-Research\n",
    "\n",
    "# Configuration Git pour commits\n",
    "!git config user.email \"research@paninifs.org\"\n",
    "!git config user.name \"PaniniFS-Colab-Intensive\"\n",
    "\n",
    "print(\"ğŸ“‚ Repository synchronized\")\n",
    "!ls -la data/*.json | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c216579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Chargement Corpus Multi-Sources\n",
    "\n",
    "def load_all_corpus():\n",
    "    \"\"\"Charge tous les corpus disponibles\"\"\"\n",
    "    \n",
    "    corpus_files = [\n",
    "        'data/corpus_scientifique.json',\n",
    "        'data/corpus_multilingue_dev.json',\n",
    "        'data/corpus_prescolaire.json'\n",
    "    ]\n",
    "    \n",
    "    all_documents = []\n",
    "    load_stats = {}\n",
    "    \n",
    "    for file_path in corpus_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different formats\n",
    "            if isinstance(data, list):\n",
    "                documents = data\n",
    "            elif isinstance(data, dict) and 'documents' in data:\n",
    "                documents = data['documents']\n",
    "            else:\n",
    "                print(f\"âš ï¸ Format non reconnu: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Standardize document format\n",
    "            for i, doc in enumerate(documents):\n",
    "                if isinstance(doc, dict):\n",
    "                    # Ensure required fields\n",
    "                    doc['source_file'] = file_path\n",
    "                    doc['doc_index'] = i\n",
    "                    if 'id' not in doc:\n",
    "                        doc['id'] = f\"{Path(file_path).stem}_{i}\"\n",
    "                    if 'content' not in doc:\n",
    "                        doc['content'] = doc.get('abstract', doc.get('text', ''))\n",
    "                    if 'language' not in doc:\n",
    "                        doc['language'] = 'en'  # default\n",
    "                    \n",
    "                    all_documents.append(doc)\n",
    "            \n",
    "            load_stats[file_path] = len(documents)\n",
    "            print(f\"âœ… {file_path}: {len(documents)} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur {file_path}: {e}\")\n",
    "            load_stats[file_path] = 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š CORPUS TOTAL: {len(all_documents)} documents\")\n",
    "    print(f\"ğŸ“ˆ Breakdown: {load_stats}\")\n",
    "    \n",
    "    return all_documents, load_stats\n",
    "\n",
    "# Load corpus\n",
    "corpus_documents, corpus_stats = load_all_corpus()\n",
    "\n",
    "# Sample document preview\n",
    "if corpus_documents:\n",
    "    print(f\"\\nğŸ“„ EXEMPLE DOCUMENT:\")\n",
    "    sample = corpus_documents[0]\n",
    "    print(f\"   ID: {sample.get('id', 'N/A')}\")\n",
    "    print(f\"   Language: {sample.get('language', 'N/A')}\")\n",
    "    print(f\"   Content: {sample.get('content', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¬ Analyseur DhÄtu Multi-HypothÃ¨ses GPU-AccÃ©lÃ©rÃ©\n",
    "\n",
    "@dataclass\n",
    "class DhatuHypothesis:\n",
    "    dhatu_root: str\n",
    "    aspect: str  # 'present', 'past', 'future', 'perfect', 'continuous'\n",
    "    intensity: float\n",
    "    confidence: float\n",
    "    language_variant: str\n",
    "    contextual_factors: List[str]\n",
    "    semantic_field: str\n",
    "\n",
    "class IntensiveDhatuAnalyzer:\n",
    "    \"\"\"Analyseur intensif multi-hypothÃ¨ses pour dhÄtu\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = device\n",
    "        print(f\"ğŸ§¬ Analyseur intensif initialisÃ© sur {self.device}\")\n",
    "        \n",
    "        # DhÄtu roots avec aspects multiples\n",
    "        self.dhatu_database = {\n",
    "            'RELATE': {\n",
    "                'root': 'âˆšbandh',\n",
    "                'patterns': {\n",
    "                    'en': [r'connect', r'link', r'bind', r'relate', r'join', r'unite', r'associate'],\n",
    "                    'fr': [r'relier', r'connecter', r'lier', r'unir', r'associer', r'joindre'],\n",
    "                    'sa': [r'bandh', r'yuj', r'sam.*gam']\n",
    "                },\n",
    "                'aspects': ['present', 'continuous', 'perfect'],\n",
    "                'semantic_fields': ['connection', 'relationship', 'unity']\n",
    "            },\n",
    "            'EXIST': {\n",
    "                'root': 'âˆšas',\n",
    "                'patterns': {\n",
    "                    'en': [r'\\bis\\b', r'\\bare\\b', r'exist', r'being', r'there.*is', r'\\bbe\\b'],\n",
    "                    'fr': [r'Ãªtre', r'\\best\\b', r'existe', r'il.*y.*a', r'se.*trouve'],\n",
    "                    'sa': [r'\\bas\\b', r'bhÅ«', r'sat']\n",
    "                },\n",
    "                'aspects': ['present', 'continuous', 'eternal'],\n",
    "                'semantic_fields': ['existence', 'being', 'reality']\n",
    "            },\n",
    "            'CAUSE': {\n",
    "                'root': 'âˆšká¹›',\n",
    "                'patterns': {\n",
    "                    'en': [r'cause', r'make', r'create', r'produce', r'generate', r'effect'],\n",
    "                    'fr': [r'causer', r'faire', r'crÃ©er', r'produire', r'gÃ©nÃ©rer', r'effectuer'],\n",
    "                    'sa': [r'ká¹›', r'jan', r'utpÄd']\n",
    "                },\n",
    "                'aspects': ['present', 'past', 'future', 'causal'],\n",
    "                'semantic_fields': ['causation', 'creation', 'action']\n",
    "            },\n",
    "            'EVAL': {\n",
    "                'root': 'âˆšman',\n",
    "                'patterns': {\n",
    "                    'en': [r'think', r'analyze', r'evaluate', r'assess', r'consider', r'judge'],\n",
    "                    'fr': [r'penser', r'analyser', r'Ã©valuer', r'considÃ©rer', r'juger'],\n",
    "                    'sa': [r'man', r'cint', r'dhÄ«']\n",
    "                },\n",
    "                'aspects': ['present', 'continuous', 'deliberative'],\n",
    "                'semantic_fields': ['cognition', 'evaluation', 'analysis']\n",
    "            },\n",
    "            'FEEL': {\n",
    "                'root': 'âˆšbhÄv',\n",
    "                'patterns': {\n",
    "                    'en': [r'feel', r'emotion', r'sense', r'experience', r'perceive'],\n",
    "                    'fr': [r'sentir', r'Ã©motion', r'ressentir', r'Ã©prouver', r'percevoir'],\n",
    "                    'sa': [r'bhÄv', r'ved', r'anubhÅ«']\n",
    "                },\n",
    "                'aspects': ['present', 'experiential', 'continuous'],\n",
    "                'semantic_fields': ['emotion', 'perception', 'experience']\n",
    "            },\n",
    "            'COMM': {\n",
    "                'root': 'âˆšvac',\n",
    "                'patterns': {\n",
    "                    'en': [r'speak', r'say', r'communicate', r'tell', r'express', r'convey'],\n",
    "                    'fr': [r'parler', r'dire', r'communiquer', r'exprimer', r'transmettre'],\n",
    "                    'sa': [r'vac', r'bhÄá¹£', r'kath']\n",
    "                },\n",
    "                'aspects': ['present', 'past', 'continuous'],\n",
    "                'semantic_fields': ['communication', 'expression', 'language']\n",
    "            },\n",
    "            'ITER': {\n",
    "                'root': 'âˆšgam',\n",
    "                'patterns': {\n",
    "                    'en': [r'go', r'move', r'process', r'iterate', r'repeat', r'continue'],\n",
    "                    'fr': [r'aller', r'bouger', r'processus', r'itÃ©rer', r'rÃ©pÃ©ter', r'continuer'],\n",
    "                    'sa': [r'gam', r'cal', r'yÄ']\n",
    "                },\n",
    "                'aspects': ['present', 'continuous', 'iterative'],\n",
    "                'semantic_fields': ['movement', 'process', 'iteration']\n",
    "            },\n",
    "            'DECIDE': {\n",
    "                'root': 'âˆšniÅ›',\n",
    "                'patterns': {\n",
    "                    'en': [r'decide', r'choose', r'determine', r'resolve', r'conclude'],\n",
    "                    'fr': [r'dÃ©cider', r'choisir', r'dÃ©terminer', r'rÃ©soudre', r'conclure'],\n",
    "                    'sa': [r'niÅ›', r'vá¹›', r'adhyavas']\n",
    "                },\n",
    "                'aspects': ['present', 'perfect', 'decisive'],\n",
    "                'semantic_fields': ['decision', 'choice', 'resolution']\n",
    "            },\n",
    "            'MODAL': {\n",
    "                'root': 'âˆšÅ›ak',\n",
    "                'patterns': {\n",
    "                    'en': [r'can', r'could', r'might', r'may', r'should', r'would', r'able'],\n",
    "                    'fr': [r'peut', r'pourrait', r'devrait', r'capable', r'possible'],\n",
    "                    'sa': [r'Å›ak', r'arh', r'Ä«Å›']\n",
    "                },\n",
    "                'aspects': ['potential', 'conditional', 'modal'],\n",
    "                'semantic_fields': ['possibility', 'capability', 'modality']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Statistical tracking\n",
    "        self.analysis_stats = {\n",
    "            'documents_processed': 0,\n",
    "            'hypotheses_generated': 0,\n",
    "            'computation_time': 0,\n",
    "            'gpu_memory_used': 0\n",
    "        }\n",
    "    \n",
    "    def generate_hypotheses_for_document(self, document: Dict[str, Any]) -> List[DhatuHypothesis]:\n",
    "        \"\"\"GÃ©nÃ¨re toutes les hypothÃ¨ses dhÄtu pour un document\"\"\"\n",
    "        \n",
    "        content = document.get('content', '').lower()\n",
    "        doc_language = document.get('language', 'en')\n",
    "        \n",
    "        hypotheses = []\n",
    "        \n",
    "        # Pour chaque dhÄtu\n",
    "        for dhatu_name, dhatu_data in self.dhatu_database.items():\n",
    "            \n",
    "            # Pour chaque langue (incluant la langue du document et variantes)\n",
    "            languages_to_test = [doc_language]\n",
    "            if doc_language != 'en':\n",
    "                languages_to_test.append('en')  # Toujours tester anglais\n",
    "            if doc_language not in ['sa', 'sanskrit']:\n",
    "                languages_to_test.append('sa')  # Toujours tester sanskrit\n",
    "            \n",
    "            for lang in languages_to_test:\n",
    "                if lang not in dhatu_data['patterns']:\n",
    "                    continue\n",
    "                \n",
    "                patterns = dhatu_data['patterns'][lang]\n",
    "                \n",
    "                # Pour chaque aspect\n",
    "                for aspect in dhatu_data['aspects']:\n",
    "                    \n",
    "                    # Pour chaque pattern dans la langue\n",
    "                    for pattern in patterns:\n",
    "                        \n",
    "                        # Match pattern dans le contenu\n",
    "                        matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "                        \n",
    "                        if matches:\n",
    "                            # Calculer mÃ©triques\n",
    "                            intensity = len(matches) / max(len(content.split()), 1)\n",
    "                            \n",
    "                            # Confidence basÃ©e sur plusieurs facteurs\n",
    "                            confidence = self._calculate_confidence(\n",
    "                                matches, pattern, content, lang, aspect\n",
    "                            )\n",
    "                            \n",
    "                            # Facteurs contextuels\n",
    "                            contextual_factors = self._extract_context_factors(\n",
    "                                content, matches, pattern\n",
    "                            )\n",
    "                            \n",
    "                            # Champ sÃ©mantique dominant\n",
    "                            semantic_field = self._determine_semantic_field(\n",
    "                                dhatu_data['semantic_fields'], content, matches\n",
    "                            )\n",
    "                            \n",
    "                            hypothesis = DhatuHypothesis(\n",
    "                                dhatu_root=dhatu_data['root'],\n",
    "                                aspect=aspect,\n",
    "                                intensity=intensity,\n",
    "                                confidence=confidence,\n",
    "                                language_variant=lang,\n",
    "                                contextual_factors=contextual_factors,\n",
    "                                semantic_field=semantic_field\n",
    "                            )\n",
    "                            \n",
    "                            hypotheses.append(hypothesis)\n",
    "                            self.analysis_stats['hypotheses_generated'] += 1\n",
    "        \n",
    "        return hypotheses\n",
    "    \n",
    "    def _calculate_confidence(self, matches: List[str], pattern: str, \n",
    "                            content: str, language: str, aspect: str) -> float:\n",
    "        \"\"\"Calcule score de confiance pour une hypothÃ¨se\"\"\"\n",
    "        \n",
    "        base_confidence = min(len(matches) * 0.1, 1.0)\n",
    "        \n",
    "        # Bonus pour correspondance de langue\n",
    "        if language == 'en':\n",
    "            base_confidence *= 1.1\n",
    "        elif language == 'sa':\n",
    "            base_confidence *= 1.3  # Sanskrit plus rare, donc plus significatif\n",
    "        \n",
    "        # Bonus pour aspects spÃ©ciaux\n",
    "        if aspect in ['perfect', 'causal', 'deliberative']:\n",
    "            base_confidence *= 1.2\n",
    "        \n",
    "        # PÃ©nalitÃ© pour patterns trop communs\n",
    "        if pattern in [r'\\bis\\b', r'\\bbe\\b', r'can', r'the']:\n",
    "            base_confidence *= 0.7\n",
    "        \n",
    "        return min(base_confidence, 1.0)\n",
    "    \n",
    "    def _extract_context_factors(self, content: str, matches: List[str], \n",
    "                                pattern: str) -> List[str]:\n",
    "        \"\"\"Extrait facteurs contextuels autour des matches\"\"\"\n",
    "        \n",
    "        factors = []\n",
    "        \n",
    "        # Analyse du contexte autour de chaque match\n",
    "        for match in matches[:3]:  # Limite pour performance\n",
    "            match_pos = content.find(match.lower())\n",
    "            if match_pos >= 0:\n",
    "                # Context window\n",
    "                start = max(0, match_pos - 50)\n",
    "                end = min(len(content), match_pos + len(match) + 50)\n",
    "                context = content[start:end]\n",
    "                \n",
    "                # Recherche indicateurs contextuels\n",
    "                if re.search(r'\\b(not|no|never)\\b', context):\n",
    "                    factors.append('negation')\n",
    "                if re.search(r'\\b(very|extremely|highly)\\b', context):\n",
    "                    factors.append('intensification')\n",
    "                if re.search(r'\\b(maybe|perhaps|possibly)\\b', context):\n",
    "                    factors.append('uncertainty')\n",
    "                if re.search(r'\\b(and|or|but)\\b', context):\n",
    "                    factors.append('conjunction')\n",
    "        \n",
    "        return list(set(factors))  # Remove duplicates\n",
    "    \n",
    "    def _determine_semantic_field(self, possible_fields: List[str], \n",
    "                                 content: str, matches: List[str]) -> str:\n",
    "        \"\"\"DÃ©termine le champ sÃ©mantique dominant\"\"\"\n",
    "        \n",
    "        field_scores = {}\n",
    "        \n",
    "        for field in possible_fields:\n",
    "            score = 0\n",
    "            \n",
    "            # Score basÃ© sur mots-clÃ©s du champ\n",
    "            field_keywords = {\n",
    "                'connection': ['network', 'system', 'link', 'relationship'],\n",
    "                'existence': ['reality', 'being', 'present', 'available'],\n",
    "                'causation': ['effect', 'result', 'consequence', 'impact'],\n",
    "                'cognition': ['research', 'analysis', 'study', 'investigation'],\n",
    "                'emotion': ['feeling', 'sentiment', 'mood', 'attitude'],\n",
    "                'communication': ['language', 'message', 'information', 'data']\n",
    "            }\n",
    "            \n",
    "            if field in field_keywords:\n",
    "                for keyword in field_keywords[field]:\n",
    "                    if keyword in content.lower():\n",
    "                        score += 1\n",
    "            \n",
    "            field_scores[field] = score\n",
    "        \n",
    "        # Retourne le champ avec le score le plus Ã©levÃ©\n",
    "        return max(field_scores, key=field_scores.get) if field_scores else possible_fields[0]\n",
    "\n",
    "# Initialisation de l'analyseur\n",
    "intensive_analyzer = IntensiveDhatuAnalyzer()\n",
    "print(\"ğŸ§¬ Analyseur multi-hypothÃ¨ses prÃªt pour traitement intensif!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecec507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ TRAITEMENT INTENSIF MULTI-HYPOTHÃˆSES\n",
    "\n",
    "def run_intensive_analysis(documents: List[Dict], batch_size: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"Lance l'analyse intensive multi-hypothÃ¨ses\"\"\"\n",
    "    \n",
    "    print(f\"ğŸš€ DÃ‰MARRAGE ANALYSE INTENSIVE\")\n",
    "    print(f\"ğŸ“Š Documents: {len(documents)}\")\n",
    "    print(f\"ğŸ”„ Batch size: {batch_size}\")\n",
    "    print(f\"â±ï¸ Estimation: {len(documents) * 2:.1f} secondes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    all_results = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx, i in enumerate(range(0, len(documents), batch_size)):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ BATCH {batch_idx + 1}/{total_batches} ({len(batch)} docs)\")\n",
    "        \n",
    "        batch_results = []\n",
    "        \n",
    "        for doc_idx, document in enumerate(batch):\n",
    "            doc_start = time.time()\n",
    "            \n",
    "            # GÃ©nÃ©ration des hypothÃ¨ses pour ce document\n",
    "            hypotheses = intensive_analyzer.generate_hypotheses_for_document(document)\n",
    "            \n",
    "            # Tri des hypothÃ¨ses par confiance\n",
    "            hypotheses.sort(key=lambda h: h.confidence, reverse=True)\n",
    "            \n",
    "            # Analyse statistique des hypothÃ¨ses\n",
    "            hypothesis_stats = {\n",
    "                'total_hypotheses': len(hypotheses),\n",
    "                'high_confidence': len([h for h in hypotheses if h.confidence > 0.7]),\n",
    "                'medium_confidence': len([h for h in hypotheses if 0.3 < h.confidence <= 0.7]),\n",
    "                'low_confidence': len([h for h in hypotheses if h.confidence <= 0.3]),\n",
    "                'languages_detected': list(set(h.language_variant for h in hypotheses)),\n",
    "                'aspects_detected': list(set(h.aspect for h in hypotheses)),\n",
    "                'semantic_fields': list(set(h.semantic_field for h in hypotheses))\n",
    "            }\n",
    "            \n",
    "            # Top hypothÃ¨ses pour export\n",
    "            top_hypotheses = []\n",
    "            for h in hypotheses[:10]:  # Top 10\n",
    "                top_hypotheses.append({\n",
    "                    'dhatu_root': h.dhatu_root,\n",
    "                    'aspect': h.aspect,\n",
    "                    'intensity': round(h.intensity, 4),\n",
    "                    'confidence': round(h.confidence, 4),\n",
    "                    'language': h.language_variant,\n",
    "                    'semantic_field': h.semantic_field,\n",
    "                    'context_factors': h.contextual_factors\n",
    "                })\n",
    "            \n",
    "            doc_result = {\n",
    "                'document_id': document.get('id'),\n",
    "                'source_file': document.get('source_file'),\n",
    "                'language': document.get('language'),\n",
    "                'content_length': len(document.get('content', '')),\n",
    "                'processing_time_ms': round((time.time() - doc_start) * 1000, 2),\n",
    "                'hypothesis_stats': hypothesis_stats,\n",
    "                'top_hypotheses': top_hypotheses\n",
    "            }\n",
    "            \n",
    "            batch_results.append(doc_result)\n",
    "            intensive_analyzer.analysis_stats['documents_processed'] += 1\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (doc_idx + 1) % 5 == 0:\n",
    "                elapsed = time.time() - batch_start\n",
    "                rate = (doc_idx + 1) / elapsed\n",
    "                print(f\"   ğŸ“„ {doc_idx + 1}/{len(batch)} docs | {rate:.1f} docs/sec | {hypothesis_stats['total_hypotheses']} hypotheses\")\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        print(f\"   âœ… Batch terminÃ© en {batch_time:.1f}s | {len(batch_results)} docs | {sum(r['hypothesis_stats']['total_hypotheses'] for r in batch_results)} hypotheses\")\n",
    "        \n",
    "        # Memory cleanup pour long processing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Final statistics\n",
    "    final_stats = {\n",
    "        'session_id': session_id,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'processing_summary': {\n",
    "            'total_documents': len(documents),\n",
    "            'total_processing_time_seconds': round(total_time, 2),\n",
    "            'average_time_per_document_ms': round((total_time / len(documents)) * 1000, 2),\n",
    "            'documents_per_second': round(len(documents) / total_time, 2),\n",
    "            'total_hypotheses_generated': intensive_analyzer.analysis_stats['hypotheses_generated'],\n",
    "            'average_hypotheses_per_document': round(intensive_analyzer.analysis_stats['hypotheses_generated'] / len(documents), 1)\n",
    "        },\n",
    "        'gpu_info': {\n",
    "            'device': str(device),\n",
    "            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "            'memory_allocated_mb': round(torch.cuda.memory_allocated() / 1e6, 2) if torch.cuda.is_available() else 0\n",
    "        },\n",
    "        'corpus_analysis': {\n",
    "            'source_breakdown': Counter(r['source_file'] for r in all_results),\n",
    "            'language_breakdown': Counter(r['language'] for r in all_results),\n",
    "            'high_confidence_docs': len([r for r in all_results if r['hypothesis_stats']['high_confidence'] > 0])\n",
    "        },\n",
    "        'detailed_results': all_results\n",
    "    }\n",
    "    \n",
    "    return final_stats\n",
    "\n",
    "# LANCEMENT ANALYSE INTENSIVE\n",
    "print(\"ğŸ§¬ DÃ‰MARRAGE TRAITEMENT INTENSIF MULTI-HYPOTHÃˆSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(corpus_documents) > 0:\n",
    "    # Configuration pour traitement intensif\n",
    "    BATCH_SIZE = 5  # Plus petit pour monitoring dÃ©taillÃ©\n",
    "    \n",
    "    intensive_results = run_intensive_analysis(corpus_documents, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(\"\\nğŸ¯ RÃ‰SULTATS FINAUX:\")\n",
    "    print(f\"   ğŸ“Š Documents traitÃ©s: {intensive_results['processing_summary']['total_documents']}\")\n",
    "    print(f\"   ğŸ§¬ HypothÃ¨ses gÃ©nÃ©rÃ©es: {intensive_results['processing_summary']['total_hypotheses_generated']}\")\n",
    "    print(f\"   â±ï¸ Temps total: {intensive_results['processing_summary']['total_processing_time_seconds']}s\")\n",
    "    print(f\"   ğŸš€ Throughput: {intensive_results['processing_summary']['documents_per_second']} docs/sec\")\n",
    "    print(f\"   ğŸ“ˆ Avg hypotheses/doc: {intensive_results['processing_summary']['average_hypotheses_per_document']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Aucun document trouvÃ© pour le traitement\")\n",
    "    intensive_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ EXPORT RÃ‰SULTATS et SYNCHRONISATION GITHUB\n",
    "\n",
    "if intensive_results:\n",
    "    # Sauvegarde des rÃ©sultats complets\n",
    "    results_filename = f\"dhatu_intensive_analysis_{session_id}.json\"\n",
    "    results_path = f\"colab_results/{results_filename}\"\n",
    "    \n",
    "    # CrÃ©er le dossier si nÃ©cessaire\n",
    "    !mkdir -p colab_results\n",
    "    \n",
    "    # Sauvegarde JSON complet\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(intensive_results, f, ensure_ascii=False, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ğŸ’¾ RÃ©sultats sauvegardÃ©s: {results_path}\")\n",
    "    \n",
    "    # GÃ©nÃ©ration rapport Markdown exÃ©cutif\n",
    "    report_filename = f\"RAPPORT_INTENSIF_{session_id}.md\"\n",
    "    report_path = f\"colab_results/{report_filename}\"\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# ğŸ§¬ Rapport Analyse DhÄtu Intensive\\n\\n\")\n",
    "        f.write(f\"**Session**: {session_id}\\n\")\n",
    "        f.write(f\"**Timestamp**: {intensive_results['timestamp']}\\n\")\n",
    "        f.write(f\"**GPU**: {intensive_results['gpu_info']['gpu_name']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## ğŸ“Š RÃ©sumÃ© du Traitement\\n\\n\")\n",
    "        summary = intensive_results['processing_summary']\n",
    "        f.write(f\"- **Documents traitÃ©s**: {summary['total_documents']:,}\\n\")\n",
    "        f.write(f\"- **HypothÃ¨ses gÃ©nÃ©rÃ©es**: {summary['total_hypotheses_generated']:,}\\n\")\n",
    "        f.write(f\"- **Temps de traitement**: {summary['total_processing_time_seconds']}s\\n\")\n",
    "        f.write(f\"- **Throughput**: {summary['documents_per_second']} docs/sec\\n\")\n",
    "        f.write(f\"- **Moyenne hypothÃ¨ses/doc**: {summary['average_hypotheses_per_document']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## ğŸŒ Analyse du Corpus\\n\\n\")\n",
    "        corpus_analysis = intensive_results['corpus_analysis']\n",
    "        f.write(f\"### Sources\\n\")\n",
    "        for source, count in corpus_analysis['source_breakdown'].items():\n",
    "            f.write(f\"- {source}: {count} documents\\n\")\n",
    "        \n",
    "        f.write(f\"\\n### Langues\\n\")\n",
    "        for lang, count in corpus_analysis['language_breakdown'].items():\n",
    "            f.write(f\"- {lang}: {count} documents\\n\")\n",
    "        \n",
    "        f.write(f\"\\n- **Documents haute confiance**: {corpus_analysis['high_confidence_docs']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## ğŸ† Top DÃ©couvertes\\n\\n\")\n",
    "        \n",
    "        # Analyse des meilleures hypothÃ¨ses\n",
    "        all_hypotheses = []\n",
    "        for result in intensive_results['detailed_results']:\n",
    "            for hyp in result['top_hypotheses']:\n",
    "                hyp['doc_id'] = result['document_id']\n",
    "                all_hypotheses.append(hyp)\n",
    "        \n",
    "        # Top hypothÃ¨ses par confiance\n",
    "        top_hyp = sorted(all_hypotheses, key=lambda x: x['confidence'], reverse=True)[:10]\n",
    "        \n",
    "        f.write(f\"### Top 10 HypothÃ¨ses (par confiance)\\n\")\n",
    "        for i, hyp in enumerate(top_hyp, 1):\n",
    "            f.write(f\"{i}. **{hyp['dhatu_root']}** ({hyp['aspect']}) - \")\n",
    "            f.write(f\"Confiance: {hyp['confidence']:.3f} | \")\n",
    "            f.write(f\"Langue: {hyp['language']} | \")\n",
    "            f.write(f\"Champ: {hyp['semantic_field']}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n---\\n\\n\")\n",
    "        f.write(f\"*GÃ©nÃ©rÃ© par PaniniFS Research - Analyse GPU Intensive*\\n\")\n",
    "        f.write(f\"*Fichier complet: {results_filename}*\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“„ Rapport gÃ©nÃ©rÃ©: {report_path}\")\n",
    "    \n",
    "    # Affichage taille des fichiers\n",
    "    !ls -lh colab_results/{results_filename}\n",
    "    !ls -lh colab_results/{report_filename}\n",
    "    \n",
    "    # SYNCHRONISATION GITHUB\n",
    "    print(\"\\nğŸ”„ Synchronisation avec GitHub...\")\n",
    "    \n",
    "    # Add et commit\n",
    "    !git add colab_results/{results_filename}\n",
    "    !git add colab_results/{report_filename}\n",
    "    \n",
    "    commit_message = f\"ğŸ§¬ Analyse intensive dhÄtu multi-hypothÃ¨ses - {session_id}\"\n",
    "    !git commit -m \"{commit_message}\"\n",
    "    \n",
    "    # Push vers GitHub\n",
    "    !git push origin main\n",
    "    \n",
    "    print(\"âœ… RÃ©sultats synchronisÃ©s avec GitHub!\")\n",
    "    print(\"\\nğŸ¯ ANALYSE INTENSIVE TERMINÃ‰E\")\n",
    "    print(f\"ğŸ“Š {intensive_results['processing_summary']['total_hypotheses_generated']} hypothÃ¨ses dhÄtu gÃ©nÃ©rÃ©es\")\n",
    "    print(f\"ğŸš€ Performance: {intensive_results['processing_summary']['documents_per_second']} docs/sec\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Pas de rÃ©sultats Ã  exporter\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
