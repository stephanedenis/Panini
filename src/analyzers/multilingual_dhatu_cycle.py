#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”„ Multilingual DhÄtu Cycle Analyzer
ModÃ©lisation bidirectionnelle Texte â†’ DhÄtu â†’ Texte pour restitution parfaite
"""

import re
import json
import hashlib
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class MultilingualDhatuMapping:
    """Mapping dhÄtu multilingue d'une Å“uvre"""
    work_key: str
    title: str
    author: str
    language_analyses: Dict[str, Dict]  # lang -> dhatu_scores
    cross_language_correlation: Dict[str, float]  # dhatu -> correlation_score
    text_to_dhatu_mapping: Dict[str, List[Tuple]]  # lang -> [(text_segment, dhatus)]
    dhatu_to_text_reconstruction: Dict[str, Dict]  # lang -> reconstructed_segments


class MultilingualDhatuAnalyzer:
    """Analyseur dhÄtu multilingue pour cycle texte â†’ dhÄtu â†’ texte"""
    
    def __init__(self):
        # Patterns dhÄtu Ã©tendus pour multi-langues
        self.multilingual_dhatu_patterns = {
            'RELATE': {
                'en': r'\b(with|to|from|between|among|relation|connect|link|related|together|against|through)\b',
                'fr': r'\b(avec|Ã |de|entre|parmi|relation|relier|lien|ensemble|contre|Ã _travers|par)\b',
                'de': r'\b(mit|zu|von|zwischen|unter|beziehung|verbinden|zusammen|gegen|durch)\b'
            },
            'MODAL': {
                'en': r'\b(can|could|may|might|must|should|would|will|shall|possible|necessary|able)\b',
                'fr': r'\b(peut|pourrait|pourra|doit|devrait|voudrait|sera|possible|nÃ©cessaire|capable)\b',
                'de': r'\b(kann|kÃ¶nnte|mag|mÃ¶chte|muss|sollte|wÃ¼rde|wird|mÃ¶glich|notwendig|fÃ¤hig)\b'
            },
            'EXIST': {
                'en': r'\b(is|are|was|were|be|being|been|exist|there|have|has|had|being)\b',
                'fr': r'\b(est|sont|Ã©tait|Ã©taient|Ãªtre|Ã©tant|Ã©tÃ©|exister|il_y_a|avoir|avait|ayant)\b',
                'de': r'\b(ist|sind|war|waren|sein|seiend|gewesen|existieren|gibt|haben|hatte|habend)\b'
            },
            'EVAL': {
                'en': r'\b(good|bad|better|worse|best|worst|great|terrible|wonderful|awful|excellent|poor)\b',
                'fr': r'\b(bon|mauvais|meilleur|pire|mieux|terrible|merveilleux|affreux|excellent|pauvre)\b',
                'de': r'\b(gut|schlecht|besser|schlechter|beste|schlechteste|groÃŸartig|schrecklich|wunderbar|schlimm)\b'
            },
            'COMM': {
                'en': r'\b(say|said|tell|told|speak|talk|communicate|express|word|voice|language|message)\b',
                'fr': r'\b(dire|dit|parler|parlÃ©|communiquer|exprimer|mot|voix|langue|message|raconter)\b',
                'de': r'\b(sagen|gesagt|sprechen|gesprochen|kommunizieren|ausdrÃ¼cken|wort|stimme|sprache|nachricht)\b'
            },
            'CAUSE': {
                'en': r'\b(because|since|therefore|thus|cause|reason|result|effect|so|hence|consequently)\b',
                'fr': r'\b(parce_que|depuis|donc|ainsi|cause|raison|rÃ©sultat|effet|alors|par_consÃ©quent)\b',
                'de': r'\b(weil|da|deshalb|daher|ursache|grund|ergebnis|wirkung|so|folglich|infolgedessen)\b'
            },
            'ITER': {
                'en': r'\b(again|repeat|continue|always|often|usually|every|each|once_more|repeatedly)\b',
                'fr': r'\b(encore|rÃ©pÃ©ter|continuer|toujours|souvent|habituellement|chaque|une_fois_de_plus)\b',
                'de': r'\b(wieder|wiederholen|fortsetzen|immer|oft|gewÃ¶hnlich|jede|noch_einmal|wiederholt)\b'
            },
            'DECIDE': {
                'en': r'\b(decide|choose|select|determine|resolve|conclude|judge|decision|choice|judgment)\b',
                'fr': r'\b(dÃ©cider|choisir|sÃ©lectionner|dÃ©terminer|rÃ©soudre|conclure|juger|dÃ©cision|choix)\b',
                'de': r'\b(entscheiden|wÃ¤hlen|auswÃ¤hlen|bestimmen|lÃ¶sen|schlieÃŸen|urteilen|entscheidung|wahl)\b'
            },
            'FEEL': {
                'en': r'\b(feel|felt|emotion|happy|sad|love|hate|like|dislike|joy|sorrow|fear|anger)\b',
                'fr': r'\b(sentir|senti|Ã©motion|heureux|triste|aimer|dÃ©tester|joie|chagrin|peur|colÃ¨re)\b',
                'de': r'\b(fÃ¼hlen|gefÃ¼hlt|emotion|glÃ¼cklich|traurig|lieben|hassen|freude|kummer|angst|wut)\b'
            }
        }
        
        self.dhatu_names = list(self.multilingual_dhatu_patterns.keys())
        print("ğŸ”„ Analyseur DhÄtu Multilingue initialisÃ©")
        print(f"   ğŸ§¬ {len(self.dhatu_names)} dhÄtu universaux")
        print(f"   ğŸŒ 3 langues: EN, FR, DE")
    
    def extract_text_segments(self, text: str, lang: str, segment_size: int = 1000) -> List[str]:
        """Extrait segments textuels pour analyse granulaire"""
        
        # Nettoyage prÃ©paratoire
        clean_text = self._clean_gutenberg_text(text)
        
        # Segmentation par taille ou par phrases
        segments = []
        words = clean_text.split()
        
        for i in range(0, len(words), segment_size):
            segment = ' '.join(words[i:i+segment_size])
            if len(segment.strip()) > 100:  # Ã‰viter segments trop courts
                segments.append(segment.strip())
        
        return segments
    
    def analyze_dhatu_in_segment(self, segment: str, lang: str) -> Dict[str, float]:
        """Analyse dhÄtu dans un segment textuel"""
        
        segment_lower = segment.lower()
        total_words = len(segment.split())
        
        dhatu_scores = {}
        
        for dhatu_name in self.dhatu_names:
            if lang in self.multilingual_dhatu_patterns[dhatu_name]:
                pattern = self.multilingual_dhatu_patterns[dhatu_name][lang]
                matches = re.findall(pattern, segment_lower)
                score = len(matches) / max(total_words, 1)
                dhatu_scores[dhatu_name] = round(score, 6)
            else:
                dhatu_scores[dhatu_name] = 0.0
        
        # Normaliser
        total_score = sum(dhatu_scores.values())
        if total_score > 0:
            dhatu_scores = {k: round(v / total_score, 6) for k, v in dhatu_scores.items()}
        
        return dhatu_scores
    
    def create_text_to_dhatu_mapping(self, text: str, lang: str) -> List[Tuple[str, Dict[str, float]]]:
        """CrÃ©e mapping dÃ©taillÃ© texte â†’ dhÄtu"""
        
        segments = self.extract_text_segments(text, lang)
        text_to_dhatu = []
        
        print(f"   ğŸ” Analyse {len(segments)} segments ({lang.upper()})")
        
        for i, segment in enumerate(segments):
            dhatu_scores = self.analyze_dhatu_in_segment(segment, lang)
            
            # Garder seulement segments avec scores significatifs
            significant_dhatus = {k: v for k, v in dhatu_scores.items() if v > 0.01}
            
            if significant_dhatus:
                text_to_dhatu.append((segment[:200] + "..." if len(segment) > 200 else segment, dhatu_scores))
            
            if i % 50 == 0 and i > 0:
                print(f"      ğŸ“Š {i}/{len(segments)} segments analysÃ©s")
        
        print(f"   âœ… {len(text_to_dhatu)} segments significatifs")
        return text_to_dhatu
    
    def reconstruct_text_from_dhatus(self, text_to_dhatu_mapping: List[Tuple], target_dhatu_profile: Dict[str, float], lang: str) -> List[str]:
        """Reconstruit texte Ã  partir profil dhÄtu cible"""
        
        reconstructed_segments = []
        
        # Trier segments par similaritÃ© au profil cible
        def dhatu_similarity(segment_dhatus: Dict[str, float]) -> float:
            similarity = 0.0
            for dhatu, target_score in target_dhatu_profile.items():
                segment_score = segment_dhatus.get(dhatu, 0.0)
                similarity += 1 - abs(target_score - segment_score)
            return similarity / len(target_dhatu_profile)
        
        # SÃ©lectionner segments les plus proches
        for segment_text, segment_dhatus in text_to_dhatu_mapping:
            similarity = dhatu_similarity(segment_dhatus)
            if similarity > 0.7:  # Seuil similaritÃ© Ã©levÃ©
                reconstructed_segments.append((segment_text, similarity))
        
        # Trier par similaritÃ© dÃ©croissante
        reconstructed_segments.sort(key=lambda x: x[1], reverse=True)
        
        return [seg[0] for seg in reconstructed_segments[:10]]  # Top 10
    
    def analyze_multilingual_work(self, work_dir: Path, work_key: str, metadata: Dict) -> MultilingualDhatuMapping:
        """Analyse complÃ¨te multilingue d'une Å“uvre"""
        
        work_info = metadata['works'][work_key]
        print(f"\nğŸ”„ ANALYSE MULTILINGUE: {work_info['title']}")
        print("=" * 60)
        
        language_analyses = {}
        text_to_dhatu_mappings = {}
        
        # Analyser chaque langue
        for lang, version_info in work_info['versions'].items():
            if not version_info.get('download_success', False):
                continue
                
            print(f"\nğŸ“– Langue: {lang.upper()}")
            
            # Lire fichier
            file_path = Path(version_info['file_path'])
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # CrÃ©er mapping texte â†’ dhÄtu
            text_to_dhatu = self.create_text_to_dhatu_mapping(content, lang)
            text_to_dhatu_mappings[lang] = text_to_dhatu
            
            # Calculer scores globaux dhÄtu
            all_dhatu_scores = defaultdict(list)
            for _, segment_dhatus in text_to_dhatu:
                for dhatu, score in segment_dhatus.items():
                    all_dhatu_scores[dhatu].append(score)
            
            # Moyennes dhÄtu pour cette langue
            lang_dhatu_profile = {}
            for dhatu, scores in all_dhatu_scores.items():
                lang_dhatu_profile[dhatu] = round(np.mean(scores) if scores else 0.0, 6)
            
            language_analyses[lang] = {
                'dhatu_profile': lang_dhatu_profile,
                'total_segments': len(text_to_dhatu),
                'content_length': len(content)
            }
            
            # Afficher profil dhÄtu
            sorted_dhatus = sorted(lang_dhatu_profile.items(), key=lambda x: x[1], reverse=True)
            print(f"   ğŸ§¬ Profil dhÄtu:")
            for dhatu, score in sorted_dhatus[:5]:
                percentage = score * 100
                print(f"      {dhatu:8}: {percentage:5.2f}%")
        
        # Calcul corrÃ©lations inter-langues
        cross_correlations = self._calculate_cross_language_correlations(language_analyses)
        
        # Test reconstruction dhÄtu â†’ texte
        dhatu_reconstructions = {}
        for lang in language_analyses.keys():
            if lang in text_to_dhatu_mappings:
                profile = language_analyses[lang]['dhatu_profile']
                reconstructed = self.reconstruct_text_from_dhatus(
                    text_to_dhatu_mappings[lang], profile, lang
                )
                dhatu_reconstructions[lang] = reconstructed
        
        return MultilingualDhatuMapping(
            work_key=work_key,
            title=work_info['title'],
            author=work_info['author'],
            language_analyses=language_analyses,
            cross_language_correlation=cross_correlations,
            text_to_dhatu_mapping=text_to_dhatu_mappings,
            dhatu_to_text_reconstruction=dhatu_reconstructions
        )
    
    def _calculate_cross_language_correlations(self, language_analyses: Dict) -> Dict[str, float]:
        """Calcule corrÃ©lations dhÄtu entre langues"""
        
        correlations = {}
        
        # Pour chaque dhÄtu, calculer corrÃ©lation entre langues
        for dhatu in self.dhatu_names:
            lang_scores = []
            for lang_data in language_analyses.values():
                lang_scores.append(lang_data['dhatu_profile'].get(dhatu, 0.0))
            
            # CorrÃ©lation = 1 - variance normalisÃ©e
            if len(lang_scores) > 1:
                variance = np.var(lang_scores)
                correlation = max(0, 1 - (variance * 10))  # Facteur Ã©chelle
                correlations[dhatu] = round(correlation, 4)
            else:
                correlations[dhatu] = 1.0
        
        return correlations
    
    def _clean_gutenberg_text(self, text: str) -> str:
        """Nettoyage spÃ©cialisÃ© texte Gutenberg"""
        
        # Supprimer mÃ©tadonnÃ©es Gutenberg
        patterns_to_remove = [
            r'\*\*\* START OF.*?\*\*\*',
            r'\*\*\* END OF.*?\*\*\*',
            r'Project Gutenberg.*?\n',
            r'Title:.*?\n',
            r'Author:.*?\n'
        ]
        
        cleaned = text
        for pattern in patterns_to_remove:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.DOTALL)
        
        # Nettoyage supplÃ©mentaire
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned)
        cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', cleaned)
        
        return cleaned.strip()
    
    def process_multilingual_corpus(self, corpus_dir: str = "data/gutenberg_multilingual_verified") -> Dict[str, MultilingualDhatuMapping]:
        """Traite tout le corpus multilingue"""
        
        corpus_path = Path(corpus_dir)
        metadata_file = corpus_path / 'multilingual_verified_metadata.json'
        
        if not metadata_file.exists():
            raise FileNotFoundError(f"MÃ©tadonnÃ©es manquantes: {metadata_file}")
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        print("ğŸ”„ PIPELINE CYCLE TEXTE â†’ DHÄ€TU â†’ TEXTE")
        print("=" * 60)
        
        results = {}
        
        for work_key in metadata['works']:
            try:
                mapping = self.analyze_multilingual_work(corpus_path, work_key, metadata)
                results[work_key] = mapping
            except Exception as e:
                print(f"âŒ Erreur {work_key}: {e}")
        
        # Sauvegarder rÃ©sultats complets
        output_file = corpus_path / 'multilingual_dhatu_cycle_results.json'
        self._save_results(results, output_file)
        
        print(f"\nğŸ“Š RÃ‰SULTATS PIPELINE CYCLE:")
        print(f"   ğŸ“š Å’uvres analysÃ©es: {len(results)}")
        print(f"   ğŸ’¾ RÃ©sultats: {output_file}")
        
        return results
    
    def _save_results(self, results: Dict[str, MultilingualDhatuMapping], output_file: Path):
        """Sauvegarde rÃ©sultats avec sÃ©rialisation"""
        
        serializable_results = {}
        
        for work_key, mapping in results.items():
            serializable_results[work_key] = {
                'title': mapping.title,
                'author': mapping.author,
                'language_analyses': mapping.language_analyses,
                'cross_language_correlation': mapping.cross_language_correlation,
                'dhatu_to_text_reconstruction': mapping.dhatu_to_text_reconstruction,
                'text_to_dhatu_mapping_summary': {
                    lang: len(text_dhatu_list) 
                    for lang, text_dhatu_list in mapping.text_to_dhatu_mapping.items()
                }
            }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)


def main():
    """Test pipeline cycle multilingue"""
    
    print("ğŸ”„ TEST PIPELINE CYCLE TEXTE â†’ DHÄ€TU â†’ TEXTE")
    print("=" * 60)
    
    analyzer = MultilingualDhatuAnalyzer()
    
    # Traiter corpus multilingue
    results = analyzer.process_multilingual_corpus()
    
    # Afficher rÃ©sumÃ© validation
    print(f"\nğŸ“Š VALIDATION CYCLE MULTILINGUE:")
    
    for work_key, mapping in results.items():
        print(f"\nğŸ“– {mapping.title}:")
        print(f"   ğŸŒ Langues: {', '.join(mapping.language_analyses.keys()).upper()}")
        
        # CorrÃ©lations inter-langues
        top_correlations = sorted(mapping.cross_language_correlation.items(), 
                                 key=lambda x: x[1], reverse=True)[:3]
        print(f"   ğŸ”— Top corrÃ©lations: {', '.join([f'{d}({c:.2f})' for d, c in top_correlations])}")
        
        # Reconstruction
        for lang in mapping.dhatu_to_text_reconstruction:
            reconstructed_count = len(mapping.dhatu_to_text_reconstruction[lang])
            print(f"   ğŸ”„ Reconstruction {lang.upper()}: {reconstructed_count} segments")
    
    print(f"\nâœ… PIPELINE CYCLE TERMINÃ‰")
    return results


if __name__ == "__main__":
    main()