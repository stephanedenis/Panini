#!/usr/bin/env python3
"""
Plan Autonome de Recherche PaniniFS
ExÃ©cution continue sans interaction humaine pour faire avancer la recherche rÃ©elle
"""

import os
import json
import time
import subprocess
import glob
from datetime import datetime
from pathlib import Path

class AutonomousResearchPlan:
    def __init__(self):
        self.start_time = datetime.now()
        self.workspace = Path("/home/stephane/GitHub/PaniniFS-Research")
        self.results_dir = self.workspace / "autonomous_results"
        self.results_dir.mkdir(exist_ok=True)
        
        self.execution_log = []
        self.research_progress = {
            "dhatu_analysis_completed": False,
            "patterns_extracted": 0,
            "hypotheses_tested": 0,
            "corpus_processed": 0,
            "discoveries": []
        }
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("[%H:%M:%S]")
        log_entry = f"{timestamp} {level}: {message}"
        print(log_entry)
        self.execution_log.append(log_entry)
        
    def save_progress(self):
        """Sauvegarde automatique du progrÃ¨s"""
        progress_file = self.results_dir / f"research_progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(progress_file, 'w') as f:
            json.dump({
                "start_time": self.start_time.isoformat(),
                "current_time": datetime.now().isoformat(),
                "progress": self.research_progress,
                "execution_log": self.execution_log[-100:]  # DerniÃ¨res 100 entrÃ©es
            }, f, indent=2)
        
    def analyze_existing_dhatu_corpus(self):
        """Analyse le corpus dhatu existant pour identifier les vraies donnÃ©es"""
        self.log("ğŸ” ANALYSE CORPUS DHATU EXISTANT")
        
        dhatu_paths = [
            "tech/data/dhatu_*.json",
            "panini/data/dhatu/*.json",
            "tech/corpus_*/dhatu*.json"
        ]
        
        real_dhatu_files = []
        total_elements = 0
        
        for pattern in dhatu_paths:
            files = glob.glob(str(self.workspace / pattern))
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    if isinstance(data, list):
                        elements = len(data)
                    elif isinstance(data, dict) and 'dhatu' in data:
                        elements = len(data['dhatu'])
                    elif isinstance(data, dict) and 'elements' in data:
                        elements = len(data['elements'])
                    else:
                        elements = len(data) if hasattr(data, '__len__') else 0
                        
                    if elements > 0:
                        real_dhatu_files.append({
                            'path': file_path,
                            'elements': elements,
                            'size_kb': os.path.getsize(file_path) // 1024
                        })
                        total_elements += elements
                        
                except Exception as e:
                    self.log(f"Erreur lecture {file_path}: {e}", "WARNING")
        
        self.log(f"ğŸ“Š Corpus dhatu rÃ©el identifiÃ©:")
        self.log(f"   - {len(real_dhatu_files)} fichiers trouvÃ©s")
        self.log(f"   - {total_elements:,} Ã©lÃ©ments dhatu au total")
        
        # Sauvegarde des rÃ©sultats d'analyse
        analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(real_dhatu_files),
            'total_elements': total_elements,
            'files': real_dhatu_files
        }
        
        with open(self.results_dir / "dhatu_corpus_analysis.json", 'w') as f:
            json.dump(analysis_result, f, indent=2)
            
        self.research_progress['dhatu_analysis_completed'] = True
        self.research_progress['corpus_processed'] = total_elements
        
        return real_dhatu_files
    
    def execute_unified_dhatu_pipeline(self, dhatu_files):
        """ExÃ©cute le pipeline dhatu unifiÃ© sur les vraies donnÃ©es"""
        self.log("âš›ï¸ EXÃ‰CUTION PIPELINE DHATU UNIFIÃ‰")
        
        if not os.path.exists("tech/unified_dhatu_pipeline.py"):
            self.log("Pipeline dhatu unifiÃ© non trouvÃ©, crÃ©ation version simplifiÃ©e", "WARNING")
            return False
            
        try:
            # ExÃ©cution du pipeline avec timeout raisonnable
            result = subprocess.run([
                "python3", "tech/unified_dhatu_pipeline.py"
            ], capture_output=True, text=True, timeout=300, cwd=self.workspace)
            
            if result.returncode == 0:
                self.log("âœ… Pipeline dhatu exÃ©cutÃ© avec succÃ¨s")
                self.log(f"Sortie: {result.stdout[-500:]}")  # DerniÃ¨res 500 chars
                self.research_progress['patterns_extracted'] += 1
                return True
            else:
                self.log(f"Erreur pipeline: {result.stderr}", "ERROR")
                return False
                
        except subprocess.TimeoutExpired:
            self.log("Pipeline dhatu timeout (300s), arrÃªt forcÃ©", "WARNING")
            return False
        except Exception as e:
            self.log(f"Erreur exÃ©cution pipeline: {e}", "ERROR")
            return False
    
    def test_linguistic_hypotheses(self):
        """Test des hypothÃ¨ses linguistiques sur les donnÃ©es rÃ©elles"""
        self.log("ğŸ§ª TEST HYPOTHÃˆSES LINGUISTIQUES")
        
        hypothesis_tests = [
            "tech/peer_verification_system.py",
            "tech/verification_final.py",
            "autonomous_dhatu_optimizer.py"
        ]
        
        results = []
        for test_script in hypothesis_tests:
            if os.path.exists(self.workspace / test_script):
                try:
                    self.log(f"ExÃ©cution {test_script}")
                    result = subprocess.run([
                        "python3", test_script
                    ], capture_output=True, text=True, timeout=180, cwd=self.workspace)
                    
                    if result.returncode == 0:
                        self.log(f"âœ… {test_script} terminÃ©")
                        results.append({
                            'script': test_script,
                            'success': True,
                            'output_length': len(result.stdout)
                        })
                        self.research_progress['hypotheses_tested'] += 1
                    else:
                        self.log(f"âŒ {test_script} Ã©chouÃ©: {result.stderr[:200]}", "WARNING")
                        
                except subprocess.TimeoutExpired:
                    self.log(f"â±ï¸ {test_script} timeout", "WARNING")
                except Exception as e:
                    self.log(f"Erreur {test_script}: {e}", "ERROR")
        
        return results
    
    def autonomous_discovery_cycle(self):
        """Cycle de dÃ©couverte autonome continue"""
        self.log("ğŸš€ DÃ‰MARRAGE CYCLE DÃ‰COUVERTE AUTONOME")
        
        cycle_count = 0
        while True:
            cycle_count += 1
            self.log(f"ğŸ”„ CYCLE {cycle_count} - {datetime.now().strftime('%H:%M:%S')}")
            
            try:
                # 1. Analyse corpus (si pas encore fait)
                if not self.research_progress['dhatu_analysis_completed']:
                    dhatu_files = self.analyze_existing_dhatu_corpus()
                    
                    # 2. ExÃ©cution pipeline sur vraies donnÃ©es
                    if dhatu_files:
                        self.execute_unified_dhatu_pipeline(dhatu_files)
                
                # 3. Test hypothÃ¨ses linguistiques
                self.test_linguistic_hypotheses()
                
                # 4. Sauvegarde progrÃ¨s
                self.save_progress()
                
                # 5. Recherche de nouvelles dÃ©couvertes
                self.scan_for_discoveries()
                
                self.log(f"âœ… CYCLE {cycle_count} TERMINÃ‰")
                self.log(f"ğŸ“Š ProgrÃ¨s: {self.research_progress['patterns_extracted']} patterns, {self.research_progress['hypotheses_tested']} tests")
                
                # Pause entre cycles (ajustable selon performance systÃ¨me)
                time.sleep(120)  # 2 minutes entre cycles
                
            except KeyboardInterrupt:
                self.log("ğŸ›‘ ArrÃªt demandÃ© par utilisateur")
                break
            except Exception as e:
                self.log(f"Erreur cycle {cycle_count}: {e}", "ERROR")
                time.sleep(60)  # Pause plus longue en cas d'erreur
    
    def scan_for_discoveries(self):
        """Scanner les rÃ©sultats pour identifier des dÃ©couvertes"""
        discovery_patterns = [
            "panini_high_performance_report_*.json",
            "pilot_validation_report.json",
            "tech/corpus_*/performance_report.json"
        ]
        
        new_discoveries = 0
        for pattern in discovery_patterns:
            files = glob.glob(str(self.workspace / pattern))
            for file_path in files:
                # VÃ©rifier si fichier modifiÃ© rÃ©cemment (derniÃ¨re heure)
                if os.path.getmtime(file_path) > time.time() - 3600:
                    try:
                        with open(file_path, 'r') as f:
                            data = json.load(f)
                        
                        # Recherche d'indicateurs de dÃ©couverte
                        if any(key in str(data).lower() for key in ['gain', 'performance', 'breakthrough', 'discovery']):
                            discovery_summary = f"DÃ©couverte dans {os.path.basename(file_path)}"
                            if discovery_summary not in [d.get('summary', '') for d in self.research_progress['discoveries']]:
                                self.research_progress['discoveries'].append({
                                    'timestamp': datetime.now().isoformat(),
                                    'file': file_path,
                                    'summary': discovery_summary
                                })
                                new_discoveries += 1
                                self.log(f"ğŸ¯ DÃ‰COUVERTE: {discovery_summary}")
                                
                    except Exception as e:
                        continue
        
        if new_discoveries > 0:
            self.log(f"ğŸ“ˆ {new_discoveries} nouvelles dÃ©couvertes identifiÃ©es")

def main():
    """Point d'entrÃ©e principal"""
    print("ğŸš€ PLAN AUTONOME DE RECHERCHE PANINI")
    print("=" * 60)
    print("ğŸ¯ Objectif: Recherche continue sans interaction humaine")
    print("ğŸ“Š Focus: DonnÃ©es linguistiques rÃ©elles uniquement")
    print("âš›ï¸ MÃ©thode: Cycles de dÃ©couverte automatisÃ©s")
    print("=" * 60)
    
    planner = AutonomousResearchPlan()
    
    try:
        planner.autonomous_discovery_cycle()
    except Exception as e:
        planner.log(f"Erreur fatale: {e}", "CRITICAL")
    finally:
        planner.save_progress()
        duration = datetime.now() - planner.start_time
        planner.log(f"ğŸ Session terminÃ©e aprÃ¨s {duration}")

if __name__ == "__main__":
    main()