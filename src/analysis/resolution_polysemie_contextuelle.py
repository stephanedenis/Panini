#!/usr/bin/env python3
"""
üß† ALGORITHME R√âSOLUTION POLYS√âMIE CONTEXTUELLE DHƒÄTU
Scoring contextuel IA pour choix automatique d√©composition optimale
Machine learning sur contexte s√©mantique + corpus optimis√©
"""

import json
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict
from collections import Counter

# Import syst√®me dhƒÅtu
try:
    from integration_complete_dhatu_trio import SystemeDhatuUnifie, CompositionDhatu
    from optimisation_corpus_scientifique_v2 import OptimisateurCorpusScientifique
except ImportError as e:
    print(f"‚ö†Ô∏è Erreur import: {e}")
    print("Modules dhƒÅtu requis")
    exit(1)


@dataclass
class CandidatDecomposition:
    """Candidat d√©composition avec scoring contextuel"""
    expression_originale: str
    decomposition: str
    dhatu_impliques: List[str]
    score_contextuel: float
    score_frequence: float
    score_coherence_semantique: float
    score_complexite_cognitive: float
    score_final: float
    contexte_detecte: str
    justification: str


@dataclass
class AnalysePolysemie:
    """Analyse compl√®te polys√©mie d'une expression"""
    expression: str
    nb_candidats: int
    candidat_optimal: CandidatDecomposition
    candidats_alternatifs: List[CandidatDecomposition]
    confiance_resolution: float
    contexte_discriminant: List[str]
    type_polysemie: str  # simple, complexe, ambigue


class ResolveurPolysemieContextuelle:
    """R√©solveur IA polys√©mie dhƒÅtu avec scoring contextuel avanc√©"""
    
    def __init__(self):
        self.systeme_dhatu = SystemeDhatuUnifie()
        
        # Base de connaissances contextuelles
        self.contextes_discriminants = {
            # Contextes MODAL
            "modal_epistemique": [
                "recherche", "√©tude", "analyse", "donn√©es", "r√©sultats",
                "semble", "para√Æt", "sugg√®re", "indique", "montre"
            ],
            "modal_deontique": [
                "r√®glement", "loi", "obligation", "interdit", "permis",
                "doit", "faut", "n√©cessaire", "requis", "autoris√©"
            ],
            "modal_dynamique": [
                "capacit√©", "comp√©tence", "aptitude", "pouvoir faire",
                "capable", "incapable", "savoir faire", "ma√Ætriser"
            ],
            
            # Contextes ASPECT  
            "aspect_inceptif": [
                "commencer", "d√©buter", "entamer", "initier", "d√©marrer",
                "premier", "initial", "nouveau", "inaugurer"
            ],
            "aspect_progressif": [
                "continuer", "poursuivre", "en cours", "en train",
                "actuellement", "maintenant", "progression", "d√©veloppement"
            ],
            "aspect_conclusif": [
                "terminer", "finir", "achever", "conclure", "finaliser",
                "dernier", "final", "aboutir", "compl√©ter"
            ],
            
            # Contextes QUANT
            "quant_cardinale": [
                "nombre", "quantit√©", "total", "somme", "comptage",
                "exactement", "pr√©cis√©ment", "combien", "chiffre"
            ],
            "quant_approximative": [
                "environ", "approximativement", "vers", "pr√®s de",
                "√† peu pr√®s", "grosso modo", "autour de"
            ],
            "quant_comparative": [
                "plus", "moins", "autant", "davantage", "sup√©rieur",
                "inf√©rieur", "comparaison", "relatif", "proportionnel"
            ]
        }
        
        # Patterns polys√©mie fr√©quents
        self.patterns_polysemie = {
            # Expressions multi-dhƒÅtu communes
            "beaucoup_probable": {
                "candidats": ["QUANT++ + MODAL+", "MODAL++ + QUANT+"],
                "contextes_discriminants": ["tr√®s", "assez", "vraiment"]
            },
            "commence_peut_√™tre": {
                "candidats": ["ASPECT+¬∑ + MODAL?", "MODAL? + ASPECT+¬∑"],
                "contextes_discriminants": ["processus", "action", "√©volution"]
            },
            "certainement_nombreux": {
                "candidats": ["MODAL+ + QUANT++", "QUANT++ + MODAL+"],
                "contextes_discriminants": ["√©vidence", "donn√©es", "constats"]
            }
        }
        
        # Weights scoring (r√©glables)
        self.weights_scoring = {
            "contexte": 0.40,      # Contexte s√©mantique
            "frequence": 0.25,     # Fr√©quence usage
            "coherence": 0.20,     # Coh√©rence s√©mantique
            "complexite": 0.15     # Simplicit√© cognitive
        }
        
        # Statistiques apprentissage
        self.stats_apprentissage = {
            "expressions_analysees": 0,
            "resolutions_reussies": 0,
            "polysemies_detectees": 0,
            "contextes_discriminants_decouverts": []
        }
        
        # Cache d√©compositions fr√©quentes
        self.cache_decompositions = {}
    
    def analyser_contexte_semantique(self, expression: str, contexte: str) -> Dict[str, float]:
        """Analyser contexte s√©mantique pour scoring"""
        
        scores_contexte = {
            "modal_epistemique": 0.0,
            "modal_deontique": 0.0, 
            "modal_dynamique": 0.0,
            "aspect_inceptif": 0.0,
            "aspect_progressif": 0.0,
            "aspect_conclusif": 0.0,
            "quant_cardinale": 0.0,
            "quant_approximative": 0.0,
            "quant_comparative": 0.0
        }
        
        # Analyse contexte complet (expression + environnement)
        texte_analyse = f"{expression} {contexte}".lower()
        
        # Scoring par type contexte
        for type_contexte, marqueurs in self.contextes_discriminants.items():
            score = 0.0
            for marqueur in marqueurs:
                if marqueur in texte_analyse:
                    score += 1.0
                # Proximit√© bonus (marqueur proche expression)
                if marqueur in contexte.lower():
                    distance = self._calculer_distance_mots(expression, marqueur, contexte)
                    if distance < 5:  # Dans les 5 mots
                        score += 0.5
            
            # Normalisation
            scores_contexte[type_contexte] = min(1.0, score / len(marqueurs))
        
        return scores_contexte
    
    def _calculer_distance_mots(self, expr1: str, expr2: str, texte: str) -> int:
        """Calculer distance en mots entre deux expressions"""
        mots = texte.lower().split()
        try:
            pos1 = next(i for i, mot in enumerate(mots) if expr1.lower() in mot)
            pos2 = next(i for i, mot in enumerate(mots) if expr2.lower() in mot)
            return abs(pos1 - pos2)
        except StopIteration:
            return 999  # Tr√®s loin si pas trouv√©
    
    def generer_candidats_decomposition(self, expression: str) -> List[CandidatDecomposition]:
        """G√©n√©rer candidats d√©composition pour expression polys√®me"""
        
        candidats = []
        
        # M√©thode 1: Syst√®me dhƒÅtu unifi√©
        composition_primaire = self.systeme_dhatu.analyser_expression_complete(expression)
        if composition_primaire and composition_primaire.validite_cognitive:
            candidat = CandidatDecomposition(
                expression_originale=expression,
                decomposition=composition_primaire.decomposition_complete,
                dhatu_impliques=composition_primaire.dhatu_impliques,
                score_contextuel=0.0,  # √Ä calculer
                score_frequence=0.0,
                score_coherence_semantique=composition_primaire.score_expressivite,
                score_complexite_cognitive=1.0 - (composition_primaire.niveau_complexite.value / 7.0),
                score_final=0.0,
                contexte_detecte="systeme_unifie",
                justification="D√©composition syst√®me unifi√© principal"
            )
            candidats.append(candidat)
        
        # M√©thode 2: Patterns polys√©mie connus
        for pattern_nom, pattern_info in self.patterns_polysemie.items():
            if self._expression_match_pattern(expression, pattern_nom):
                for decomp_candidate in pattern_info["candidats"]:
                    candidat = CandidatDecomposition(
                        expression_originale=expression,
                        decomposition=decomp_candidate,
                        dhatu_impliques=self._extraire_dhatu_decomposition(decomp_candidate),
                        score_contextuel=0.0,
                        score_frequence=0.7,  # Pattern connu = fr√©quent
                        score_coherence_semantique=0.8,  # Pattern valid√©
                        score_complexite_cognitive=0.6,  # Complexit√© moyenne
                        score_final=0.0,
                        contexte_detecte="pattern_connu",
                        justification=f"Pattern polys√©mie: {pattern_nom}"
                    )
                    candidats.append(candidat)
        
        # M√©thode 3: D√©compositions dhƒÅtu individuels
        candidats_individuels = self._generer_candidats_individuels(expression)
        candidats.extend(candidats_individuels)
        
        # D√©duplication
        candidats_uniques = self._dedupliquer_candidats(candidats)
        
        return candidats_uniques
    
    def _expression_match_pattern(self, expression: str, pattern: str) -> bool:
        """V√©rifier si expression match pattern polys√©mie"""
        # Matching simple bas√© sur mots-cl√©s
        mots_expression = set(expression.lower().split())
        mots_pattern = set(pattern.replace("_", " ").split())
        
        intersection = mots_expression.intersection(mots_pattern)
        return len(intersection) >= len(mots_pattern) // 2
    
    def _extraire_dhatu_decomposition(self, decomposition: str) -> List[str]:
        """Extraire dhƒÅtu impliqu√©s dans d√©composition"""
        dhatu_trouves = []
        if "MODAL" in decomposition:
            dhatu_trouves.append("MODAL")
        if "ASPECT" in decomposition:
            dhatu_trouves.append("ASPECT")
        if "QUANT" in decomposition:
            dhatu_trouves.append("QUANT")
        return dhatu_trouves
    
    def _generer_candidats_individuels(self, expression: str) -> List[CandidatDecomposition]:
        """G√©n√©rer candidats par dhƒÅtu individuels"""
        candidats = []
        
        # Test MODAL
        result_modal = self.systeme_dhatu.modal_dhatu.analyser_expression(expression)
        if result_modal:
            candidat = self._creer_candidat_individuel(expression, result_modal, "MODAL")
            candidats.append(candidat)
        
        # Test ASPECT
        result_aspect = self.systeme_dhatu.aspect_dhatu.analyser_expression_aspectuelle(expression)
        if result_aspect:
            candidat = self._creer_candidat_individuel(expression, result_aspect, "ASPECT")
            candidats.append(candidat)
        
        # Test QUANT
        result_quant = self.systeme_dhatu.quant_dhatu.analyser_expression_quantitative(expression)
        if result_quant:
            candidat = self._creer_candidat_individuel(expression, result_quant, "QUANT")
            candidats.append(candidat)
        
        return candidats
    
    def _creer_candidat_individuel(self, expression: str, resultat_dhatu: any, dhatu_type: str) -> CandidatDecomposition:
        """Cr√©er candidat √† partir r√©sultat dhƒÅtu individuel"""
        return CandidatDecomposition(
            expression_originale=expression,
            decomposition=resultat_dhatu.decomposition,
            dhatu_impliques=[dhatu_type],
            score_contextuel=0.0,
            score_frequence=0.5,  # Moyenne
            score_coherence_semantique=0.7,  # DhƒÅtu valid√©
            score_complexite_cognitive=0.8,  # Simple (1 dhƒÅtu)
            score_final=0.0,
            contexte_detecte=f"dhatu_{dhatu_type.lower()}",
            justification=f"DhƒÅtu {dhatu_type} individuel"
        )
    
    def _dedupliquer_candidats(self, candidats: List[CandidatDecomposition]) -> List[CandidatDecomposition]:
        """D√©duplication candidats similaires"""
        candidats_uniques = []
        decompositions_vues = set()
        
        for candidat in candidats:
            if candidat.decomposition not in decompositions_vues:
                candidats_uniques.append(candidat)
                decompositions_vues.add(candidat.decomposition)
        
        return candidats_uniques
    
    def scorer_candidats_contextuels(self, candidats: List[CandidatDecomposition], 
                                   expression: str, contexte: str) -> List[CandidatDecomposition]:
        """Scorer candidats selon contexte s√©mantique"""
        
        # Analyse contexte global
        scores_contexte = self.analyser_contexte_semantique(expression, contexte)
        
        candidats_scores = []
        for candidat in candidats:
            # Score contextuel bas√© sur dhƒÅtu impliqu√©s
            score_ctx = 0.0
            for dhatu in candidat.dhatu_impliques:
                if dhatu == "MODAL":
                    score_ctx += max(scores_contexte["modal_epistemique"],
                                   scores_contexte["modal_deontique"],
                                   scores_contexte["modal_dynamique"])
                elif dhatu == "ASPECT":
                    score_ctx += max(scores_contexte["aspect_inceptif"],
                                   scores_contexte["aspect_progressif"], 
                                   scores_contexte["aspect_conclusif"])
                elif dhatu == "QUANT":
                    score_ctx += max(scores_contexte["quant_cardinale"],
                                   scores_contexte["quant_approximative"],
                                   scores_contexte["quant_comparative"])
            
            # Normalisation par nombre dhƒÅtu
            if candidat.dhatu_impliques:
                score_ctx /= len(candidat.dhatu_impliques)
            
            # Mise √† jour candidat
            candidat.score_contextuel = score_ctx
            
            # Score final pond√©r√©
            candidat.score_final = (
                candidat.score_contextuel * self.weights_scoring["contexte"] +
                candidat.score_frequence * self.weights_scoring["frequence"] +
                candidat.score_coherence_semantique * self.weights_scoring["coherence"] +
                candidat.score_complexite_cognitive * self.weights_scoring["complexite"]
            )
            
            candidats_scores.append(candidat)
        
        # Tri par score final
        candidats_scores.sort(key=lambda x: x.score_final, reverse=True)
        
        return candidats_scores
    
    def resoudre_polysemie(self, expression: str, contexte: str = "") -> AnalysePolysemie:
        """R√©solution compl√®te polys√©mie avec scoring contextuel"""
        
        # G√©n√©ration candidats
        candidats = self.generer_candidats_decomposition(expression)
        
        if not candidats:
            # Aucun candidat trouv√©
            return AnalysePolysemie(
                expression=expression,
                nb_candidats=0,
                candidat_optimal=None,
                candidats_alternatifs=[],
                confiance_resolution=0.0,
                contexte_discriminant=[],
                type_polysemie="non_resolu"
            )
        
        # Scoring contextuel
        candidats_scores = self.scorer_candidats_contextuels(candidats, expression, contexte)
        
        # S√©lection optimal
        candidat_optimal = candidats_scores[0]
        candidats_alternatifs = candidats_scores[1:5]  # Top 5 alternatives
        
        # Calcul confiance
        if len(candidats_scores) > 1:
            diff_scores = candidat_optimal.score_final - candidats_scores[1].score_final
            confiance = min(1.0, diff_scores * 2)  # Facteur 2 pour amplifier
        else:
            confiance = candidat_optimal.score_final
        
        # Type polys√©mie
        if len(candidats_scores) == 1:
            type_polysemie = "simple"
        elif confiance > 0.7:
            type_polysemie = "resolu_confiance"
        elif confiance > 0.4:
            type_polysemie = "resolu_incertain"
        else:
            type_polysemie = "ambigue"
        
        # Contexte discriminant
        scores_ctx = self.analyser_contexte_semantique(expression, contexte)
        contexte_discriminant = [ctx for ctx, score in scores_ctx.items() if score > 0.3]
        
        # Mise √† jour statistiques
        self.stats_apprentissage["expressions_analysees"] += 1
        if confiance > 0.5:
            self.stats_apprentissage["resolutions_reussies"] += 1
        if len(candidats_scores) > 1:
            self.stats_apprentissage["polysemies_detectees"] += 1
        
        return AnalysePolysemie(
            expression=expression,
            nb_candidats=len(candidats_scores),
            candidat_optimal=candidat_optimal,
            candidats_alternatifs=candidats_alternatifs,
            confiance_resolution=confiance,
            contexte_discriminant=contexte_discriminant,
            type_polysemie=type_polysemie
        )
    
    def tester_resolution_polysemie_massive(self, expressions_test: List[Tuple[str, str]]) -> Dict:
        """Test r√©solution polys√©mie sur corpus d'expressions"""
        
        resultats = []
        stats_globales = {
            "total_expressions": len(expressions_test),
            "resolutions_reussies": 0,
            "polysemies_detectees": 0,
            "confiance_moyenne": 0.0,
            "types_polysemie": Counter()
        }
        
        print(f"üß† Test r√©solution polys√©mie: {len(expressions_test)} expressions")
        
        for i, (expression, contexte) in enumerate(expressions_test):
            if i % 20 == 0:
                print(f"   Progression: {i}/{len(expressions_test)}")
            
            analyse = self.resoudre_polysemie(expression, contexte)
            resultats.append(asdict(analyse))
            
            # Stats
            if analyse.confiance_resolution > 0.5:
                stats_globales["resolutions_reussies"] += 1
            if analyse.nb_candidats > 1:
                stats_globales["polysemies_detectees"] += 1
            
            stats_globales["confiance_moyenne"] += analyse.confiance_resolution
            stats_globales["types_polysemie"][analyse.type_polysemie] += 1
        
        # Moyennes
        if stats_globales["total_expressions"] > 0:
            stats_globales["confiance_moyenne"] /= stats_globales["total_expressions"]
        
        # Calcul taux r√©ussite
        stats_globales["taux_reussite"] = (stats_globales["resolutions_reussies"] / 
                                         stats_globales["total_expressions"]) * 100
        
        stats_globales["taux_polysemie"] = (stats_globales["polysemies_detectees"] / 
                                          stats_globales["total_expressions"]) * 100
        
        return {
            "statistiques_globales": stats_globales,
            "resultats_detailles": resultats[:50],  # Sample
            "stats_apprentissage": self.stats_apprentissage
        }

def main():
    """Test algorithme r√©solution polys√©mie contextuelle"""
    print("üß† ALGORITHME R√âSOLUTION POLYS√âMIE CONTEXTUELLE DHƒÄTU")
    print("Scoring contextuel IA + Machine learning s√©mantique")
    print("="*60)
    
    # Initialisation r√©solveur
    resolveur = ResolveurPolysemieContextuelle()
    
    # Expressions test polys√©miques
    expressions_test = [
        # Polys√©mie MODAL/QUANT
        ("probablement beaucoup", "Dans cette recherche scientifique"),
        ("tr√®s probable", "Les donn√©es sugg√®rent que c'est"),
        ("certainement nombreux", "Les participants √† l'√©tude sont"),
        
        # Polys√©mie ASPECT/MODAL  
        ("commence peut-√™tre", "Le processus d'analyse"),
        ("va certainement finir", "Le projet de recherche"),
        ("pourrait continuer", "Cette tendance observ√©e"),
        
        # Polys√©mie ASPECT/QUANT
        ("de plus en plus nombreux", "Les cas observ√©s deviennent"),
        ("plusieurs fois commencer", "Il faut"),
        ("beaucoup progresser", "Les r√©sultats montrent qu'il faut"),
        
        # Polys√©mie complexe tri-dhƒÅtu
        ("va probablement beaucoup augmenter", "Le nombre de cas"),
        ("devrait certainement √©norm√©ment diminuer", "La fr√©quence d'erreur"),
        
        # Expressions simples (contr√¥le)
        ("impossible", "Il est"),
        ("beaucoup", "Il y en a"),
        ("commencer", "Il faut"),
        
        # Expressions ambigu√´s
        ("peut beaucoup", "Il"),
        ("tr√®s finir", "C'est"),
        ("probablement commencer peut-√™tre", "Il va")
    ]
    
    print(f"üìä Test sur {len(expressions_test)} expressions polys√©miques")
    
    # Test r√©solution massive
    resultats = resolveur.tester_resolution_polysemie_massive(expressions_test)
    
    # Affichage r√©sultats
    stats = resultats["statistiques_globales"]
    print(f"\nüìà R√âSULTATS R√âSOLUTION POLYS√âMIE:")
    print("="*35)
    print(f"‚úÖ Expressions test√©es: {stats['total_expressions']}")
    print(f"‚úÖ R√©solutions r√©ussies: {stats['resolutions_reussies']} ({stats['taux_reussite']:.1f}%)")
    print(f"‚úÖ Polys√©mies d√©tect√©es: {stats['polysemies_detectees']} ({stats['taux_polysemie']:.1f}%)")
    print(f"‚úÖ Confiance moyenne: {stats['confiance_moyenne']:.3f}")
    
    print(f"\nüìä TYPES POLYS√âMIE:")
    print("="*18)
    for type_poly, count in stats["types_polysemie"].items():
        pourcentage = (count / stats['total_expressions']) * 100
        print(f"   {type_poly}: {count} ({pourcentage:.1f}%)")
    
    # Exemples r√©ussites
    print(f"\nüéØ EXEMPLES R√âSOLUTIONS R√âUSSIES:")
    print("="*33)
    exemples_reussis = [r for r in resultats["resultats_detailles"] 
                       if r['confiance_resolution'] > 0.7][:5]
    
    for exemple in exemples_reussis:
        expr = exemple['expression']
        decomp = exemple['candidat_optimal']['decomposition']
        conf = exemple['confiance_resolution']
        print(f"‚úÖ '{expr}' ‚Üí {decomp} (confiance: {conf:.2f})")
    
    # Sauvegarde
    fichier_resultats = "resolution_polysemie_contextuelle_resultats.json"
    with open(fichier_resultats, "w", encoding="utf-8") as f:
        json.dump(resultats, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ R√©sultats sauvegard√©s: {fichier_resultats}")
    
    # Validation objectifs
    taux_reussite = stats['taux_reussite']
    objectif_resolution = 70.0  # 70% r√©solutions r√©ussies
    
    print(f"\nüéä VALIDATION ALGORITHME POLYS√âMIE:")
    print("="*35)
    print(f"üéØ Taux r√©ussite: {taux_reussite:.1f}%")
    print(f"üéØ Objectif: {objectif_resolution}%")
    print(f"üéØ Status: {'‚úÖ OBJECTIF ATTEINT' if taux_reussite >= objectif_resolution else '‚ö†Ô∏è √Ä am√©liorer'}")
    
    if taux_reussite >= objectif_resolution:
        print(f"\nüöÄ ALGORITHME POLYS√âMIE OP√âRATIONNEL!")
        print("Pr√™t pour int√©gration production syst√®me dhƒÅtu")
    else:
        print(f"\n‚ö†Ô∏è Optimisation recommand√©e:")
        print("- Enrichir contextes discriminants")
        print("- Ajuster weights scoring")
        print("- √âtendre patterns polys√©mie")
    
    return resolveur, resultats

if __name__ == "__main__":
    resolveur, resultats = main()