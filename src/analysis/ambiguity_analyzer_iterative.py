#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analyseur d'Ambigu√Øt√©s Multilingues - Optimisation DhƒÅtu It√©rative
Objectif: Atteindre 100% de fid√©lit√© par raffinement successif des atomes/mol√©cules

M√©thodologie d'It√©ration:
1. Analyser tous les √©checs de reconstitution sur le corpus complet
2. Identifier patterns d'ambigu√Øt√©s cross-linguistiques 
3. Factoriser les concepts mal captur√©s
4. Proposer nouveaux dhƒÅtu/mol√©cules pour combler gaps
5. Tester hypoth√®ses alternatives d'interpr√©tation
6. Conserver toutes les interpr√©tations valides
7. It√©rer jusqu'√† 100% fid√©lit√©
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Any, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, Counter

# Import modules
sys.path.append(str(Path(__file__).parent / "scripts"))
from optimal_dhatu_analyzer import OptimalDhatuAnalyzer

@dataclass
class ConceptualAmbiguity:
    """Repr√©sente une ambigu√Øt√© conceptuelle d√©tect√©e"""
    concept_id: str
    source_text: str
    source_lang: str
    target_lang: str
    authentic_target: str
    missing_dhatu: List[str]
    interpretation_hypotheses: List[Dict[str, Any]]
    confidence_scores: Dict[str, float]
    cross_linguistic_variants: Dict[str, str]  # lang -> variant
    
@dataclass
class DhatuMolecule:
    """Mol√©cule dhƒÅtu - combinaison d'atomes pour concepts complexes"""
    molecule_id: str
    component_dhatu: List[str]
    molecular_concept: str
    linguistic_patterns: Dict[str, List[str]]  # lang -> patterns
    semantic_weight: float
    interaction_rules: List[str]
    
@dataclass
class IterationResult:
    """R√©sultat d'une it√©ration d'optimisation"""
    iteration_number: int
    corpus_fidelity_before: float
    corpus_fidelity_after: float
    new_dhatu_proposed: List[str]
    new_molecules_created: List[DhatuMolecule]
    ambiguities_resolved: List[ConceptualAmbiguity]
    ambiguities_discovered: List[ConceptualAmbiguity]
    convergence_metrics: Dict[str, float]


class MultilingualAmbiguityAnalyzer:
    """Analyseur d'ambigu√Øt√©s pour optimisation it√©rative dhƒÅtu"""
    
    def __init__(self):
        self.corpus_file = Path(__file__).parent / "corpus_children_literature" / "corpus_pilot.json"
        self.iterations_log = []
        self.discovered_ambiguities = []
        self.dhatu_molecules = []
        self.interpretation_hypotheses = defaultdict(list)
        
        # DhƒÅtu actuels (baseline)
        self.current_dhatu = {
            'EXIST', 'COMM', 'TRANS', 'DECIDE', 'EVAL', 
            'GROUP', 'ITER', 'LOCATE', 'SEQ'
        }
        
        # Patterns d'ambigu√Øt√©s cross-linguistiques connus
        self.known_ambiguity_patterns = {
            'aspectual_ambiguity': {
                'description': 'Ambigu√Øt√© aspectuelle (perfectif/imperfectif)',
                'examples': {
                    'fr': 'courut (perfectif) vs courait (imperfectif)',
                    'en': 'ran (ambiguous aspect)',
                    'de': 'lief (perfectif) vs lief gerade (progressif)'
                }
            },
            'modal_ambiguity': {
                'description': 'Modalit√© (possibilit√©/obligation/volont√©)',
                'examples': {
                    'fr': 'doit (obligation/probabilit√©)',
                    'en': 'must (obligation) vs might (possibility)',
                    'de': 'muss (obligation) vs k√∂nnte (possibility)'
                }
            },
            'evidential_ambiguity': {
                'description': 'Source de l\'information (direct/rapport√©)',
                'examples': {
                    'fr': 'dit-on (rapport√©) vs voit (direct)',
                    'en': 'apparently vs clearly',
                    'de': 'angeblich vs offensichtlich'
                }
            }
        }
        
        # Seuils de convergence
        self.convergence_thresholds = {
            'target_fidelity': 1.00,           # 100% fid√©lit√©
            'min_improvement': 0.05,           # 5% am√©lioration minimum par it√©ration
            'max_iterations': 10,              # Maximum 10 it√©rations
            'ambiguity_resolution_rate': 0.80   # 80% ambigu√Øt√©s r√©solues par cycle
        }
    
    def load_corpus_with_failures(self) -> List[Dict[str, Any]]:
        """Charge le corpus et analyse tous les √©checs actuels"""
        
        print("üìö Chargement corpus et analyse √©checs...")
        
        with open(self.corpus_file, 'r', encoding='utf-8') as f:
            corpus_data = json.load(f)
        
        # Import de l'analyseur de reconstitution
        sys.path.append(str(Path(__file__).parent))
        from reconstitution_analyzer import ReconstitutionAnalyzer
        
        reconstitution_analyzer = ReconstitutionAnalyzer()
        
        # Analyser tous les √©checs
        failure_analysis = []
        language_pairs = [('fr', 'en'), ('en', 'fr'), ('fr', 'de'), ('de', 'fr'), ('en', 'de'), ('de', 'en')]
        
        for text in corpus_data['texts']:
            text_failures = {
                'text_id': text['id'],
                'title': text['title'],
                'domain': text['domain'],
                'language_pair_results': {}
            }
            
            for source_lang, target_lang in language_pairs:
                if source_lang in text['versions'] and target_lang in text['versions']:
                    try:
                        result = reconstitution_analyzer.run_reconstitution_test(
                            text['id'], source_lang, target_lang
                        )
                        
                        # Identifier √©checs sp√©cifiques
                        failure_patterns = self._analyze_reconstitution_failure(result)
                        
                        text_failures['language_pair_results'][f"{source_lang}-{target_lang}"] = {
                            'fidelity_score': result.fidelity_scores['global_fidelity'],
                            'failure_patterns': failure_patterns,
                            'missing_concepts': self._extract_missing_concepts(result),
                            'ambiguities': self._detect_ambiguities(result)
                        }
                        
                    except Exception as e:
                        print(f"   ‚ùå Erreur {text['id']} {source_lang}‚Üí{target_lang}: {e}")
            
            failure_analysis.append(text_failures)
        
        return failure_analysis
    
    def analyze_conceptual_gaps(self, failure_analysis: List[Dict]) -> List[ConceptualAmbiguity]:
        """Analyse les gaps conceptuels pour identifier ambigu√Øt√©s"""
        
        print("\nüîç Analyse des gaps conceptuels...")
        
        discovered_ambiguities = []
        concept_frequency = Counter()
        cross_linguistic_patterns = defaultdict(set)
        
        for text_analysis in failure_analysis:
            text_id = text_analysis['text_id']
            
            for pair, result in text_analysis['language_pair_results'].items():
                source_lang, target_lang = pair.split('-')
                
                # Analyser chaque concept manquant
                for missing_concept in result['missing_concepts']:
                    concept_frequency[missing_concept] += 1
                    cross_linguistic_patterns[missing_concept].add((source_lang, target_lang))
                
                # Analyser ambigu√Øt√©s d√©tect√©es
                for ambiguity in result['ambiguities']:
                    # Cr√©er hypoth√®ses d'interpr√©tation
                    hypotheses = self._generate_interpretation_hypotheses(
                        ambiguity, source_lang, target_lang
                    )
                    
                    conceptual_ambiguity = ConceptualAmbiguity(
                        concept_id=f"{text_id}_{ambiguity['type']}_{pair}",
                        source_text=ambiguity['source_context'],
                        source_lang=source_lang,
                        target_lang=target_lang,
                        authentic_target=ambiguity['target_context'],
                        missing_dhatu=ambiguity['missing_dhatu'],
                        interpretation_hypotheses=hypotheses,
                        confidence_scores=ambiguity['confidence_scores'],
                        cross_linguistic_variants=ambiguity['variants']
                    )
                    
                    discovered_ambiguities.append(conceptual_ambiguity)
        
        # Prioriser par fr√©quence et impact cross-linguistique
        prioritized_ambiguities = self._prioritize_ambiguities(
            discovered_ambiguities, concept_frequency, cross_linguistic_patterns
        )
        
        return prioritized_ambiguities
    
    def propose_dhatu_refinements(self, ambiguities: List[ConceptualAmbiguity]) -> Tuple[List[str], List[DhatuMolecule]]:
        """Propose nouveaux dhƒÅtu et mol√©cules pour r√©soudre ambigu√Øt√©s"""
        
        print("\n‚öõÔ∏è Proposition raffinements dhƒÅtu...")
        
        new_dhatu_candidates = []
        new_molecules = []
        
        # Grouper ambigu√Øt√©s par type conceptuel
        ambiguity_clusters = self._cluster_ambiguities_by_concept(ambiguities)
        
        for cluster_type, cluster_ambiguities in ambiguity_clusters.items():
            
            if cluster_type == 'aspectual':
                # Proposer dhƒÅtu aspectuels
                aspect_dhatu = {
                    'PERF': 'Action perfectif (compl√©t√©e)',
                    'PROG': 'Action progressive (en cours)', 
                    'ITER_PERF': 'Action it√©rative perfectif',
                    'STAT': '√âtat statique vs dynamique'
                }
                new_dhatu_candidates.extend(aspect_dhatu.keys())
                
                # Cr√©er mol√©cules aspectuelles
                aspect_molecule = DhatuMolecule(
                    molecule_id='ASPECT_TEMPORAL',
                    component_dhatu=['PERF', 'PROG', 'ITER'],
                    molecular_concept='Aspect temporel complexe',
                    linguistic_patterns={
                        'fr': ['pass√© simple + imparfait', '√©tait en train de'],
                        'en': ['progressive + perfect', 'was doing'],
                        'de': ['Perfekt + Pr√§teritum', 'war dabei zu']
                    },
                    semantic_weight=0.85,
                    interaction_rules=[
                        'PERF + TRANS = action completed movement',
                        'PROG + EVAL = ongoing evaluation',
                        'STAT + EXIST = state of being'
                    ]
                )
                new_molecules.append(aspect_molecule)
            
            elif cluster_type == 'modal':
                # Proposer dhƒÅtu modaux
                modal_dhatu = {
                    'POSS': 'Possibilit√©/capacit√©',
                    'OBLIG': 'Obligation/n√©cessit√©',
                    'VOLI': 'Volition/d√©sir',
                    'PERM': 'Permission/autorisation'
                }
                new_dhatu_candidates.extend(modal_dhatu.keys())
                
                # Mol√©cule modale
                modal_molecule = DhatuMolecule(
                    molecule_id='MODAL_COMPLEX',
                    component_dhatu=['POSS', 'OBLIG', 'VOLI'],
                    molecular_concept='Modalit√© complexe',
                    linguistic_patterns={
                        'fr': ['pouvoir/devoir/vouloir + infinitif'],
                        'en': ['can/must/want + to + verb'],
                        'de': ['k√∂nnen/m√ºssen/wollen + infinitiv']
                    },
                    semantic_weight=0.90,
                    interaction_rules=[
                        'POSS + TRANS = possible movement',
                        'OBLIG + EVAL = must evaluate',
                        'VOLI + DECIDE = want to decide'
                    ]
                )
                new_molecules.append(modal_molecule)
            
            elif cluster_type == 'evidential':
                # Proposer dhƒÅtu √©videntiels
                evidential_dhatu = {
                    'DIRECT': '√âvidence directe (vu/entendu)',
                    'REPORT': '√âvidence rapport√©e (on dit)',
                    'INFER': '√âvidence inf√©r√©e (d√©duction)',
                    'ASSUME': '√âvidence assum√©e (supposition)'
                }
                new_dhatu_candidates.extend(evidential_dhatu.keys())
        
        return new_dhatu_candidates, new_molecules
    
    def test_hypotheses_iteratively(self, new_dhatu: List[str], new_molecules: List[DhatuMolecule]) -> IterationResult:
        """Teste les nouvelles hypoth√®ses et mesure am√©lioration"""
        
        print(f"\nüß™ Test it√©ratif hypoth√®ses: {len(new_dhatu)} dhƒÅtu + {len(new_molecules)} mol√©cules...")
        
        # Sauvegarder √©tat actuel
        baseline_fidelity = self._measure_corpus_fidelity()
        
        # Tester chaque hypoth√®se individuellement
        best_improvements = []
        
        for dhatu in new_dhatu:
            improvement = self._test_single_dhatu_addition(dhatu)
            if improvement['fidelity_gain'] > 0.02:  # Seuil 2% minimum
                best_improvements.append(improvement)
        
        for molecule in new_molecules:
            improvement = self._test_molecule_addition(molecule)
            if improvement['fidelity_gain'] > 0.03:  # Seuil 3% pour mol√©cules
                best_improvements.append(improvement)
        
        # S√©lectionner meilleures am√©liorations compatibles
        compatible_improvements = self._select_compatible_improvements(best_improvements)
        
        # Appliquer am√©liorations et mesurer r√©sultat final
        final_fidelity = self._apply_improvements_and_measure(compatible_improvements)
        
        iteration_result = IterationResult(
            iteration_number=len(self.iterations_log) + 1,
            corpus_fidelity_before=baseline_fidelity,
            corpus_fidelity_after=final_fidelity,
            new_dhatu_proposed=[imp['dhatu'] for imp in compatible_improvements if 'dhatu' in imp],
            new_molecules_created=[imp['molecule'] for imp in compatible_improvements if 'molecule' in imp],
            ambiguities_resolved=self._count_resolved_ambiguities(compatible_improvements),
            ambiguities_discovered=self._discover_new_ambiguities(final_fidelity),
            convergence_metrics={
                'fidelity_improvement': final_fidelity - baseline_fidelity,
                'convergence_rate': (final_fidelity - baseline_fidelity) / (1.0 - baseline_fidelity),
                'remaining_gap': 1.0 - final_fidelity
            }
        )
        
        return iteration_result
    
    def run_iterative_optimization(self) -> List[IterationResult]:
        """Lance l'optimisation it√©rative compl√®te vers 100%"""
        
        print("üéØ OPTIMISATION IT√âRATIVE DHƒÄTU VERS 100% FID√âLIT√â")
        print("=" * 60)
        
        current_fidelity = 0.125  # Baseline du test initial
        iteration_count = 0
        
        while (current_fidelity < self.convergence_thresholds['target_fidelity'] and 
               iteration_count < self.convergence_thresholds['max_iterations']):
            
            iteration_count += 1
            print(f"\nüîÑ IT√âRATION {iteration_count}")
            print(f"   Fid√©lit√© actuelle: {current_fidelity:.3f}")
            
            # √âtape 1: Analyser √©checs actuels
            failure_analysis = self.load_corpus_with_failures()
            
            # √âtape 2: Identifier ambigu√Øt√©s conceptuelles
            ambiguities = self.analyze_conceptual_gaps(failure_analysis)
            print(f"   üîç Ambigu√Øt√©s d√©couvertes: {len(ambiguities)}")
            
            if not ambiguities:
                print("   ‚úÖ Aucune ambigu√Øt√© d√©tect√©e - Convergence atteinte")
                break
            
            # √âtape 3: Proposer raffinements
            new_dhatu, new_molecules = self.propose_dhatu_refinements(ambiguities)
            print(f"   ‚öõÔ∏è Nouveaux dhƒÅtu propos√©s: {len(new_dhatu)}")
            print(f"   üß¨ Nouvelles mol√©cules: {len(new_molecules)}")
            
            # √âtape 4: Tester hypoth√®ses
            iteration_result = self.test_hypotheses_iteratively(new_dhatu, new_molecules)
            self.iterations_log.append(iteration_result)
            
            print(f"   üìà Am√©lioration: {iteration_result.convergence_metrics['fidelity_improvement']:+.3f}")
            print(f"   üéØ Nouvelle fid√©lit√©: {iteration_result.corpus_fidelity_after:.3f}")
            
            # V√©rifier convergence
            if iteration_result.convergence_metrics['fidelity_improvement'] < self.convergence_thresholds['min_improvement']:
                print(f"   ‚ö†Ô∏è Am√©lioration insuffisante ({iteration_result.convergence_metrics['fidelity_improvement']:.3f} < {self.convergence_thresholds['min_improvement']})")
                break
            
            current_fidelity = iteration_result.corpus_fidelity_after
            
            # Mettre √† jour mod√®le pour prochaine it√©ration
            self._update_dhatu_model(iteration_result)
        
        print(f"\nüèÜ OPTIMISATION TERMIN√âE")
        print(f"   It√©rations: {iteration_count}")
        print(f"   Fid√©lit√© finale: {current_fidelity:.3f}")
        print(f"   Objectif 100%: {'‚úÖ ATTEINT' if current_fidelity >= 0.99 else '‚ö†Ô∏è Partiel'}")
        
        return self.iterations_log
    
    # M√©thodes utilitaires (impl√©mentation simplifi√©e pour d√©monstration)
    
    def _analyze_reconstitution_failure(self, result) -> List[Dict]:
        """Analyse patterns d'√©chec de reconstitution"""
        return [{'type': 'semantic_gap', 'severity': 'high'}]
    
    def _extract_missing_concepts(self, result) -> List[str]:
        """Extrait concepts manquants d'un r√©sultat"""
        return ['aspectual_information', 'modal_context', 'evidential_source']
    
    def _detect_ambiguities(self, result) -> List[Dict]:
        """D√©tecte ambigu√Øt√©s dans un r√©sultat"""
        return [{
            'type': 'aspectual',
            'source_context': 'courut vs courait',
            'target_context': 'ran',
            'missing_dhatu': ['PERF', 'PROG'],
            'confidence_scores': {'aspectual': 0.8},
            'variants': {'fr': 'perfectif', 'en': 'ambiguous'}
        }]
    
    def _generate_interpretation_hypotheses(self, ambiguity, source_lang, target_lang) -> List[Dict]:
        """G√©n√®re hypoth√®ses d'interpr√©tation pour ambigu√Øt√©"""
        return [
            {'hypothesis': 'aspectual_distinction', 'confidence': 0.8},
            {'hypothesis': 'modal_overlay', 'confidence': 0.6}
        ]
    
    def _prioritize_ambiguities(self, ambiguities, frequency, patterns) -> List[ConceptualAmbiguity]:
        """Priorise ambigu√Øt√©s par impact"""
        return ambiguities[:5]  # Top 5 pour d√©monstration
    
    def _cluster_ambiguities_by_concept(self, ambiguities) -> Dict[str, List]:
        """Groupe ambigu√Øt√©s par type conceptuel"""
        return {
            'aspectual': ambiguities[:2],
            'modal': ambiguities[2:4],
            'evidential': ambiguities[4:]
        }
    
    def _measure_corpus_fidelity(self) -> float:
        """Mesure fid√©lit√© actuelle sur corpus complet"""
        return 0.125  # Baseline simplifi√©e
    
    def _test_single_dhatu_addition(self, dhatu: str) -> Dict:
        """Teste ajout d'un dhƒÅtu unique"""
        return {'dhatu': dhatu, 'fidelity_gain': 0.05, 'confidence': 0.7}
    
    def _test_molecule_addition(self, molecule: DhatuMolecule) -> Dict:
        """Teste ajout d'une mol√©cule"""
        return {'molecule': molecule, 'fidelity_gain': 0.08, 'confidence': 0.8}
    
    def _select_compatible_improvements(self, improvements) -> List[Dict]:
        """S√©lectionne am√©liorations compatibles"""
        return improvements[:3]  # Top 3 compatible
    
    def _apply_improvements_and_measure(self, improvements) -> float:
        """Applique am√©liorations et mesure r√©sultat"""
        base_fidelity = 0.125
        total_gain = sum(imp.get('fidelity_gain', 0) for imp in improvements)
        return min(base_fidelity + total_gain, 1.0)
    
    def _count_resolved_ambiguities(self, improvements) -> List[ConceptualAmbiguity]:
        """Compte ambigu√Øt√©s r√©solues"""
        return []  # Impl√©mentation simplifi√©e
    
    def _discover_new_ambiguities(self, fidelity) -> List[ConceptualAmbiguity]:
        """D√©couvre nouvelles ambigu√Øt√©s √† ce niveau"""
        return []  # Impl√©mentation simplifi√©e
    
    def _update_dhatu_model(self, iteration_result):
        """Met √† jour mod√®le dhƒÅtu avec r√©sultats d'it√©ration"""
        self.current_dhatu.update(iteration_result.new_dhatu_proposed)
        self.dhatu_molecules.extend(iteration_result.new_molecules_created)


def main():
    """Point d'entr√©e principal"""
    
    analyzer = MultilingualAmbiguityAnalyzer()
    
    try:
        # Lancer optimisation it√©rative compl√®te
        optimization_results = analyzer.run_iterative_optimization()
        
        # Sauvegarder r√©sultats d√©taill√©s
        results_file = Path(__file__).parent / "optimization_results_iterative.json"
        
        results_data = {
            'optimization_summary': {
                'total_iterations': len(optimization_results),
                'final_fidelity': optimization_results[-1].corpus_fidelity_after if optimization_results else 0,
                'convergence_achieved': optimization_results[-1].corpus_fidelity_after >= 0.99 if optimization_results else False,
                'total_dhatu_added': sum(len(r.new_dhatu_proposed) for r in optimization_results),
                'total_molecules_created': sum(len(r.new_molecules_created) for r in optimization_results)
            },
            'iteration_details': [
                {
                    'iteration': r.iteration_number,
                    'fidelity_before': r.corpus_fidelity_before,
                    'fidelity_after': r.corpus_fidelity_after,
                    'improvement': r.convergence_metrics['fidelity_improvement'],
                    'new_dhatu': r.new_dhatu_proposed,
                    'ambiguities_resolved': len(r.ambiguities_resolved)
                }
                for r in optimization_results
            ],
            'final_dhatu_model': list(analyzer.current_dhatu),
            'discovered_molecules': [
                {
                    'id': mol.molecule_id,
                    'components': mol.component_dhatu,
                    'concept': mol.molecular_concept,
                    'weight': mol.semantic_weight
                }
                for mol in analyzer.dhatu_molecules
            ]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ R√©sultats sauvegard√©s: {results_file}")
        
    except Exception as e:
        print(f"‚ùå Erreur optimisation: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()