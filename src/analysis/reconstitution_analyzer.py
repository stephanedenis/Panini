#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analyseur d'√âcarts de Reconstitution DhƒÅtu
Test empirique de fid√©lit√© s√©mantique par d√©composition/reconstitution

M√©thodologie:
1. Extraction dhƒÅtu depuis texte source 
2. Reconstitution s√©mantique vers langue cible
3. Mesure √©carts avec version authentique
4. Identification patterns d'√©chec et optimisation mod√®le
"""

import json
import re
import math
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import sys

# Import des modules existants
sys.path.append(str(Path(__file__).parent / "scripts"))
from optimal_dhatu_analyzer import OptimalDhatuAnalyzer


@dataclass
class DhatuExtraction:
    """Repr√©sente l'extraction dhƒÅtu d'un texte"""
    source_text: str
    source_lang: str
    extracted_dhatu: List[Dict[str, Any]]
    coverage_score: float
    semantic_gaps: List[str]
    
@dataclass
class ReconstitutionResult:
    """R√©sultat d'une reconstitution cross-linguistique"""
    source_extraction: DhatuExtraction
    target_lang: str
    reconstructed_text: str
    authentic_target: str
    fidelity_scores: Dict[str, float]
    semantic_deviations: List[str]
    success_metrics: Dict[str, Any]


class ReconstitutionAnalyzer:
    """Analyseur principal des √©carts de reconstitution"""
    
    def __init__(self):
        self.dhatu_analyzer = OptimalDhatuAnalyzer()
        self.corpus_file = Path(__file__).parent / "corpus_children_literature" / "corpus_pilot.json"
        
        # R√®gles de reconstitution s√©mantique
        self.reconstitution_rules = {
            'EXIST': {
                'fr': ['il √©tait', 'il y avait', 'vivait', 'se trouvait'],
                'en': ['there was', 'there lived', 'existed', 'was'],
                'de': ['es war', 'es gab', 'lebte', 'existierte']
            },
            'COMM': {
                'fr': ['dit', 'parla', 'demanda', 'r√©pondit', 'd√©clara'],
                'en': ['said', 'spoke', 'asked', 'replied', 'declared'],
                'de': ['sagte', 'sprach', 'fragte', 'antwortete', 'erkl√§rte']
            },
            'TRANS': {
                'fr': ['alla', 'vint', 'courut', 'marcha', 'se dirigea'],
                'en': ['went', 'came', 'ran', 'walked', 'moved'],
                'de': ['ging', 'kam', 'lief', 'wanderte', 'bewegte sich']
            },
            'DECIDE': {
                'fr': ['d√©cida', 'choisit', 'r√©solut', 'opta'],
                'en': ['decided', 'chose', 'resolved', 'opted'],
                'de': ['entschied', 'w√§hlte', 'beschloss', 'entschloss sich']
            },
            'EVAL': {
                'fr': ['beau', 'bon', 'mauvais', 'excellent', 'terrible'],
                'en': ['beautiful', 'good', 'bad', 'excellent', 'terrible'],
                'de': ['sch√∂n', 'gut', 'schlecht', 'ausgezeichnet', 'schrecklich']
            },
            'GROUP': {
                'fr': ['ensemble', 'groupe', 'famille', '√©quipe'],
                'en': ['together', 'group', 'family', 'team'],
                'de': ['zusammen', 'Gruppe', 'Familie', 'Team']
            },
            'ITER': {
                'fr': ['encore', 'de nouveau', 'r√©p√©ta', 'continua'],
                'en': ['again', 'once more', 'repeated', 'continued'],
                'de': ['wieder', 'nochmals', 'wiederholte', 'fortsetzte']
            },
            'LOCATE': {
                'fr': ['dans', 'sur', 'pr√®s de', '√† c√¥t√© de'],
                'en': ['in', 'on', 'near', 'beside'],
                'de': ['in', 'auf', 'nahe', 'neben']
            },
            'SEQ': {
                'fr': ['puis', 'ensuite', 'apr√®s', 'alors'],
                'en': ['then', 'next', 'after', 'so'],
                'de': ['dann', 'danach', 'nach', 'so']
            }
        }
        
        # M√©triques de fid√©lit√©
        self.fidelity_thresholds = {
            'word_overlap_ratio': 0.40,     # 40% mots communs minimum
            'semantic_preservation': 0.65,   # 65% pr√©servation s√©mantique
            'narrative_flow': 0.70,         # 70% coh√©rence narrative
            'cultural_fidelity': 0.60       # 60% fid√©lit√© culturelle
        }
    
    def load_corpus(self) -> List[Dict[str, Any]]:
        """Charge le corpus litt√©rature jeunesse"""
        
        if not self.corpus_file.exists():
            raise FileNotFoundError(f"Corpus non trouv√©: {self.corpus_file}")
        
        with open(self.corpus_file, 'r', encoding='utf-8') as f:
            corpus_data = json.load(f)
        
        return corpus_data['texts']
    
    def extract_dhatu_from_text(self, text: str, lang: str) -> DhatuExtraction:
        """Extrait les dhƒÅtu d'un texte source"""
        
        analysis = self.dhatu_analyzer.analyze_text(text)
        
        # Structurer l'extraction dhƒÅtu
        extracted_dhatu = []
        for dhatu_name, count in analysis['dhatu_distribution'].items():
            if count > 0:
                dhatu_info = {
                    'dhatu': dhatu_name,
                    'frequency': count,
                    'semantic_weight': count / len(text.split()),
                    'context_examples': self._find_dhatu_contexts(text, dhatu_name)
                }
                extracted_dhatu.append(dhatu_info)
        
        # Identifier gaps s√©mantiques
        semantic_gaps = [gap.text for gap in analysis['semantic_gaps'][:5]]
        
        return DhatuExtraction(
            source_text=text,
            source_lang=lang,
            extracted_dhatu=extracted_dhatu,
            coverage_score=analysis['coverage_stats']['semantic_coverage'],
            semantic_gaps=semantic_gaps
        )
    
    def reconstruct_from_dhatu(self, extraction: DhatuExtraction, target_lang: str) -> str:
        """Reconstitue un texte dans la langue cible √† partir des dhƒÅtu"""
        
        # Strat√©gie de reconstitution bas√©e sur dhƒÅtu
        reconstructed_segments = []
        
        # Analyser la structure narrative du texte original
        source_sentences = extraction.source_text.split('.')
        source_words = extraction.source_text.split()
        
        # Pour chaque dhƒÅtu extrait, g√©n√©rer un segment reconstitu√©
        for dhatu_info in extraction.extracted_dhatu:
            dhatu = dhatu_info['dhatu']
            frequency = dhatu_info['frequency']
            
            # S√©lectionner expressions cibles
            if dhatu in self.reconstitution_rules and target_lang in self.reconstitution_rules[dhatu]:
                target_expressions = self.reconstitution_rules[dhatu][target_lang]
                
                # Choisir expression bas√©e sur contexte et fr√©quence
                selected_expr = self._select_best_expression(
                    target_expressions, dhatu_info['context_examples'], frequency
                )
                
                reconstructed_segments.append(selected_expr)
        
        # Reconstituer structure narrative
        if target_lang == 'en':
            narrative_structure = self._build_english_narrative(reconstructed_segments, extraction)
        elif target_lang == 'de':
            narrative_structure = self._build_german_narrative(reconstructed_segments, extraction)
        elif target_lang == 'fr':
            narrative_structure = self._build_french_narrative(reconstructed_segments, extraction)
        else:
            narrative_structure = " ".join(reconstructed_segments)
        
        return narrative_structure
    
    def measure_fidelity(self, reconstructed: str, authentic: str, target_lang: str) -> Dict[str, float]:
        """Mesure la fid√©lit√© entre texte reconstitu√© et version authentique"""
        
        fidelity_scores = {}
        
        # 1. Word Overlap Ratio (Jaccard similarity)
        recon_words = set(reconstructed.lower().split())
        auth_words = set(authentic.lower().split())
        
        intersection = len(recon_words & auth_words)
        union = len(recon_words | auth_words)
        fidelity_scores['word_overlap_ratio'] = intersection / union if union > 0 else 0
        
        # 2. Semantic Preservation (approximation via mots-cl√©s s√©mantiques)
        semantic_preservation = self._calculate_semantic_preservation(reconstructed, authentic, target_lang)
        fidelity_scores['semantic_preservation'] = semantic_preservation
        
        # 3. Narrative Flow (s√©quence d'√©v√©nements)
        narrative_flow = self._calculate_narrative_flow(reconstructed, authentic)
        fidelity_scores['narrative_flow'] = narrative_flow
        
        # 4. Cultural Fidelity (pr√©servation √©l√©ments culturels)
        cultural_fidelity = self._calculate_cultural_fidelity(reconstructed, authentic, target_lang)
        fidelity_scores['cultural_fidelity'] = cultural_fidelity
        
        # Score global pond√©r√©
        weights = {'word_overlap_ratio': 0.25, 'semantic_preservation': 0.35, 
                  'narrative_flow': 0.25, 'cultural_fidelity': 0.15}
        
        fidelity_scores['global_fidelity'] = sum(
            fidelity_scores[metric] * weight for metric, weight in weights.items()
        )
        
        return fidelity_scores
    
    def run_reconstitution_test(self, text_id: str, source_lang: str, target_lang: str) -> ReconstitutionResult:
        """Ex√©cute un test complet de reconstitution pour un texte"""
        
        corpus = self.load_corpus()
        
        # Trouver le texte dans le corpus
        target_text = None
        for text in corpus:
            if text['id'] == text_id:
                target_text = text
                break
        
        if not target_text:
            raise ValueError(f"Texte '{text_id}' non trouv√© dans le corpus")
        
        if source_lang not in target_text['versions'] or target_lang not in target_text['versions']:
            raise ValueError(f"Langues {source_lang}/{target_lang} non disponibles pour {text_id}")
        
        # √âtape 1: Extraction dhƒÅtu depuis source
        source_text = target_text['versions'][source_lang]
        extraction = self.extract_dhatu_from_text(source_text, source_lang)
        
        # √âtape 2: Reconstitution vers langue cible
        reconstructed = self.reconstruct_from_dhatu(extraction, target_lang)
        
        # √âtape 3: Mesure fid√©lit√© avec version authentique
        authentic_target = target_text['versions'][target_lang]
        fidelity_scores = self.measure_fidelity(reconstructed, authentic_target, target_lang)
        
        # √âtape 4: Identifier d√©viations s√©mantiques
        semantic_deviations = self._identify_semantic_deviations(
            reconstructed, authentic_target, extraction
        )
        
        # M√©triques de succ√®s
        success_metrics = {
            'passes_fidelity_threshold': fidelity_scores['global_fidelity'] >= 0.65,
            'preserves_key_concepts': len(semantic_deviations) <= 3,
            'maintains_narrative_structure': fidelity_scores['narrative_flow'] >= 0.70,
            'dhatu_coverage_adequate': extraction.coverage_score >= 0.60
        }
        
        return ReconstitutionResult(
            source_extraction=extraction,
            target_lang=target_lang,
            reconstructed_text=reconstructed,
            authentic_target=authentic_target,
            fidelity_scores=fidelity_scores,
            semantic_deviations=semantic_deviations,
            success_metrics=success_metrics
        )
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Lance validation compl√®te sur tout le corpus"""
        
        print("üß™ VALIDATION COMPL√àTE RECONSTITUTION DHƒÄTU")
        print("=" * 50)
        
        corpus = self.load_corpus()
        validation_results = {
            'test_results': [],
            'global_metrics': {},
            'language_pair_performance': {},
            'failure_patterns': [],
            'recommendations': []
        }
        
        # Test toutes les paires de langues pour tous les textes
        language_pairs = [('fr', 'en'), ('en', 'fr'), ('fr', 'de'), ('de', 'fr'), ('en', 'de'), ('de', 'en')]
        
        all_fidelity_scores = []
        pair_scores = {f"{src}-{tgt}": [] for src, tgt in language_pairs}
        
        for text in corpus:
            text_id = text['id']
            print(f"\nüìñ Test: {text['title']}")
            
            for source_lang, target_lang in language_pairs:
                if source_lang in text['versions'] and target_lang in text['versions']:
                    try:
                        result = self.run_reconstitution_test(text_id, source_lang, target_lang)
                        validation_results['test_results'].append({
                            'text_id': text_id,
                            'source_lang': source_lang,
                            'target_lang': target_lang,
                            'fidelity_score': result.fidelity_scores['global_fidelity'],
                            'success_metrics': result.success_metrics,
                            'dhatu_count': len(result.source_extraction.extracted_dhatu)
                        })
                        
                        all_fidelity_scores.append(result.fidelity_scores['global_fidelity'])
                        pair_scores[f"{source_lang}-{target_lang}"].append(result.fidelity_scores['global_fidelity'])
                        
                        print(f"   {source_lang}‚Üí{target_lang}: {result.fidelity_scores['global_fidelity']:.3f}")
                        
                    except Exception as e:
                        print(f"   ‚ùå {source_lang}‚Üí{target_lang}: Erreur - {e}")
        
        # Calculer m√©triques globales
        validation_results['global_metrics'] = {
            'mean_fidelity': sum(all_fidelity_scores) / len(all_fidelity_scores) if all_fidelity_scores else 0,
            'total_tests': len(all_fidelity_scores),
            'success_rate': sum(1 for score in all_fidelity_scores if score >= 0.65) / len(all_fidelity_scores) if all_fidelity_scores else 0,
            'min_fidelity': min(all_fidelity_scores) if all_fidelity_scores else 0,
            'max_fidelity': max(all_fidelity_scores) if all_fidelity_scores else 0
        }
        
        # Performance par paire de langues
        for pair, scores in pair_scores.items():
            if scores:
                validation_results['language_pair_performance'][pair] = {
                    'mean_fidelity': sum(scores) / len(scores),
                    'test_count': len(scores),
                    'success_rate': sum(1 for score in scores if score >= 0.65) / len(scores)
                }
        
        return validation_results
    
    # M√©thodes utilitaires priv√©es
    
    def _find_dhatu_contexts(self, text: str, dhatu_name: str) -> List[str]:
        """Trouve les contextes o√π un dhƒÅtu appara√Æt dans le texte"""
        # Impl√©mentation simplifi√©e
        return [f"contexte_{dhatu_name}"]
    
    def _select_best_expression(self, expressions: List[str], contexts: List[str], frequency: int) -> str:
        """S√©lectionne la meilleure expression cible bas√©e sur le contexte"""
        # Strat√©gie simple: prendre la premi√®re expression
        return expressions[0] if expressions else "unknown"
    
    def _build_english_narrative(self, segments: List[str], extraction: DhatuExtraction) -> str:
        """Construit narrative anglaise √† partir des segments"""
        if not segments:
            return "A story unfolds..."
        
        # Structure narrative anglaise basique
        narrative = "Once upon a time, " + segments[0]
        for segment in segments[1:]:
            narrative += f". {segment.capitalize()}"
        narrative += "."
        
        return narrative
    
    def _build_german_narrative(self, segments: List[str], extraction: DhatuExtraction) -> str:
        """Construit narrative allemande √† partir des segments"""
        if not segments:
            return "Es war einmal..."
        
        narrative = "Es war einmal, " + segments[0]
        for segment in segments[1:]:
            narrative += f". {segment.capitalize()}"
        narrative += "."
        
        return narrative
    
    def _build_french_narrative(self, segments: List[str], extraction: DhatuExtraction) -> str:
        """Construit narrative fran√ßaise √† partir des segments"""
        if not segments:
            return "Il √©tait une fois..."
        
        narrative = "Il √©tait une fois, " + segments[0]
        for segment in segments[1:]:
            narrative += f". {segment.capitalize()}"
        narrative += "."
        
        return narrative
    
    def _calculate_semantic_preservation(self, reconstructed: str, authentic: str, lang: str) -> float:
        """Calcule pr√©servation s√©mantique (approximation via mots-cl√©s)"""
        
        # Mots-cl√©s s√©mantiques importants par langue
        semantic_keywords = {
            'fr': ['dit', 'alla', '√©tait', 'fit', 'vit', 'prit'],
            'en': ['said', 'went', 'was', 'did', 'saw', 'took'],
            'de': ['sagte', 'ging', 'war', 'tat', 'sah', 'nahm']
        }
        
        if lang not in semantic_keywords:
            return 0.5  # Score neutre par d√©faut
        
        keywords = semantic_keywords[lang]
        recon_matches = sum(1 for kw in keywords if kw in reconstructed.lower())
        auth_matches = sum(1 for kw in keywords if kw in authentic.lower())
        
        if auth_matches == 0:
            return 1.0 if recon_matches == 0 else 0.0
        
        return min(recon_matches / auth_matches, 1.0)
    
    def _calculate_narrative_flow(self, reconstructed: str, authentic: str) -> float:
        """Calcule coh√©rence du flow narratif"""
        
        # Indicateurs de s√©quence narrative
        sequence_indicators = ['puis', 'ensuite', 'alors', 'then', 'next', 'after', 'dann', 'danach']
        
        recon_sequences = sum(1 for ind in sequence_indicators if ind in reconstructed.lower())
        auth_sequences = sum(1 for ind in sequence_indicators if ind in authentic.lower())
        
        if auth_sequences == 0:
            return 1.0 if recon_sequences == 0 else 0.8
        
        return min(recon_sequences / auth_sequences, 1.0)
    
    def _calculate_cultural_fidelity(self, reconstructed: str, authentic: str, lang: str) -> float:
        """Calcule fid√©lit√© culturelle (expressions idiomatiques, etc.)"""
        
        # Expressions culturelles typiques par langue
        cultural_expressions = {
            'fr': ['il √©tait une fois', 'bien s√ªr', 'tout de suite'],
            'en': ['once upon a time', 'of course', 'right away'],
            'de': ['es war einmal', 'nat√ºrlich', 'sofort']
        }
        
        if lang not in cultural_expressions:
            return 0.7  # Score neutre
        
        expressions = cultural_expressions[lang]
        recon_cultural = sum(1 for expr in expressions if expr in reconstructed.lower())
        auth_cultural = sum(1 for expr in expressions if expr in authentic.lower())
        
        if auth_cultural == 0:
            return 0.8  # Score par d√©faut si pas d'expressions culturelles
        
        return min(recon_cultural / auth_cultural, 1.0)
    
    def _identify_semantic_deviations(self, reconstructed: str, authentic: str, extraction: DhatuExtraction) -> List[str]:
        """Identifie les d√©viations s√©mantiques majeures"""
        
        deviations = []
        
        # V√©rifier si les dhƒÅtu principaux sont repr√©sent√©s
        for dhatu_info in extraction.extracted_dhatu[:3]:  # Top 3 dhƒÅtu
            dhatu = dhatu_info['dhatu']
            if dhatu not in str(extraction.source_text).upper():  # Approximation simplifi√©e
                deviations.append(f"DhƒÅtu {dhatu} perdu dans reconstitution")
        
        # V√©rifier longueur relative
        recon_words = len(reconstructed.split())
        auth_words = len(authentic.split())
        
        if abs(recon_words - auth_words) > auth_words * 0.5:
            deviations.append(f"Longueur tr√®s diff√©rente: {recon_words} vs {auth_words} mots")
        
        return deviations


def main():
    """Point d'entr√©e principal"""
    
    analyzer = ReconstitutionAnalyzer()
    
    try:
        # Test simple d'abord
        print("üß™ TEST PILOTE - Tortoise and Hare")
        print("-" * 40)
        
        result = analyzer.run_reconstitution_test('tortoise_hare', 'fr', 'en')
        
        print(f"üìù Texte source (FR): {result.source_extraction.source_text[:100]}...")
        print(f"üîÑ Reconstitu√© (EN): {result.reconstructed_text}")
        print(f"‚úÖ Authentique (EN): {result.authentic_target[:100]}...")
        print(f"\nüìä Scores de fid√©lit√©:")
        for metric, score in result.fidelity_scores.items():
            print(f"   ‚Ä¢ {metric}: {score:.3f}")
        
        print(f"\nüéØ M√©triques de succ√®s:")
        for metric, success in result.success_metrics.items():
            status = "‚úÖ" if success else "‚ùå"
            print(f"   {status} {metric}: {success}")
        
        if result.semantic_deviations:
            print(f"\n‚ö†Ô∏è D√©viations s√©mantiques:")
            for deviation in result.semantic_deviations:
                print(f"   ‚Ä¢ {deviation}")
        
        # Validation compl√®te si test pilote r√©ussi
        if result.fidelity_scores['global_fidelity'] >= 0.50:
            print(f"\nüöÄ LANCEMENT VALIDATION COMPL√àTE...")
            validation = analyzer.run_comprehensive_validation()
            
            print(f"\nüìà R√âSULTATS GLOBAUX:")
            print(f"   ‚Ä¢ Tests effectu√©s: {validation['global_metrics']['total_tests']}")
            print(f"   ‚Ä¢ Fid√©lit√© moyenne: {validation['global_metrics']['mean_fidelity']:.3f}")
            print(f"   ‚Ä¢ Taux de succ√®s: {validation['global_metrics']['success_rate']:.1%}")
            print(f"   ‚Ä¢ Fid√©lit√© min/max: {validation['global_metrics']['min_fidelity']:.3f} / {validation['global_metrics']['max_fidelity']:.3f}")
            
            print(f"\nüåç PERFORMANCE PAR PAIRE DE LANGUES:")
            for pair, metrics in validation['language_pair_performance'].items():
                print(f"   ‚Ä¢ {pair}: {metrics['mean_fidelity']:.3f} (succ√®s: {metrics['success_rate']:.1%})")
        
        else:
            print(f"\n‚ö†Ô∏è Test pilote en dessous du seuil - optimisation n√©cessaire")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()