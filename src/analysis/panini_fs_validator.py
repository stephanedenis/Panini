#!/usr/bin/env python3
"""
Validateur PaniniFS - Framework Validation Multi-Format
Validation exhaustive ingestion/restitution avec intÃ©gritÃ© 100%
ISO 8601 compliant timestamp format
"""

import hashlib
import json
import time
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Any, Tuple, Optional


class IntegrityError(Exception):
    """Exception levÃ©e quand l'intÃ©gritÃ© n'est pas 100%"""
    pass


class PaniniFSValidator:
    """
    Framework de validation exhaustif pour PaniniFS
    Support multi-format avec garantie d'intÃ©gritÃ© bit-Ã -bit
    """
    
    def __init__(self, workspace: Optional[Path] = None):
        """
        Initialise le validateur PaniniFS
        
        Args:
            workspace: RÃ©pertoire de travail (dÃ©faut: rÃ©pertoire courant)
        """
        self.workspace = workspace or Path.cwd() / 'panini_fs_validation'
        self.workspace.mkdir(exist_ok=True, parents=True)
        
        # RÃ©pertoires de travail
        self.ingestion_dir = self.workspace / 'ingestion'
        self.compressed_dir = self.workspace / 'compressed'
        self.restitution_dir = self.workspace / 'restitution'
        self.reports_dir = self.workspace / 'reports'
        
        for directory in [self.ingestion_dir, self.compressed_dir, 
                         self.restitution_dir, self.reports_dir]:
            directory.mkdir(exist_ok=True, parents=True)
        
        # Formats supportÃ©s par catÃ©gorie
        self.supported_formats = {
            'text': ['pdf', 'txt', 'epub', 'docx', 'md'],
            'audio': ['mp3', 'wav', 'flac', 'ogg'],
            'video': ['mp4', 'mkv', 'avi', 'webm'],
            'image': ['jpg', 'jpeg', 'png', 'gif', 'svg', 'webp']
        }
        
        # MÃ©triques de validation
        self.validation_metrics = {
            'total_files': 0,
            'successful_validations': 0,
            'failed_validations': 0,
            'success_rate': 0.0,  # Taux de rÃ©ussite (nb_succÃ¨s / nb_total)
            'by_format': {},
            'performance_metrics': {}
        }
        
        self.log("ğŸš€ Validateur PaniniFS initialisÃ©")
        self.log(f"ğŸ“ Workspace: {self.workspace}")
    
    def log(self, message: str, level: str = "INFO"):
        """
        Logging avec timestamp ISO 8601 UTC
        
        Args:
            message: Message Ã  logger
            level: Niveau de log (INFO, WARNING, ERROR, SUCCESS)
        """
        timestamp = datetime.now(timezone.utc).isoformat()
        prefix = {
            "INFO": "â„¹ï¸",
            "WARNING": "âš ï¸",
            "ERROR": "âŒ",
            "SUCCESS": "âœ…"
        }.get(level, "â„¹ï¸")
        print(f"[{timestamp}] {prefix} {message}")
    
    def compute_file_hash(self, file_path: Path, algorithm: str = 'sha256') -> str:
        """
        Calcule le hash d'un fichier pour vÃ©rification d'intÃ©gritÃ©
        
        Args:
            file_path: Chemin du fichier
            algorithm: Algorithme de hash (sha256, md5, sha512)
            
        Returns:
            Hash hexadÃ©cimal du fichier
        """
        hash_func = getattr(hashlib, algorithm)()
        
        with open(file_path, 'rb') as f:
            # Lecture par blocs pour fichiers volumineux
            for chunk in iter(lambda: f.read(4096), b''):
                hash_func.update(chunk)
        
        return hash_func.hexdigest()
    
    def detect_format(self, file_path: Path) -> Tuple[str, str]:
        """
        DÃ©tecte le format et la catÃ©gorie d'un fichier
        
        Args:
            file_path: Chemin du fichier
            
        Returns:
            Tuple (catÃ©gorie, format)
        """
        extension = file_path.suffix.lower().lstrip('.')
        
        for category, formats in self.supported_formats.items():
            if extension in formats:
                return category, extension
        
        return 'unknown', extension
    
    def validate_file_integrity(
        self,
        original_path: Path,
        restored_path: Path
    ) -> bool:
        """
        Valide l'intÃ©gritÃ© bit-Ã -bit entre fichier original et restituÃ©
        INTÃ‰GRITÃ‰ 100% OU Ã‰CHEC - pas de zone grise
        
        Args:
            original_path: Chemin du fichier original
            restored_path: Chemin du fichier restituÃ©
            
        Returns:
            True si intÃ©gritÃ© 100%, sinon lÃ¨ve IntegrityError
            
        Raises:
            IntegrityError: Si intÃ©gritÃ© n'est pas 100%
            FileNotFoundError: Si un fichier est introuvable
        """
        start_time = time.time()
        
        # VÃ©rification existence fichiers
        if not original_path.exists():
            raise FileNotFoundError(f'Fichier original introuvable: {original_path}')
        
        if not restored_path.exists():
            raise FileNotFoundError(f'Fichier restituÃ© introuvable: {restored_path}')
        
        # Calcul des hashes
        original_hash = self.compute_file_hash(original_path)
        restored_hash = self.compute_file_hash(restored_path)
        
        # Comparaison tailles
        original_size = original_path.stat().st_size
        restored_size = restored_path.stat().st_size
        
        elapsed_time = time.time() - start_time
        
        # Validation bit-Ã -bit: 100% ou Ã‰CHEC
        if original_hash != restored_hash:
            raise IntegrityError(
                f"Reconstitution incomplÃ¨te - Hash mismatch: "
                f"original={original_hash} != restored={restored_hash}. "
                f"Fichier inutilisable."
            )
        
        if original_size != restored_size:
            raise IntegrityError(
                f"Reconstitution incomplÃ¨te - Size mismatch: "
                f"original={original_size} != restored={restored_size} bytes. "
                f"Fichier inutilisable."
            )
        
        # Si on arrive ici, intÃ©gritÃ© 100%
        self.log(f"âœ… IntÃ©gritÃ© 100% validÃ©e: {original_path.name}", "SUCCESS")
        return True
    
    def validate_format_pipeline(
        self,
        file_path: Path,
        compression_callback=None,
        decompression_callback=None
    ) -> Dict[str, Any]:
        """
        Pipeline complet de validation pour un fichier
        Ingestion â†’ Compression â†’ DÃ©compression â†’ Restitution â†’ Validation
        
        Args:
            file_path: Fichier Ã  valider
            compression_callback: Fonction de compression personnalisÃ©e
            decompression_callback: Fonction de dÃ©compression personnalisÃ©e
            
        Returns:
            RÃ©sultat complet de validation
        """
        self.log(f"ğŸ”„ DÃ©marrage pipeline validation: {file_path.name}")
        
        category, format_type = self.detect_format(file_path)
        
        if category == 'unknown':
            self.log(f"âš ï¸ Format non supportÃ©: {format_type}", "WARNING")
        
        # Phase 1: Ingestion
        original_hash = self.compute_file_hash(file_path)
        original_size = file_path.stat().st_size
        
        self.log(f"ğŸ“¥ Ingestion: {file_path.name} ({original_size} bytes)")
        
        # Phase 2: Compression (simulation si pas de callback)
        compressed_path = self.compressed_dir / f"{file_path.stem}.panini"
        compression_start = time.time()
        
        if compression_callback:
            compressed_data = compression_callback(file_path)
            with open(compressed_path, 'wb') as f:
                f.write(compressed_data)
        else:
            # Simulation: copie directe pour tests
            import shutil
            shutil.copy2(file_path, compressed_path)
        
        compression_time = time.time() - compression_start
        compressed_size = compressed_path.stat().st_size
        compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0
        
        self.log(f"ğŸ—œï¸  Compression: {compressed_size} bytes (ratio: {compression_ratio:.2f}x)")
        
        # Phase 3: DÃ©compression
        restored_path = self.restitution_dir / file_path.name
        decompression_start = time.time()
        
        if decompression_callback:
            decompressed_data = decompression_callback(compressed_path)
            with open(restored_path, 'wb') as f:
                f.write(decompressed_data)
        else:
            # Simulation: copie directe pour tests
            import shutil
            shutil.copy2(compressed_path, restored_path)
        
        decompression_time = time.time() - decompression_start
        
        self.log(f"ğŸ“¤ Restitution: {restored_path.name}")
        
        # Phase 4: Validation intÃ©gritÃ© (100% ou Ã‰CHEC)
        try:
            integrity_valid = self.validate_file_integrity(file_path, restored_path)
            integrity_status = 'SUCCESS'
        except (IntegrityError, FileNotFoundError) as e:
            self.log(f"âŒ Ã‰CHEC intÃ©gritÃ©: {str(e)}", "ERROR")
            integrity_valid = False
            integrity_status = 'FAILED'
        
        # RÃ©sultat complet
        result = {
            'file_name': file_path.name,
            'category': category,
            'format': format_type,
            'original_size': original_size,
            'compressed_size': compressed_size,
            'compression_ratio': compression_ratio,
            'compression_time': compression_time,
            'decompression_time': decompression_time,
            'total_time': compression_time + decompression_time,
            'integrity_valid': integrity_valid,  # bool: True (100%) ou False (Ã©chec)
            'integrity_status': integrity_status,  # 'SUCCESS' ou 'FAILED'
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        # Mise Ã  jour mÃ©triques
        self.validation_metrics['total_files'] += 1
        if integrity_valid:
            self.validation_metrics['successful_validations'] += 1
        else:
            self.validation_metrics['failed_validations'] += 1
        
        # MÃ©triques par format
        if format_type not in self.validation_metrics['by_format']:
            self.validation_metrics['by_format'][format_type] = {
                'total': 0,
                'success': 0,
                'failed': 0
            }
        
        self.validation_metrics['by_format'][format_type]['total'] += 1
        if integrity_valid:
            self.validation_metrics['by_format'][format_type]['success'] += 1
        else:
            self.validation_metrics['by_format'][format_type]['failed'] += 1
        
        return result
    
    def validate_corpus(
        self,
        corpus_dir: Path,
        compression_callback=None,
        decompression_callback=None
    ) -> Dict[str, Any]:
        """
        Valide un corpus complet de fichiers multi-format
        
        Args:
            corpus_dir: RÃ©pertoire contenant les fichiers Ã  valider
            compression_callback: Fonction de compression
            decompression_callback: Fonction de dÃ©compression
            
        Returns:
            Rapport de validation du corpus
        """
        self.log("=" * 60)
        self.log("ğŸ§ª VALIDATION CORPUS MULTI-FORMAT")
        self.log("=" * 60)
        
        if not corpus_dir.exists():
            self.log(f"âŒ Corpus introuvable: {corpus_dir}", "ERROR")
            return {'error': f'Corpus directory not found: {corpus_dir}'}
        
        # Collecte fichiers par catÃ©gorie
        files_by_category = {
            'text': [],
            'audio': [],
            'video': [],
            'image': [],
            'unknown': []
        }
        
        for file_path in corpus_dir.rglob('*'):
            if file_path.is_file():
                category, _ = self.detect_format(file_path)
                files_by_category[category].append(file_path)
        
        self.log(f"ğŸ“Š Fichiers dÃ©tectÃ©s:")
        for category, files in files_by_category.items():
            if files:
                self.log(f"   {category.upper()}: {len(files)} fichiers")
        
        # Validation de tous les fichiers
        validation_results = []
        
        for category, files in files_by_category.items():
            if category == 'unknown':
                continue
            
            for file_path in files:
                try:
                    result = self.validate_format_pipeline(
                        file_path,
                        compression_callback,
                        decompression_callback
                    )
                    validation_results.append(result)
                except Exception as e:
                    self.log(f"âŒ Erreur validation {file_path.name}: {e}", "ERROR")
                    validation_results.append({
                        'file_name': file_path.name,
                        'error': str(e),
                        'success': False
                    })
        
        # Calcul taux de rÃ©ussite (nb_succÃ¨s / nb_total)
        if self.validation_metrics['total_files'] > 0:
            self.validation_metrics['success_rate'] = (
                self.validation_metrics['successful_validations'] /
                self.validation_metrics['total_files']
            )
        
        # GÃ©nÃ©ration rapport
        report = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'corpus_path': str(corpus_dir),
            'metrics': self.validation_metrics,
            'validation_results': validation_results
        }
        
        # Sauvegarde rapport
        report_file = self.reports_dir / f"validation_report_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.log("=" * 60)
        self.log("ğŸ“ˆ RÃ‰SULTATS VALIDATION")
        self.log("=" * 60)
        self.log(f"Total fichiers: {self.validation_metrics['total_files']}")
        self.log(f"Validations rÃ©ussies: {self.validation_metrics['successful_validations']}")
        self.log(f"Validations Ã©chouÃ©es: {self.validation_metrics['failed_validations']}")
        self.log(f"Taux de rÃ©ussite: {self.validation_metrics['success_rate']*100:.2f}%")
        self.log(f"ğŸ“„ Rapport sauvegardÃ©: {report_file}")
        
        return report
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """
        Retourne un rÃ©sumÃ© des mÃ©triques de validation
        
        Returns:
            MÃ©triques de validation
        """
        return self.validation_metrics.copy()
    
    def generate_performance_benchmark(
        self,
        test_files: List[Path]
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re un benchmark de performance pour comparaison ext4/NTFS
        
        Args:
            test_files: Liste de fichiers de test
            
        Returns:
            RÃ©sultats de benchmark
        """
        self.log("ğŸ GÃ©nÃ©ration benchmark performance")
        
        benchmark_results = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'test_files_count': len(test_files),
            'total_size': sum(f.stat().st_size for f in test_files if f.exists()),
            'performance_by_format': {},
            'overall_metrics': {
                'avg_compression_time': 0.0,
                'avg_decompression_time': 0.0,
                'avg_compression_ratio': 0.0,
                'avg_throughput_mbps': 0.0
            }
        }
        
        total_compression_time = 0.0
        total_decompression_time = 0.0
        total_compression_ratio = 0.0
        
        for file_path in test_files:
            if not file_path.exists():
                continue
            
            result = self.validate_format_pipeline(file_path)
            
            format_type = result['format']
            if format_type not in benchmark_results['performance_by_format']:
                benchmark_results['performance_by_format'][format_type] = {
                    'count': 0,
                    'avg_compression_time': 0.0,
                    'avg_decompression_time': 0.0,
                    'avg_compression_ratio': 0.0
                }
            
            fmt_metrics = benchmark_results['performance_by_format'][format_type]
            fmt_metrics['count'] += 1
            fmt_metrics['avg_compression_time'] += result['compression_time']
            fmt_metrics['avg_decompression_time'] += result['decompression_time']
            fmt_metrics['avg_compression_ratio'] += result['compression_ratio']
            
            total_compression_time += result['compression_time']
            total_decompression_time += result['decompression_time']
            total_compression_ratio += result['compression_ratio']
        
        # Calcul moyennes
        if len(test_files) > 0:
            benchmark_results['overall_metrics']['avg_compression_time'] = (
                total_compression_time / len(test_files)
            )
            benchmark_results['overall_metrics']['avg_decompression_time'] = (
                total_decompression_time / len(test_files)
            )
            benchmark_results['overall_metrics']['avg_compression_ratio'] = (
                total_compression_ratio / len(test_files)
            )
            
            total_time = total_compression_time + total_decompression_time
            if total_time > 0:
                total_mb = benchmark_results['total_size'] / (1024 * 1024)
                benchmark_results['overall_metrics']['avg_throughput_mbps'] = (
                    total_mb / total_time
                )
        
        # Moyennes par format
        for format_type, metrics in benchmark_results['performance_by_format'].items():
            if metrics['count'] > 0:
                metrics['avg_compression_time'] /= metrics['count']
                metrics['avg_decompression_time'] /= metrics['count']
                metrics['avg_compression_ratio'] /= metrics['count']
        
        # Sauvegarde benchmark
        benchmark_file = self.reports_dir / f"benchmark_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.json"
        with open(benchmark_file, 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, indent=2, ensure_ascii=False)
        
        self.log(f"ğŸ“Š Benchmark sauvegardÃ©: {benchmark_file}")
        
        return benchmark_results


def main():
    """Fonction principale de dÃ©monstration"""
    print("ğŸ§¬ VALIDATEUR PANINI FS")
    print("=" * 60)
    print("Framework validation multi-format avec intÃ©gritÃ© 100%")
    print("=" * 60)
    
    # Initialisation validateur
    validator = PaniniFSValidator()
    
    # Affichage formats supportÃ©s
    print("\nğŸ“‹ Formats supportÃ©s:")
    for category, formats in validator.supported_formats.items():
        print(f"   {category.upper()}: {', '.join(formats)}")
    
    print(f"\nâœ… Validateur prÃªt")
    print(f"ğŸ“ Workspace: {validator.workspace}")
    print("\nUtilisez la classe PaniniFSValidator pour valider vos fichiers")


if __name__ == '__main__':
    main()
