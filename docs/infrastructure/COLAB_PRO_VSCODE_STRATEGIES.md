# ðŸš€ StratÃ©gies AvancÃ©es Colab Pro + VSCode

**ProblÃ©matique**: 
- Code GPU impossible Ã  dÃ©bugger localement
- SSH vers Colab trop fragile (dÃ©connexions)
- GitHub Copilot fonctionne mieux dans VSCode
- Jobs asynchrones difficiles Ã  orchestrer

**Solutions ProposÃ©es**: 5 architectures du plus simple au plus robuste

---

## ðŸŽ¯ Solution 1: VSCode Remote Tunnels (RECOMMANDÃ‰)

### Architecture
```
VSCode Local â†’ VSCode Server (Colab) â†’ GPU Kernel
     â†“              â†“                        â†“
  Copilot      Code Sync              ExÃ©cution
```

### Avantages
âœ… **Meilleure stabilitÃ©** que SSH direct  
âœ… **Copilot natif** dans VSCode  
âœ… **Reconnexion automatique** aprÃ¨s dÃ©connexions  
âœ… **Port forwarding** automatique  
âœ… **Extension sync** (Pylance, Python, etc.)

### Setup Colab Pro

#### Notebook Initial (Une seule cellule)
```python
# Cell 1: Setup VSCode Server + Tunnel
!curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' \
  --output vscode_cli.tar.gz
!tar -xf vscode_cli.tar.gz

# IMPORTANT: Sur Colab, il faut d'abord s'authentifier, PUIS lancer le tunnel
# Ã‰tape 1: Authentification GitHub (device flow - pas de callback)
!./code tunnel user login --provider github

# Ã‰tape 2: Lancer le tunnel (aprÃ¨s auth rÃ©ussie)
!./code tunnel --accept-server-license-terms --name colab-panini-gpu

# Alternative tout-en-un (peut avoir problÃ¨me callback sur Colab):
# !./code tunnel --accept-server-license-terms --name colab-panini-gpu
```

**Output attendu**:
```
To grant access to the server, please log into https://github.com/login/device 
and use code: XXXX-XXXX

[AprÃ¨s auth GitHub]
âœ“ Tunnel successfully created

Open in VS Code (Web): https://vscode.dev/tunnel/colab-panini-gpu
Open in VS Code (Desktop): vscode://vscode-remote/tunnel/colab-panini-gpu
```

#### Option A: VSCode Local Desktop (RECOMMANDÃ‰ - Copilot complet)

**Setup une seule fois**:
1. Dans VSCode local, installer extension: **Remote - Tunnels** (`ms-vscode.remote-server`)
2. `Ctrl+Shift+P` â†’ `Remote-Tunnels: Connect to Tunnel`
3. Login avec votre compte GitHub
4. SÃ©lectionner `colab-panini-gpu` dans la liste
5. **Nouvelle fenÃªtre VSCode s'ouvre connectÃ©e Ã  Colab!** ðŸš€

**Avantages**:
âœ… Toutes vos extensions locales (Copilot, Pylance, thÃ¨mes, keybindings)
âœ… Performance meilleure (rendering local)
âœ… Pas de limitation navigateur
âœ… Multi-workspace, split editors, etc.

#### Option B: VSCode Web (rapide mais limitÃ©)

**Si vous n'avez pas VSCode installÃ© localement**:
1. Cliquer sur le lien `https://vscode.dev/tunnel/colab-panini-gpu`
2. Login GitHub si demandÃ©
3. **VSCode dans le navigateur!**

**Limitations**:
âš ï¸ Extensions limitÃ©es (Copilot fonctionne mais pas toutes les extensions)
âš ï¸ Performance moindre (rendering dans navigateur)
âš ï¸ Pas de terminal local

### Workflow de DÃ©veloppement
```python
# Structure projet Colab
/content/
â”œâ”€â”€ work/              # Code sync depuis GitHub
â”‚   â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ data/              # Datasets cachÃ©s
â””â”€â”€ outputs/           # RÃ©sultats â†’ Google Drive

# Cell 2: Clone repo + install deps
!git clone https://github.com/stephanedenis/Panini.git work
%cd work
!pip install -r requirements.txt -q

# Cell 3: Mount Google Drive (persistance)
from google.colab import drive
drive.mount('/content/drive')
!ln -s /content/drive/MyDrive/colab_outputs /content/outputs

# Cell 4: Setup dev tools
!pip install ipdb pytest-gpu -q  # Debugger + tests GPU
```

### DÃ©veloppement Typique
1. **VSCode Local**: Ã‰diter code avec Copilot
2. **Auto-sync**: Tunnel sync les fichiers vers Colab
3. **Terminal Colab**: ExÃ©cuter avec GPU (`python train.py --gpu`)
4. **Debug**: Breakpoints fonctionnent! (via `ipdb`)
5. **Outputs**: SauvegardÃ©s dans Google Drive

### Robustesse
- **DÃ©connexion**: Tunnel reconnecte automatiquement
- **Perte VM**: Script redÃ©marre tunnel (voir Cell 1)
- **Ã‰tat**: Checkpoints dans Google Drive

---

## ðŸ”„ Solution 2: Hybrid Local Dev + Remote Exec

### Architecture
```
Local VSCode â†’ Git Push â†’ Colab Pulls â†’ GPU Exec â†’ Results Sync
    â†“            â†“           â†“             â†“            â†“
  Copilot     GitHub    Auto-fetch    Training     Drive/S3
```

### Principe
- **DÃ©velopper 100% localement** (VSCode + Copilot)
- **Commit â†’ Push** quand prÃªt Ã  tester GPU
- **Colab auto-pull** et exÃ©cute
- **RÃ©sultats sync** vers storage accessible

### Setup Colab (Daemon)

```python
# Cell 1: Configuration
REPO_URL = "https://github.com/stephanedenis/Panini.git"
BRANCH = "gpu-experiments"  # Branche dÃ©diÃ©e expÃ©riences
WATCH_INTERVAL = 60  # Secondes entre checks

# Cell 2: Daemon qui watch le repo
import subprocess
import time
import os
from pathlib import Path

def setup_repo():
    """Clone ou update repo"""
    if Path("work").exists():
        os.chdir("work")
        subprocess.run(["git", "fetch", "origin", BRANCH])
        subprocess.run(["git", "reset", "--hard", f"origin/{BRANCH}"])
    else:
        subprocess.run(["git", "clone", "-b", BRANCH, REPO_URL, "work"])
        os.chdir("work")
    
    # Install/update deps si requirements.txt a changÃ©
    subprocess.run(["pip", "install", "-r", "requirements.txt", "-q"])

def get_latest_commit():
    """RÃ©cupÃ¨re hash du dernier commit"""
    result = subprocess.run(
        ["git", "rev-parse", "HEAD"],
        capture_output=True, text=True
    )
    return result.stdout.strip()

def run_experiments():
    """ExÃ©cute les expÃ©riences dÃ©finies dans experiments.json"""
    import json
    
    if not Path("experiments.json").exists():
        print("â­ï¸  Pas d'expÃ©riences Ã  lancer")
        return
    
    with open("experiments.json") as f:
        experiments = json.load(f)
    
    for exp in experiments:
        if exp.get("status") == "pending":
            print(f"ðŸš€ Lancement: {exp['name']}")
            
            # ExÃ©cuter avec timeout
            try:
                result = subprocess.run(
                    exp["command"],
                    shell=True,
                    timeout=exp.get("timeout", 3600),
                    capture_output=True,
                    text=True
                )
                
                # Sauvegarder rÃ©sultats
                exp["status"] = "completed" if result.returncode == 0 else "failed"
                exp["output"] = result.stdout
                exp["error"] = result.stderr
                
                print(f"âœ… TerminÃ©: {exp['name']}")
            
            except subprocess.TimeoutExpired:
                exp["status"] = "timeout"
                print(f"â±ï¸ Timeout: {exp['name']}")
    
    # Mettre Ã  jour experiments.json
    with open("experiments.json", "w") as f:
        json.dump(experiments, f, indent=2)
    
    # Commit rÃ©sultats
    subprocess.run(["git", "add", "experiments.json", "outputs/"])
    subprocess.run([
        "git", "commit", "-m", 
        f"results: ExpÃ©riences complÃ©tÃ©es sur Colab"
    ])
    subprocess.run(["git", "push", "origin", BRANCH])

# Cell 3: Boucle principale
last_commit = None
print("ðŸ”„ Daemon dÃ©marrÃ© - Watching repo...")

while True:
    try:
        setup_repo()
        current_commit = get_latest_commit()
        
        if current_commit != last_commit:
            print(f"ðŸ†• Nouveau commit dÃ©tectÃ©: {current_commit[:8]}")
            run_experiments()
            last_commit = current_commit
        else:
            print("â³ Aucun changement...")
        
        time.sleep(WATCH_INTERVAL)
    
    except KeyboardInterrupt:
        print("ðŸ›‘ Daemon arrÃªtÃ©")
        break
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        time.sleep(WATCH_INTERVAL)
```

### Workflow Local (VSCode)

```bash
# 1. CrÃ©er expÃ©rience
cat > experiments.json <<EOF
[
  {
    "name": "test_audio_fingerprint_gpu",
    "command": "python tests/test_audio_fingerprinting_gpu.py",
    "status": "pending",
    "timeout": 600
  }
]
EOF

# 2. Commit + Push
git add experiments.json
git commit -m "exp: Test audio fingerprinting avec GPU"
git push origin gpu-experiments

# 3. Attendre rÃ©sultats (Colab daemon pull + exec + push)
# 4. Pull rÃ©sultats
git pull origin gpu-experiments

# 5. Voir rÃ©sultats
cat experiments.json  # Status + outputs
ls outputs/  # Fichiers gÃ©nÃ©rÃ©s
```

### Avantages
âœ… **DÃ©veloppement 100% local** (VSCode + Copilot)  
âœ… **Aucune connexion SSH** fragile  
âœ… **Async naturel** (push â†’ background exec)  
âœ… **Reproducible** (tout dans Git)  
âœ… **Scalable** (plusieurs Colab instances possibles)

---

## ðŸŽ¨ Solution 3: Mock GPU Localement + Validation Remote

### Principe
```
Local Dev â†’ Mock GPU (CPU fallback) â†’ Tests passent â†’ Push â†’ Colab GPU (validation finale)
```

### Mock GPU Local

```python
# utils/gpu_mock.py
"""Mock GPU pour dÃ©veloppement local"""
import torch
import numpy as np

class MockGPU:
    """Simule API GPU sur CPU"""
    
    @staticmethod
    def is_available():
        return False  # Force CPU localement
    
    @staticmethod
    def device_count():
        return 0
    
    @staticmethod
    def get_device_name(idx=0):
        return "CPU (Mocked GPU)"

# Patch torch.cuda au dÃ©marrage
if not torch.cuda.is_available():
    print("ðŸ”§ GPU Mock activÃ© - DÃ©veloppement CPU")
    torch.cuda = MockGPU
```

### Code GPU-agnostic

```python
# experiments/audio_gpu_processing.py
import torch
from utils.gpu_mock import MockGPU

def get_device():
    """Retourne device appropriÃ© (GPU si dispo, sinon CPU)"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        print("âš ï¸ CPU: GPU non disponible (mock actif)")
    return device

def process_audio_batch(audio_data, model):
    """Traitement batch audio (GPU ou CPU)"""
    device = get_device()
    
    # Conversion tensors
    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).to(device)
    model = model.to(device)
    
    # Traitement (mÃªme code GPU/CPU!)
    with torch.no_grad():
        features = model(audio_tensor)
    
    return features.cpu().numpy()  # Toujours retourner sur CPU

# Tests locaux (CPU mock)
if __name__ == "__main__":
    import numpy as np
    
    # Test avec donnÃ©es synthÃ©tiques
    audio = np.random.randn(1000, 44100)  # 1000 fichiers
    model = torch.nn.Linear(44100, 128)  # Model simple
    
    # ExÃ©cute sur device disponible (CPU local, GPU Colab)
    features = process_audio_batch(audio, model)
    print(f"âœ… Features extraites: {features.shape}")
```

### Workflow
1. **Local**: DÃ©velopper avec mock GPU (tests CPU rapides)
2. **VSCode**: Copilot suggÃ¨re code PyTorch standard
3. **Tests locaux**: Valident logique (pas performance)
4. **Push**: Code vers branche `gpu-experiments`
5. **Colab**: ExÃ©cute avec vrai GPU (validation perf)

### Avantages
âœ… **Debug rapide** localement (CPU)  
âœ… **Copilot optimal** (PyTorch standard)  
âœ… **Tests unitaires** exÃ©cutables partout  
âœ… **Validation GPU** seulement si nÃ©cessaire  

---

## ðŸ”¬ Solution 4: Jupyter Extension + Remote Kernel

### Architecture
```
VSCode Local â†’ Jupyter Extension â†’ Colab Kernel (GPU)
    â†“              â†“                      â†“
  .ipynb       Remote URL           GPU Exec
```

### Setup

#### 1. Colab: Exposer Kernel via ngrok
```python
# Cell 1: Setup Jupyter + ngrok
!pip install jupyter jupyter_http_over_ws -q
!jupyter serverextension enable --py jupyter_http_over_ws

# Lancer Jupyter avec token
import subprocess
import threading

def run_jupyter():
    subprocess.run([
        "jupyter", "notebook",
        "--NotebookApp.allow_origin='*'",
        "--port=8888",
        "--NotebookApp.token='YOUR_SECRET_TOKEN'",
        "--no-browser"
    ])

# Background thread
thread = threading.Thread(target=run_jupyter, daemon=True)
thread.start()

# Cell 2: Exposer avec ngrok
!pip install pyngrok -q
from pyngrok import ngrok

public_url = ngrok.connect(8888)
print(f"ðŸ”— Jupyter URL: {public_url}")
print("   Token: YOUR_SECRET_TOKEN")
```

#### 2. VSCode Local: Connecter Remote Kernel
1. Installer extension "Jupyter"
2. CrÃ©er notebook `.ipynb`
3. "Select Kernel" â†’ "Existing Jupyter Server"
4. Coller URL ngrok + token
5. **VSCode utilise GPU Colab directement!**

### Workflow
```python
# Dans VSCode (notebook local connectÃ© Ã  Colab GPU)

# Cell 1: VÃ©rifier GPU
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
# Output: GPU: Tesla T4 (Colab)

# Cell 2: DÃ©velopper avec Copilot
# Copilot suggÃ¨re code â†’ Exec sur GPU immÃ©diatement!

def train_model(data, epochs=10):
    # Copilot auto-complete avec GPU context
    device = torch.device("cuda")
    model = MyModel().to(device)
    # ... training loop ...

# Cell 3: Tester interactivement
train_model(train_data, epochs=5)  # ExÃ©cute sur Colab GPU!
```

### Avantages
âœ… **Interactif** (REPL GPU en live)  
âœ… **Copilot** dans VSCode  
âœ… **Pas de sync** fichiers (kernel remote)  
âœ… **Debug visuel** (variables, plots)

### InconvÃ©nients
âš ï¸ **StabilitÃ© ngrok** (gratuit = 8h max)  
âš ï¸ **Latence** (chaque cell â†’ rÃ©seau)

---

## ðŸ—ï¸ Solution 5: DevContainer avec GPU Passthrough (Local)

### Pour Ceux Avec GPU Local LimitÃ©

Si vous avez une GPU locale (mÃªme ancienne), utilisez DevContainer:

```json
// .devcontainer/devcontainer.json
{
  "name": "Panini GPU Dev",
  "image": "pytorch/pytorch:latest",
  "runArgs": ["--gpus=all"],
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "GitHub.copilot",
        "ms-toolsai.jupyter"
      ]
    }
  },
  "postCreateCommand": "pip install -r requirements.txt"
}
```

**Avantages**:
- VSCode natif + GPU local
- Pas de latence rÃ©seau
- Copilot optimal

**Pour AMD GPU** (votre RX 480):
```bash
# Installer ROCm pour PyTorch
docker run -it --device=/dev/kfd --device=/dev/dri \
  rocm/pytorch:latest bash
```

---

## ðŸŽ¯ Recommandation Finale

### Pour Votre Cas (RX 480 local + Colab Pro)

**StratÃ©gie Hybride** (Solutions 2 + 3):

#### Phase 1: DÃ©veloppement (Local)
```
VSCode + Copilot â†’ Mock GPU (CPU) â†’ Tests unitaires
```

#### Phase 2: Validation (Colab Pro)
```
Push â†’ Colab Daemon â†’ GPU Exec â†’ Results Pull
```

#### Phase 3: ExpÃ©riences Longues (Colab Pro)
```
VSCode Remote Tunnel â†’ Edit code â†’ Exec GPU â†’ Checkpoints Drive
```

### Scripts Ã  CrÃ©er

#### 1. `tools/colab_daemon_setup.py`
Script pour lancer daemon Colab (Solution 2)

#### 2. `utils/gpu_mock.py`
Mock GPU pour dev local (Solution 3)

#### 3. `experiments/template.json`
Template pour dÃ©finir expÃ©riences

#### 4. `tools/sync_results.sh`
Pull rÃ©sultats depuis Colab

---

## ðŸ“Š Comparaison Solutions

| Solution | StabilitÃ© | Copilot | Latence | ComplexitÃ© |
|----------|-----------|---------|---------|------------|
| **1. VSCode Tunnel** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| **2. Git Push/Pull** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **3. Mock GPU** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| **4. Remote Kernel** | â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ |
| **5. DevContainer** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |

**LÃ©gende**: â­ = Mauvais â†’ â­â­â­â­â­ = Excellent

---

## ðŸš€ Action Plan

### Semaine 1: Setup Infrastructure
1. CrÃ©er branche `gpu-experiments`
2. Setup VSCode Remote Tunnel (Solution 1)
3. CrÃ©er mock GPU (Solution 3)
4. Tester workflow hybride

### Semaine 2: Automatisation
1. Script daemon Colab (Solution 2)
2. Template experiments.json
3. CI/CD pour validation GPU

### Semaine 3: Production
1. Audio fingerprinting GPU
2. Batch processing Ã  grande Ã©chelle
3. Benchmarks performance

---

**Voulez-vous que je crÃ©e les scripts pour une solution spÃ©cifique?**
