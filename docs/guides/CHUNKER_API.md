# üìö PaniniFS Chunker - API Documentation

**Version**: 0.2.0  
**Date**: 2025-11-13  
**Module**: `modules/core/filesystem/src/panini_fs_chunker.py`

---

## üéØ Overview

Le **PaniniFS Chunker** d√©coupe fichiers en chunks s√©mantiquement coh√©rents bas√©s sur leur structure interne, permettant une compression GPU-acc√©l√©r√©e optimale et une reconstruction bit-perfect.

**Philosophie**: D√©coupage **intelligent** bas√© sur la s√©mantique du format (pas de taille fixe arbitraire).

---

## üöÄ Quick Start

```python
from panini_fs_chunker import FormatDetector, SemanticChunker

# 1. D√©tecter format
data = Path('video.mp4').read_bytes()
format_name, media_type, grammar_id = FormatDetector.detect(data)
print(f"Format: {format_name} ({media_type}), Grammar: {grammar_id}")

# 2. Cr√©er chunker
chunker = SemanticChunker(grammar_id=grammar_id)

# 3. D√©couper en chunks s√©mantiques
chunks = chunker.chunk(data)

# 4. Analyser r√©sultats
for i, (offset, size, pattern) in enumerate(chunks):
    print(f"Chunk {i}: offset={offset}, size={size}, pattern={pattern}")
```

**Output exemple (MP4)**:
```
Format: MP4 (video), Grammar: mp4_v1
Chunk 0: offset=0, size=24, pattern=ISOBMFF_FTYP_FILETYPE
Chunk 1: offset=24, size=116, pattern=ISOBMFF_MOOV_METADATA
Chunk 2: offset=140, size=264, pattern=ISOBMFF_MDAT_MEDIA
```

---

## üìñ API Reference

### 1. `FormatDetector`

D√©tecte automatiquement le format d'un fichier binaire.

#### `FormatDetector.detect(data: bytes) -> Tuple[str, str, str]`

**Param√®tres**:
- `data`: Donn√©es binaires du fichier (au moins 512 premiers bytes recommand√©s)

**Retour**: Tuple `(format_name, media_type, grammar_id)`
- `format_name`: Nom du format ("PNG", "MP4", "WebM", etc.)
- `media_type`: Type de m√©dia ("image", "video", "audio", "document")
- `grammar_id`: ID de grammaire pour le chunker ("png_v1", "mp4_v1", etc.)

**Formats support√©s** (25+):

| Format | Media Type | Grammar ID | Description |
|--------|------------|------------|-------------|
| **Images** |
| PNG | image | `png_v1` | PNG avec chunks IHDR/IDAT/IEND |
| JPEG | image | `jpeg_v1` | JPEG avec segments SOI/APP/DQT/SOS |
| GIF | image | `gif_v1` | GIF87a/89a avec images/extensions |
| WebP | image | `webp_v1` | WebP (RIFF container) |
| TIFF | image | `tiff_v1` | TIFF avec IFD structures |
| BMP | image | `bmp_v1` | Bitmap Windows |
| **Vid√©o** |
| MP4 | video | `mp4_v1` | ISO BMFF (mp41/mp42/isom) |
| MOV | video | `mov_v1` | QuickTime (qt brand) |
| WebM | video | `webm_v1` | WebM (EBML/Matroska) |
| MKV | video | `mkv_v1` | Matroska (d√©tect√© comme WebM) |
| AVI | video | `avi_v1` | AVI (RIFF video) |
| **Audio** |
| WAV | audio | `wav_v1` | WAV (RIFF audio) |
| MP3 | audio | `mp3_v1` | MP3 avec frames ID3 |
| **Documents** |
| PDF | document | `pdf_v1` | PDF avec objects/xref/trailer |
| ZIP | archive | `zip_v1` | ZIP avec central directory |
| GZIP | archive | `gzip_v1` | GZIP compression |

**Exemple**:
```python
# PNG detection
png_data = b'\x89PNG\r\n\x1a\n...'
format_name, media_type, grammar_id = FormatDetector.detect(png_data)
# ‚Üí ("PNG", "image", "png_v1")

# MP4 detection
mp4_data = b'\x00\x00\x00\x20ftyp...'
format_name, media_type, grammar_id = FormatDetector.detect(mp4_data)
# ‚Üí ("MP4", "video", "mp4_v1")

# Format inconnu
unknown_data = b'\xff\xff\xff\xff...'
result = FormatDetector.detect(unknown_data)
# ‚Üí None (format non support√©)
```

---

### 2. `SemanticChunker`

D√©coupe fichiers en chunks s√©mantiques bas√©s sur leur structure.

#### `SemanticChunker(grammar_id: str)`

**Constructeur**:
- `grammar_id`: ID de grammaire retourn√© par `FormatDetector.detect()`

**Exemple**:
```python
chunker = SemanticChunker(grammar_id='mp4_v1')
```

#### `chunk(data: bytes, max_chunk_size: int = 1MB) -> List[Tuple[int, int, str]]`

D√©coupe donn√©es en chunks s√©mantiques.

**Param√®tres**:
- `data`: Donn√©es binaires compl√®tes du fichier
- `max_chunk_size`: Taille max d'un chunk (d√©faut: 1MB, pour split de gros blobs)

**Retour**: Liste de tuples `(offset, size, pattern)`
- `offset`: Position de d√©but du chunk (bytes)
- `size`: Taille du chunk (bytes)
- `pattern`: Type s√©mantique du chunk (ex: "PNG_IHDR_HEADER")

**Exemple d√©taill√© (PNG)**:
```python
png_data = Path('image.png').read_bytes()
chunker = SemanticChunker(grammar_id='png_v1')
chunks = chunker.chunk(png_data)

# R√©sultat:
# [
#   (0, 8, 'PNG_SIGNATURE'),           # Magic number
#   (8, 25, 'PNG_IHDR_HEADER'),        # Image header
#   (33, 256, 'PNG_IDAT_DATA'),        # Image data (compressed)
#   (289, 12, 'PNG_IEND_TRAILER')      # End marker
# ]

# V√©rification coverage
total_size = sum(size for _, size, _ in chunks)
assert total_size == len(png_data)  # ‚úÖ Coverage 100%
```

---

## üé® Patterns S√©mantiques par Format

### PNG (Portable Network Graphics)

```python
patterns = [
    'PNG_SIGNATURE',        # 8 bytes: \x89PNG\r\n\x1a\n
    'PNG_IHDR_HEADER',      # Image dimensions, bit depth, color type
    'PNG_PLTE_PALETTE',     # Palette colors (si indexed)
    'PNG_IDAT_DATA',        # Image data (DEFLATE compressed)
    'PNG_tEXt_METADATA',    # Text metadata
    'PNG_IEND_TRAILER'      # End of file
]
```

**Structure**:
```
Signature (8) ‚Üí IHDR ‚Üí [chunks...] ‚Üí IDAT (n fois) ‚Üí IEND
```

### JPEG (Joint Photographic Experts Group)

```python
patterns = [
    'JPEG_SOI_START',       # Start of Image (0xFFD8)
    'JPEG_APP0_JFIF',       # JFIF application data
    'JPEG_DQT_QUANT',       # Quantization tables
    'JPEG_DHT_HUFFMAN',     # Huffman tables
    'JPEG_SOS_SCAN',        # Start of Scan
    'JPEG_SCAN_DATA',       # Compressed image data
    'JPEG_EOI_END'          # End of Image (0xFFD9)
]
```

### MP4/MOV (ISO Base Media File Format)

```python
patterns = [
    'ISOBMFF_FTYP_FILETYPE',  # File type/brand (mp42, qt, etc.)
    'ISOBMFF_MOOV_METADATA',  # Movie metadata (moov box)
    'ISOBMFF_MDAT_MEDIA',     # Media data (mdat box)
    'ISOBMFF_FREE_PADDING',   # Free space (optional)
]
```

**Structure ISO BMFF**:
```
ftyp (file type) ‚Üí moov (metadata) ‚Üí mdat (media data)

moov contient:
  - mvhd: Movie header (duration, timescale)
  - trak: Tracks (video/audio)
    - tkhd: Track header
    - mdia: Media info
      - minf: Media information
        - stbl: Sample table
          - stss: Sync sample table (KEYFRAMES!)
          - stsc: Sample-to-chunk
          - stsz: Sample sizes
          - stco: Chunk offsets
```

### WebM/MKV (EBML/Matroska)

```python
patterns = [
    'EBML_HEADER',          # EBML header (0x1A45DFA3)
    'EBML_SEGMENT',         # Segment container
    'EBML_SEEKHEAD',        # Index des positions
    'EBML_INFO',            # Segment info (duration, etc.)
    'EBML_TRACKS',          # Track definitions
    'EBML_CLUSTER',         # Cluster de frames (r√©p√©t√©)
]
```

**Structure EBML**:
```
EBML Header ‚Üí Segment
  ‚îú‚îÄ‚îÄ SeekHead (index)
  ‚îú‚îÄ‚îÄ Info (metadata)
  ‚îú‚îÄ‚îÄ Tracks (d√©finitions pistes)
  ‚îî‚îÄ‚îÄ Cluster (donn√©es frames) [r√©p√©t√©]
```

### AVI (Audio Video Interleave - RIFF)

```python
patterns = [
    'AVI_RIFF_HEADER',       # RIFF container header
    'AVI_LIST_HEADERS',      # LIST hdrl (headers)
    'AVI_LIST_MOVIE_DATA',   # LIST movi (movie data)
    'AVI_INDEX'              # idx1 (old-style index)
]
```

**Structure RIFF AVI**:
```
RIFF AVI 
  ‚îú‚îÄ‚îÄ LIST hdrl (headers)
  ‚îÇ   ‚îú‚îÄ‚îÄ avih (AVI header)
  ‚îÇ   ‚îî‚îÄ‚îÄ LIST strl (stream list)
  ‚îú‚îÄ‚îÄ LIST movi (movie data)
  ‚îÇ   ‚îú‚îÄ‚îÄ ##dc (video chunk)
  ‚îÇ   ‚îî‚îÄ‚îÄ ##wb (audio chunk)
  ‚îî‚îÄ‚îÄ idx1 (index)
```

### PDF (Portable Document Format)

```python
patterns = [
    'PDF_HEADER',           # %PDF-1.x
    'PDF_OBJECT',           # Object definitions (r√©p√©t√©)
    'PDF_STREAM',           # Stream data (compressed)
    'PDF_XREF',             # Cross-reference table
    'PDF_TRAILER',          # Trailer dict
    'PDF_EOF'               # %%EOF
]
```

---

## üîß Advanced Usage

### Chunking avec M√©tadonn√©es

```python
from panini_fs_chunker import FormatDetector, SemanticChunker
from dataclasses import asdict
import json

def chunk_with_metadata(file_path: Path, output_dir: Path):
    """Chunk file et sauvegarde avec m√©tadonn√©es compl√®tes"""
    
    # Lire fichier
    data = file_path.read_bytes()
    original_hash = hashlib.sha256(data).hexdigest()
    
    # D√©tecter format
    format_info = FormatDetector.detect(data)
    if not format_info:
        raise ValueError(f"Format non support√©: {file_path}")
    
    format_name, media_type, grammar_id = format_info
    
    # Chunker
    chunker = SemanticChunker(grammar_id=grammar_id)
    chunks = chunker.chunk(data)
    
    # Sauvegarder chunks avec m√©tadonn√©es
    base_name = file_path.stem
    chunks_dir = output_dir / base_name
    chunks_dir.mkdir(parents=True, exist_ok=True)
    
    for i, (offset, size, pattern) in enumerate(chunks):
        chunk_dir = chunks_dir / f"chunk_{i:04d}"
        chunk_dir.mkdir(exist_ok=True)
        
        # Donn√©es binaires
        chunk_data = data[offset:offset+size]
        (chunk_dir / 'data.bin').write_bytes(chunk_data)
        
        # M√©tadonn√©es
        metadata = {
            'chunk_id': i,
            'offset': offset,
            'size': size,
            'pattern': pattern,
            'original_hash': hashlib.sha256(chunk_data).hexdigest(),
            'format': format_name,
            'media_type': media_type,
            'grammar_id': grammar_id,
            'file_original_hash': original_hash,
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        }
        (chunk_dir / 'metadata.json').write_text(json.dumps(metadata, indent=2))
    
    # Manifest global
    manifest = {
        'source_file': str(file_path),
        'format': format_name,
        'media_type': media_type,
        'grammar_id': grammar_id,
        'total_chunks': len(chunks),
        'total_size': len(data),
        'original_hash': original_hash,
        'chunks': [
            {'id': i, 'offset': o, 'size': s, 'pattern': p}
            for i, (o, s, p) in enumerate(chunks)
        ]
    }
    (chunks_dir / 'manifest.json').write_text(json.dumps(manifest, indent=2))
    
    print(f"‚úÖ Chunked {file_path.name}: {len(chunks)} chunks ‚Üí {chunks_dir}")
    return chunks_dir
```

### Validation Coverage

```python
def validate_chunk_coverage(data: bytes, chunks: List[Tuple[int, int, str]]) -> bool:
    """Valide que chunks couvrent 100% du fichier sans gaps/overlaps"""
    
    if not chunks:
        return False
    
    # Trier par offset
    sorted_chunks = sorted(chunks, key=lambda x: x[0])
    
    # V√©rifier continuit√©
    expected_offset = 0
    for offset, size, pattern in sorted_chunks:
        if offset != expected_offset:
            print(f"‚ùå Gap d√©tect√©: {expected_offset} ‚Üí {offset}")
            return False
        expected_offset = offset + size
    
    # V√©rifier couverture compl√®te
    if expected_offset != len(data):
        print(f"‚ùå Coverage incomplet: {expected_offset}/{len(data)} bytes")
        return False
    
    print(f"‚úÖ Coverage 100% valid√©: {len(data)} bytes")
    return True

# Usage
chunks = chunker.chunk(data)
assert validate_chunk_coverage(data, chunks)
```

### Reconstruction Bit-Perfect

```python
def reconstruct_from_chunks(chunks_dir: Path) -> bytes:
    """Reconstruit fichier original depuis chunks"""
    
    # Charger manifest
    manifest_file = chunks_dir / 'manifest.json'
    with open(manifest_file) as f:
        manifest = json.load(f)
    
    # Charger chunks dans l'ordre
    reconstructed = bytearray()
    for i in range(manifest['total_chunks']):
        chunk_dir = chunks_dir / f"chunk_{i:04d}"
        chunk_data = (chunk_dir / 'data.bin').read_bytes()
        reconstructed.extend(chunk_data)
    
    # Valider hash
    reconstructed_hash = hashlib.sha256(reconstructed).hexdigest()
    original_hash = manifest['original_hash']
    
    if reconstructed_hash != original_hash:
        raise ValueError(f"Hash mismatch! Expected {original_hash}, got {reconstructed_hash}")
    
    print(f"‚úÖ Reconstruction bit-perfect valid√©e: {len(reconstructed)} bytes")
    return bytes(reconstructed)
```

---

## üéØ Performance & Best Practices

### Performances Typiques

| Format | Taille | Chunks | Temps | Throughput |
|--------|--------|--------|-------|------------|
| PNG 1MB | 1,048,576 | 12 | 15ms | 70 MB/s |
| JPEG 2MB | 2,097,152 | 8 | 20ms | 105 MB/s |
| MP4 10MB | 10,485,760 | 3 | 80ms | 131 MB/s |
| PDF 5MB | 5,242,880 | 150 | 120ms | 44 MB/s |

**Note**: Temps mesur√© sur CPU i7-10700K, sans compression GPU.

### Best Practices

#### ‚úÖ DO

1. **Toujours valider coverage**:
   ```python
   assert validate_chunk_coverage(data, chunks)
   ```

2. **Sauvegarder m√©tadonn√©es compl√®tes** (hash, offset, pattern):
   ```python
   metadata = {
       'chunk_id': i,
       'offset': offset,
       'size': size,
       'pattern': pattern,
       'original_hash': hashlib.sha256(chunk_data).hexdigest()
   }
   ```

3. **Utiliser grammar_id correct**:
   ```python
   format_info = FormatDetector.detect(data)
   if format_info:
       chunker = SemanticChunker(grammar_id=format_info[2])
   ```

4. **Streamer gros fichiers** (>1GB):
   ```python
   # Lire par blocs pour √©viter OOM
   with open(file_path, 'rb') as f:
       header = f.read(4096)  # D√©tecter format
       format_info = FormatDetector.detect(header)
       # ... chunker avec stream
   ```

#### ‚ùå DON'T

1. **Ne pas utiliser taille fixe arbitraire**:
   ```python
   # ‚ùå BAD: D√©coupage arbitraire
   chunks = [(i*64*1024, 64*1024, 'FIXED_SIZE') for i in range(n)]
   
   # ‚úÖ GOOD: D√©coupage s√©mantique
   chunks = chunker.chunk(data)  # Bas√© sur structure
   ```

2. **Ne pas ignorer format detection**:
   ```python
   # ‚ùå BAD: Forcer un grammar_id
   chunker = SemanticChunker(grammar_id='mp4_v1')
   chunker.chunk(png_data)  # R√©sultat incorrect!
   
   # ‚úÖ GOOD: D√©tecter automatiquement
   format_info = FormatDetector.detect(data)
   chunker = SemanticChunker(grammar_id=format_info[2])
   ```

3. **Ne pas m√©langer chunks de diff√©rents fichiers**:
   ```python
   # ‚ùå BAD: Chunks sans contexte
   all_chunks = []
   for file in files:
       all_chunks.extend(chunker.chunk(file.read_bytes()))
   
   # ‚úÖ GOOD: S√©parer par fichier avec manifest
   for file in files:
       chunks = chunker.chunk(file.read_bytes())
       save_with_manifest(file, chunks)
   ```

---

## üêõ Troubleshooting

### Format non d√©tect√©

**Probl√®me**: `FormatDetector.detect()` retourne `None`

**Solutions**:
1. V√©rifier que le fichier est valide (pas corrompu)
2. Lire au moins 512 premiers bytes pour d√©tection
3. V√©rifier que le format est support√© (voir table ci-dessus)
4. Ajouter support custom si besoin (voir extension)

### Chunks incomplets

**Probl√®me**: Coverage < 100%

**Solutions**:
1. V√©rifier que `data` contient le fichier complet
2. Utiliser `validate_chunk_coverage()` pour debug
3. V√©rifier logs de parsing (warnings de chunks ignor√©s)

### Performance lente

**Probl√®me**: Chunking prend trop de temps

**Solutions**:
1. Utiliser streaming pour fichiers >1GB
2. V√©rifier qu'il n'y a pas de regex complexes dans patterns
3. Profiler avec `cProfile` pour identifier bottlenecks
4. Consid√©rer version Rust (5-10x plus rapide)

---

## üîå Extension: Ajouter un Nouveau Format

### 1. Ajouter d√©tection dans `FormatDetector`

```python
class FormatDetector:
    MAGIC_NUMBERS = {
        # ... existants ...
        b'\x50\x4b\x03\x04': ('ZIP', 'archive'),  # Nouveau format
    }
    
    @classmethod
    def detect(cls, data: bytes) -> Optional[Tuple[str, str, str]]:
        # Ajouter logique custom si n√©cessaire
        if data[:4] == b'\x50\x4b\x03\x04':
            return ('ZIP', 'archive', 'zip_v1')
        # ...
```

### 2. Impl√©menter m√©thode de chunking

```python
class SemanticChunker:
    def _chunk_zip(self, data: bytes) -> List[Tuple[int, int, str]]:
        """Chunk ZIP avec local file headers + central directory"""
        chunks = []
        offset = 0
        
        # Parse local file headers
        while offset < len(data) - 4:
            sig = struct.unpack('<I', data[offset:offset+4])[0]
            
            if sig == 0x04034b50:  # Local file header
                # Parse header
                size = self._parse_zip_local_header(data, offset)
                chunks.append((offset, size, 'ZIP_LOCAL_FILE'))
                offset += size
            
            elif sig == 0x02014b50:  # Central directory
                size = self._parse_zip_central_dir(data, offset)
                chunks.append((offset, size, 'ZIP_CENTRAL_DIR'))
                break
        
        return chunks
```

### 3. Enregistrer dans dispatch

```python
def chunk(self, data: bytes, ...) -> List[Tuple[int, int, str]]:
    if self.grammar_id == 'zip_v1':
        return self._chunk_zip(data)
    # ... autres formats
```

---

## üìö Ressources

### Code Source
- **Main module**: `modules/core/filesystem/src/panini_fs_chunker.py`
- **Tests**: `tools/validation/test_video_formats.py`
- **Validation**: `tools/validation/reconstruction_validator.py`

### Documentation Connexe
- [RECONSTRUCTION_RECIPES.md](RECONSTRUCTION_RECIPES.md) - Format des recipes de reconstruction
- [ASYNC_SEMANTIC_COMPRESSION_PIPELINE.md](../architecture/ASYNC_SEMANTIC_COMPRESSION_PIPELINE.md) - Pipeline complet
- [GITHUB_ACTIONS_SETUP.md](GITHUB_ACTIONS_SETUP.md) - Configuration CI/CD
- [COLAB_PRO_SETUP.md](COLAB_PRO_SETUP.md) - Worker GPU setup

### Sp√©cifications Formats
- **PNG**: [RFC 2083](https://www.rfc-editor.org/rfc/rfc2083)
- **JPEG**: [ITU T.81](https://www.w3.org/Graphics/JPEG/itu-t81.pdf)
- **MP4**: [ISO/IEC 14496-12](https://www.iso.org/standard/68960.html)
- **WebM**: [WebM Container Guidelines](https://www.webmproject.org/docs/container/)
- **EBML**: [EBML RFC](https://www.rfc-editor.org/rfc/rfc8794.html)

---

**Version**: 0.2.0  
**Derni√®re mise √† jour**: 2025-11-13  
**Auteur**: √âquipe PaniniFS  
**License**: MIT
