"""
Mock GPU pour d√©veloppement local (Solution 3)

Permet de d√©velopper et tester du code GPU sur CPU.
Le m√™me code s'ex√©cute ensuite sur Colab avec un vrai GPU.

Usage:
    # En d√©but de script
    from utils.gpu_mock import setup_device
    
    device = setup_device()  # GPU si dispo, sinon CPU avec mock
    
    # Utiliser normalement
    model = model.to(device)
    data = data.to(device)

Features:
    - D√©tection automatique GPU/CPU
    - Mock transparent de torch.cuda
    - Messages informatifs
    - Compatible avec tout code PyTorch
"""

import sys
from typing import Optional

def setup_device(force_cpu: bool = False, verbose: bool = True):
    """
    Setup device appropri√© (GPU ou CPU avec mock)
    
    Args:
        force_cpu: Forcer CPU m√™me si GPU disponible (pour tests)
        verbose: Afficher messages informatifs
    
    Returns:
        torch.device: Device √† utiliser (cuda ou cpu)
    """
    try:
        import torch
    except ImportError:
        if verbose:
            print("‚ö†Ô∏è  PyTorch non install√©, impossible d'utiliser GPU")
        return None
    
    # Check GPU disponible
    if not force_cpu and torch.cuda.is_available():
        device = torch.device("cuda")
        if verbose:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"‚úÖ GPU d√©tect√©: {gpu_name}")
            print(f"   M√©moire: {gpu_memory:.1f} GB")
            print(f"   Device: {device}")
    else:
        device = torch.device("cpu")
        if verbose:
            if force_cpu:
                print("üîß CPU forc√© pour tests (GPU disponible mais d√©sactiv√©)")
            else:
                print("‚ö†Ô∏è  Aucun GPU d√©tect√©, utilisation du CPU")
                print("   Le code fonctionnera mais sera plus lent")
            print(f"   Device: {device}")
    
    return device


class MockGPU:
    """
    Mock de torch.cuda pour d√©veloppement sans GPU
    
    Simule l'API CUDA avec des op√©rations CPU.
    Transparent pour le code utilisateur.
    """
    
    @staticmethod
    def is_available() -> bool:
        """GPU non disponible (mock)"""
        return False
    
    @staticmethod
    def device_count() -> int:
        """0 GPU disponibles"""
        return 0
    
    @staticmethod
    def get_device_name(device: int = 0) -> str:
        """Nom du device mock√©"""
        return "CPU (Mocked GPU)"
    
    @staticmethod
    def get_device_properties(device: int = 0):
        """Propri√©t√©s mock√©es"""
        class MockProperties:
            name = "CPU (Mocked)"
            total_memory = 0
            major = 0
            minor = 0
        return MockProperties()
    
    @staticmethod
    def current_device() -> int:
        """Device actuel (toujours 0 en mock)"""
        return 0
    
    @staticmethod
    def synchronize():
        """Sync (no-op en CPU)"""
        pass
    
    @staticmethod
    def empty_cache():
        """Clear cache (no-op en CPU)"""
        pass


def apply_gpu_mock(verbose: bool = True):
    """
    Applique le mock GPU sur torch.cuda
    
    Permet d'ex√©cuter du code GPU sur CPU en rempla√ßant
    torch.cuda par notre mock.
    
    Args:
        verbose: Afficher message d'activation
    
    Returns:
        bool: True si mock appliqu√©, False si GPU d√©j√† disponible
    """
    try:
        import torch
    except ImportError:
        if verbose:
            print("‚ö†Ô∏è  PyTorch non install√©, mock GPU impossible")
        return False
    
    if torch.cuda.is_available():
        if verbose:
            print("‚ÑπÔ∏è  GPU disponible, mock non n√©cessaire")
        return False
    
    # Remplacer torch.cuda par notre mock
    torch.cuda = MockGPU
    
    if verbose:
        print("üîß GPU Mock activ√©")
        print("   Code GPU s'ex√©cutera sur CPU")
        print("   Performance r√©duite mais logique identique")
    
    return True


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_device_info() -> dict:
    """
    R√©cup√®re infos sur le device actuel
    
    Returns:
        dict: {
            'has_gpu': bool,
            'device_type': str,
            'device_name': str,
            'device_count': int,
            'memory_gb': float or None
        }
    """
    try:
        import torch
        
        has_gpu = torch.cuda.is_available()
        
        if has_gpu:
            device_name = torch.cuda.get_device_name(0)
            device_count = torch.cuda.device_count()
            memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        else:
            device_name = "CPU"
            device_count = 0
            memory = None
        
        return {
            'has_gpu': has_gpu,
            'device_type': 'cuda' if has_gpu else 'cpu',
            'device_name': device_name,
            'device_count': device_count,
            'memory_gb': memory
        }
    
    except ImportError:
        return {
            'has_gpu': False,
            'device_type': 'none',
            'device_name': 'PyTorch not installed',
            'device_count': 0,
            'memory_gb': None
        }


def print_device_info():
    """Affiche les infos device de fa√ßon lisible"""
    info = get_device_info()
    
    print("=" * 60)
    print("DEVICE INFORMATION")
    print("=" * 60)
    print(f"GPU Available: {'‚úÖ Yes' if info['has_gpu'] else '‚ùå No'}")
    print(f"Device Type: {info['device_type']}")
    print(f"Device Name: {info['device_name']}")
    print(f"Device Count: {info['device_count']}")
    
    if info['memory_gb']:
        print(f"Memory: {info['memory_gb']:.1f} GB")
    
    print("=" * 60)


# ============================================================================
# AUTO-SETUP (optionnel)
# ============================================================================

def auto_setup(force_cpu: bool = False, verbose: bool = True):
    """
    Setup automatique au import
    
    Usage:
        from utils.gpu_mock import auto_setup
        device = auto_setup()  # Configure tout automatiquement
    
    Args:
        force_cpu: Forcer CPU
        verbose: Messages informatifs
    
    Returns:
        torch.device or None
    """
    device = setup_device(force_cpu=force_cpu, verbose=verbose)
    
    if device is not None and device.type == 'cpu':
        # Appliquer mock si CPU
        try:
            import torch
            if not torch.cuda.is_available():
                apply_gpu_mock(verbose=verbose)
        except ImportError:
            pass
    
    return device


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("GPU MOCK - DEMONSTRATION")
    print("=" * 60 + "\n")
    
    # Afficher infos device
    print_device_info()
    
    print("\n--- Test Setup Device ---\n")
    device = setup_device()
    
    if device:
        print(f"\n‚úÖ Device configur√©: {device}")
        
        try:
            import torch
            
            # Test basique
            print("\n--- Test PyTorch Operations ---\n")
            x = torch.randn(3, 3).to(device)
            y = torch.randn(3, 3).to(device)
            z = x @ y
            
            print(f"Matrix multiplication OK")
            print(f"Result shape: {z.shape}")
            print(f"Result device: {z.device}")
            
        except ImportError:
            print("\n‚ö†Ô∏è  PyTorch non install√©, impossible de tester")
    
    else:
        print("\n‚ùå Impossible de configurer device")
    
    print("\n" + "=" * 60)
