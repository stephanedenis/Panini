#!/usr/bin/env python3
"""
Template Analyse de Logs - PaniniFS
Simplification de l'analyse et surveillance des fichiers de logs.
"""

import re
import datetime
from pathlib import Path
from collections import Counter, defaultdict
import logging

# Configuration Panini
WORKSPACE_ROOT = Path(__file__).parent.parent.parent
logger = logging.getLogger(__name__)

def tail_log_file(file_path, lines_count=100):
    """Lit les derni√®res lignes d'un fichier de log."""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            all_lines = f.readlines()
            return all_lines[-lines_count:] if len(all_lines) > lines_count else all_lines
    except (FileNotFoundError, PermissionError) as e:
        logger.error(f"‚ùå Erreur lecture {file_path}: {e}")
        return []

def parse_log_entry(line, log_format='auto'):
    """Parse une ligne de log selon le format."""
    entry = {
        'timestamp': None,
        'level': None,
        'message': line.strip(),
        'raw': line
    }
    
    # Format automatique - d√©tection de patterns communs
    if log_format == 'auto':
        # Pattern timestamp ISO
        timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2})', line)
        if timestamp_match:
            entry['timestamp'] = timestamp_match.group(1)
        
        # Pattern niveau de log
        level_match = re.search(r'\b(DEBUG|INFO|WARNING|WARN|ERROR|CRITICAL|FATAL)\b', line, re.IGNORECASE)
        if level_match:
            entry['level'] = level_match.group(1).upper()
    
    return entry

def filter_log_entries(entries, filters):
    """Filtre les entr√©es selon les crit√®res."""
    filtered = []
    
    for entry in entries:
        include = True
        
        # Filtre par niveau
        if 'level' in filters and filters['level']:
            if entry['level'] != filters['level'].upper():
                include = False
        
        # Filtre par pattern de message
        if 'pattern' in filters and filters['pattern']:
            if not re.search(filters['pattern'], entry['message'], re.IGNORECASE):
                include = False
        
        # Filtre par p√©riode
        if 'after' in filters and filters['after'] and entry['timestamp']:
            # Conversion simplifi√©e - √† adapter selon le format
            if entry['timestamp'] < filters['after']:
                include = False
        
        if include:
            filtered.append(entry)
    
    return filtered

def analyze_log_patterns(entries):
    """Analyse les patterns dans les logs."""
    analysis = {
        'total_entries': len(entries),
        'levels': Counter(),
        'hour_distribution': Counter(),
        'common_messages': Counter(),
        'errors': []
    }
    
    for entry in entries:
        # Comptage par niveau
        if entry['level']:
            analysis['levels'][entry['level']] += 1
        
        # Distribution horaire
        if entry['timestamp']:
            hour_match = re.search(r'(\d{2}):\d{2}:\d{2}', entry['timestamp'])
            if hour_match:
                analysis['hour_distribution'][hour_match.group(1)] += 1
        
        # Messages communs (premiers 50 caract√®res)
        message_key = entry['message'][:50] + "..." if len(entry['message']) > 50 else entry['message']
        analysis['common_messages'][message_key] += 1
        
        # Collecter les erreurs
        if entry['level'] in ['ERROR', 'CRITICAL', 'FATAL']:
            analysis['errors'].append(entry)
    
    return analysis

def display_analysis(analysis):
    """Affiche les r√©sultats d'analyse."""
    logger.info(f"üìä Analyse de {analysis['total_entries']} entr√©es de log")
    
    # Niveaux de log
    if analysis['levels']:
        logger.info("üìã Distribution par niveau:")
        for level, count in analysis['levels'].most_common():
            logger.info(f"  {level}: {count}")
    
    # Distribution horaire
    if analysis['hour_distribution']:
        logger.info("üïê Distribution horaire:")
        for hour in sorted(analysis['hour_distribution'].keys()):
            count = analysis['hour_distribution'][hour]
            logger.info(f"  {hour}h: {count}")
    
    # Messages les plus fr√©quents
    logger.info("üí¨ Messages les plus fr√©quents:")
    for message, count in analysis['common_messages'].most_common(5):
        logger.info(f"  ({count}x) {message}")
    
    # Erreurs r√©centes
    if analysis['errors']:
        logger.info(f"‚ùå Erreurs trouv√©es: {len(analysis['errors'])}")
        for error in analysis['errors'][-5:]:  # Derni√®res 5 erreurs
            logger.info(f"  {error['timestamp']} - {error['message'][:100]}")

def monitor_log_real_time(file_path, callback=None):
    """Surveille un fichier de log en temps r√©el."""
    import time
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # Aller √† la fin du fichier
            f.seek(0, 2)
            
            logger.info(f"üëÅÔ∏è Surveillance de {file_path} (Ctrl+C pour arr√™ter)")
            
            while True:
                line = f.readline()
                if line:
                    entry = parse_log_entry(line)
                    
                    # Affichage ou callback personnalis√©
                    if callback:
                        callback(entry)
                    else:
                        timestamp = entry['timestamp'] or 'NO_TIME'
                        level = entry['level'] or 'INFO'
                        logger.info(f"[{timestamp}] {level}: {entry['message']}")
                else:
                    time.sleep(0.1)
                    
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Surveillance arr√™t√©e")

def main():
    """Fonction principale."""
    try:
        # TODO: Configurer les param√®tres d'analyse
        log_file = "/var/log/syslog"  # Fichier √† analyser
        lines_to_read = 1000  # Nombre de lignes √† lire
        filters = {
            # 'level': 'ERROR',  # Filtrer par niveau
            # 'pattern': 'python',  # Filtrer par pattern
        }
        
        logger.info(f"üìñ Analyse du fichier: {log_file}")
        
        # Lire les derni√®res lignes
        lines = tail_log_file(log_file, lines_to_read)
        if not lines:
            logger.warning("Aucune ligne lue")
            return 1
        
        logger.info(f"üìã {len(lines)} lignes lues")
        
        # Parser les entr√©es
        entries = [parse_log_entry(line) for line in lines]
        
        # Appliquer les filtres
        if filters:
            entries = filter_log_entries(entries, filters)
            logger.info(f"üîç {len(entries)} entr√©es apr√®s filtrage")
        
        # Analyser
        analysis = analyze_log_patterns(entries)
        display_analysis(analysis)
        
        # TODO: Surveillance en temps r√©el si n√©cessaire
        # monitor_log_real_time(log_file)
        
        logger.info("‚úÖ Analyse termin√©e")
        return 0
        
    except KeyboardInterrupt:
        logger.warning("‚èπÔ∏è Interruption utilisateur")
        return 130
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        return 1

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    exit(main())