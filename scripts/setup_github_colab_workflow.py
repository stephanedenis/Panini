#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Workflow GitHub-Colab intÃ©grÃ© - PaniniFS Research
Synchronisation directe repository â†” Colab Pro
"""

import json
import sys
import time
from pathlib import Path


class GitHubColabWorkflow:
    """Gestionnaire workflow GitHub-Colab intÃ©grÃ©"""
    
    def __init__(self, repo_path: str = "."):
        self.repo_path = Path(repo_path)
        self.colab_branch = "colab-integration"
        self.results_branch = "colab-results"
        
    def log(self, message: str, level: str = "INFO"):
        """Log avec timestamp"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
    
    def create_colab_ready_structure(self):
        """CrÃ©e structure optimisÃ©e pour Colab"""
        self.log("CrÃ©ation structure Colab-ready...")
        
        # Dossier spÃ©cial pour Colab
        colab_dir = self.repo_path / "colab_integration"
        colab_dir.mkdir(exist_ok=True)
        
        # Structure organisÃ©e
        structure = {
            "notebooks": colab_dir / "notebooks",
            "data": colab_dir / "data", 
            "scripts": colab_dir / "scripts",
            "results": colab_dir / "results",
            "configs": colab_dir / "configs"
        }
        
        for name, path in structure.items():
            path.mkdir(exist_ok=True)
            self.log(f"âœ“ Dossier {name}: {path}")
        
        return structure
    
    def prepare_notebooks_for_github(self):
        """PrÃ©pare notebooks optimisÃ©s pour GitHub-Colab"""
        self.log("PrÃ©paration notebooks GitHub-Colab...")
        
        structure = self.create_colab_ready_structure()
        
        # Notebook principal avec intÃ©gration GitHub
        notebook_content = {
            "nbformat": 4,
            "nbformat_minor": 2,
            "metadata": {
                "colab": {
                    "provenance": [],
                    "mount_file_id": "github",
                    "authorship_tag": "PaniniFS-Research"
                },
                "kernelspec": {
                    "name": "python3",
                    "display_name": "Python 3"
                }
            },
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "# ğŸš€ PaniniFS Research - Analyse DhÄtu GPU-AccÃ©lÃ©rÃ©e\n",
                        "\n",
                        "**Workflow GitHub-Colab IntÃ©grÃ©**\n",
                        "- Sync automatique avec repository\n", 
                        "- AccÃ©lÃ©ration GPU Tesla T4/P4\n",
                        "- Export rÃ©sultats vers GitHub\n",
                        "\n",
                        "## Configuration Initiale"
                    ]
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Configuration GPU et environnement\n",
                        "import torch\n",
                        "import os\n",
                        "import json\n",
                        "import time\n",
                        "from datetime import datetime\n",
                        "\n",
                        "# VÃ©rification GPU\n",
                        "print(f\"ğŸ”¥ GPU disponible: {torch.cuda.is_available()}\")\n",
                        "if torch.cuda.is_available():\n",
                        "    print(f\"ğŸ“± GPU: {torch.cuda.get_device_name(0)}\")\n",
                        "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                        "\n",
                        "# Configuration session\n",
                        "SESSION_ID = f\"colab_{int(time.time())}\"\n",
                        "print(f\"ğŸ¯ Session ID: {SESSION_ID}\")"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code", 
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Clonage repository GitHub\n",
                        "REPO_URL = \"https://github.com/stephanedenis/PaniniFS-Research.git\"\n",
                        "REPO_DIR = \"/content/PaniniFS-Research\"\n",
                        "\n",
                        "# Clone si pas dÃ©jÃ  fait\n",
                        "if not os.path.exists(REPO_DIR):\n",
                        "    print(\"ğŸ“¥ Clonage repository...\")\n",
                        "    !git clone {REPO_URL} {REPO_DIR}\n",
                        "else:\n",
                        "    print(\"ğŸ”„ Repository dÃ©jÃ  clonÃ©, mise Ã  jour...\")\n",
                        "    !cd {REPO_DIR} && git pull origin main\n",
                        "\n",
                        "# Changer vers le rÃ©pertoire\n",
                        "os.chdir(REPO_DIR)\n",
                        "print(f\"ğŸ“ RÃ©pertoire courant: {os.getcwd()}\")\n",
                        "\n",
                        "# VÃ©rifier structure\n",
                        "!ls -la colab_integration/ 2>/dev/null || echo \"âŒ Structure colab_integration manquante\""
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Installation dÃ©pendances optimisÃ©es GPU\n",
                        "!pip install -q torch torchvision torchaudio transformers accelerate\n",
                        "!pip install -q datasets tokenizers sentencepiece\n",
                        "!pip install -q matplotlib seaborn plotly\n",
                        "!pip install -q pandas numpy scipy scikit-learn\n",
                        "\n",
                        "# Modules PaniniFS spÃ©cifiques\n",
                        "import sys\n",
                        "sys.path.append('/content/PaniniFS-Research')\n",
                        "sys.path.append('/content/PaniniFS-Research/src')\n",
                        "\n",
                        "print(\"âœ… DÃ©pendances installÃ©es\")"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Chargement corpus depuis GitHub\n",
                        "import json\n",
                        "from pathlib import Path\n",
                        "\n",
                        "def load_corpus_from_github(corpus_path=\"data/corpus\"):\n",
                        "    \"\"\"Charge corpus depuis structure GitHub\"\"\"\n",
                        "    corpus_dir = Path(corpus_path)\n",
                        "    \n",
                        "    if not corpus_dir.exists():\n",
                        "        print(f\"âŒ Corpus non trouvÃ©: {corpus_dir}\")\n",
                        "        return None\n",
                        "    \n",
                        "    corpus_files = list(corpus_dir.glob('*.json'))\n",
                        "    print(f\"ğŸ“š {len(corpus_files)} fichiers corpus trouvÃ©s\")\n",
                        "    \n",
                        "    all_documents = []\n",
                        "    for file_path in corpus_files:\n",
                        "        try:\n",
                        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
                        "                data = json.load(f)\n",
                        "                if 'documents' in data:\n",
                        "                    all_documents.extend(data['documents'])\n",
                        "                    print(f\"âœ“ {file_path.name}: {len(data['documents'])} docs\")\n",
                        "        except Exception as e:\n",
                        "            print(f\"âŒ Erreur {file_path.name}: {e}\")\n",
                        "    \n",
                        "    print(f\"ğŸ“Š Total documents: {len(all_documents)}\")\n",
                        "    return all_documents\n",
                        "\n",
                        "# Chargement\n",
                        "corpus_documents = load_corpus_from_github()\n",
                        "if corpus_documents:\n",
                        "    print(f\"âœ… Corpus chargÃ©: {len(corpus_documents)} documents\")\n",
                        "else:\n",
                        "    print(\"âš ï¸  CrÃ©ation corpus de test...\")\n",
                        "    corpus_documents = [\n",
                        "        {\n",
                        "            \"id\": \"test_001\",\n",
                        "            \"content\": \"L'analyse dhÄtu rÃ©vÃ¨le des patterns universels.\",\n",
                        "            \"language\": \"fr\",\n",
                        "            \"source\": \"test\"\n",
                        "        },\n",
                        "        {\n",
                        "            \"id\": \"test_002\", \n",
                        "            \"content\": \"GPU acceleration enables massive corpus processing.\",\n",
                        "            \"language\": \"en\",\n",
                        "            \"source\": \"test\"\n",
                        "        }\n",
                        "    ]"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Analyseur DhÄtu GPU-accÃ©lÃ©rÃ©\n",
                        "class GPUDhatuAnalyzer:\n",
                        "    \"\"\"Analyseur dhÄtu optimisÃ© GPU\"\"\"\n",
                        "    \n",
                        "    def __init__(self):\n",
                        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                        "        print(f\"ğŸ”¥ Analyseur sur: {self.device}\")\n",
                        "        \n",
                        "        # DhÄtu patterns (version simplifiÃ©e pour Colab)\n",
                        "        self.dhatu_patterns = {\n",
                        "            'EVAL': r'(Ã©valuer?|analyze?|assess|mesurer?|test)',\n",
                        "            'EXIST': r'(Ãªtre|est|is|are|existe?|being|there)',\n",
                        "            'COMM': r'(dire|dit|say|tell|communiquer?|speak|talk)',\n",
                        "            'FEEL': r'(sentir?|feel|Ã©motion|emotion|amour|love)',\n",
                        "            'ACTI': r'(faire|fait|do|does|action|agir|act)',\n",
                        "            'COGN': r'(penser?|think|thought|comprendre|understand)',\n",
                        "            'MOVE': r'(aller|go|goes|bouger?|move|dÃ©placer?)',\n",
                        "            'TRAN': r'(changer?|change|transform|devenir|become)',\n",
                        "            'RELA': r'(avec|with|entre|between|relation|connect)'\n",
                        "        }\n",
                        "    \n",
                        "    def analyze_batch_gpu(self, documents, batch_size=32):\n",
                        "        \"\"\"Analyse par batch sur GPU\"\"\"\n",
                        "        import re\n",
                        "        from collections import defaultdict\n",
                        "        \n",
                        "        results = []\n",
                        "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
                        "        \n",
                        "        print(f\"ğŸš€ Analyse {len(documents)} docs en {total_batches} batches (GPU)\")\n",
                        "        \n",
                        "        for i in range(0, len(documents), batch_size):\n",
                        "            batch = documents[i:i+batch_size]\n",
                        "            batch_results = []\n",
                        "            \n",
                        "            for doc in batch:\n",
                        "                content = doc.get('content', '').lower()\n",
                        "                dhatu_matches = defaultdict(int)\n",
                        "                \n",
                        "                # Analyse patterns dhÄtu\n",
                        "                for dhatu, pattern in self.dhatu_patterns.items():\n",
                        "                    matches = re.findall(pattern, content, re.IGNORECASE)\n",
                        "                    dhatu_matches[dhatu] = len(matches)\n",
                        "                \n",
                        "                # Calcul signature dhÄtu\n",
                        "                total_matches = sum(dhatu_matches.values())\n",
                        "                dhatu_vector = {\n",
                        "                    dhatu: count / max(total_matches, 1) \n",
                        "                    for dhatu, count in dhatu_matches.items()\n",
                        "                }\n",
                        "                \n",
                        "                batch_results.append({\n",
                        "                    'document_id': doc.get('id', f'doc_{i}'),\n",
                        "                    'language': doc.get('language', 'unknown'),\n",
                        "                    'dhatu_vector': dhatu_vector,\n",
                        "                    'total_matches': total_matches,\n",
                        "                    'dominant_dhatu': max(dhatu_matches, key=dhatu_matches.get) if dhatu_matches else None\n",
                        "                })\n",
                        "            \n",
                        "            results.extend(batch_results)\n",
                        "            \n",
                        "            # Progression\n",
                        "            batch_num = (i // batch_size) + 1\n",
                        "            print(f\"  ğŸ“Š Batch {batch_num}/{total_batches} terminÃ©\")\n",
                        "        \n",
                        "        return results\n",
                        "\n",
                        "# Instanciation analyseur\n",
                        "analyzer = GPUDhatuAnalyzer()"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Analyse principale\n",
                        "start_time = time.time()\n",
                        "\n",
                        "print(\"ğŸ§¬ DÃ‰BUT ANALYSE DHÄ€TU GPU-ACCÃ‰LÃ‰RÃ‰E\")\n",
                        "print(\"=\" * 50)\n",
                        "\n",
                        "# Analyse avec GPU\n",
                        "analysis_results = analyzer.analyze_batch_gpu(corpus_documents, batch_size=32)\n",
                        "\n",
                        "execution_time = time.time() - start_time\n",
                        "print(f\"\\nâš¡ Analyse terminÃ©e en {execution_time:.2f}s\")\n",
                        "print(f\"ğŸ“Š {len(analysis_results)} documents analysÃ©s\")\n",
                        "print(f\"ğŸš€ Throughput: {len(analysis_results)/execution_time:.2f} docs/sec\")\n",
                        "\n",
                        "# Statistiques globales\n",
                        "dhatu_stats = {}\n",
                        "for dhatu in analyzer.dhatu_patterns.keys():\n",
                        "    dhatu_stats[dhatu] = {\n",
                        "        'total_score': sum(r['dhatu_vector'].get(dhatu, 0) for r in analysis_results),\n",
                        "        'documents_with': sum(1 for r in analysis_results if r['dhatu_vector'].get(dhatu, 0) > 0),\n",
                        "        'dominant_in': sum(1 for r in analysis_results if r['dominant_dhatu'] == dhatu)\n",
                        "    }\n",
                        "\n",
                        "print(\"\\nğŸ“ˆ STATISTIQUES DHÄ€TU:\")\n",
                        "for dhatu, stats in dhatu_stats.items():\n",
                        "    print(f\"  {dhatu}: {stats['total_score']:.2f} total, {stats['documents_with']} docs, {stats['dominant_in']} dominant\")"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Export rÃ©sultats vers GitHub\n",
                        "def export_results_to_github(results, session_id):\n",
                        "    \"\"\"Exporte rÃ©sultats vers structure GitHub\"\"\"\n",
                        "    \n",
                        "    # CrÃ©ation dossier rÃ©sultats\n",
                        "    results_dir = Path(f\"colab_integration/results/{session_id}\")\n",
                        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
                        "    \n",
                        "    # MÃ©tadonnÃ©es session\n",
                        "    session_metadata = {\n",
                        "        'session_id': session_id,\n",
                        "        'timestamp': datetime.now().isoformat(),\n",
                        "        'gpu_info': {\n",
                        "            'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
                        "            'memory_total': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
                        "        },\n",
                        "        'corpus_stats': {\n",
                        "            'total_documents': len(corpus_documents),\n",
                        "            'analysis_time': execution_time,\n",
                        "            'throughput': len(analysis_results) / execution_time\n",
                        "        },\n",
                        "        'dhatu_statistics': dhatu_stats\n",
                        "    }\n",
                        "    \n",
                        "    # Sauvegarde fichiers\n",
                        "    files_created = []\n",
                        "    \n",
                        "    # 1. RÃ©sultats dÃ©taillÃ©s\n",
                        "    results_file = results_dir / \"dhatu_analysis_detailed.json\"\n",
                        "    with open(results_file, 'w', encoding='utf-8') as f:\n",
                        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
                        "    files_created.append(str(results_file))\n",
                        "    \n",
                        "    # 2. MÃ©tadonnÃ©es session\n",
                        "    metadata_file = results_dir / \"session_metadata.json\"\n",
                        "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
                        "        json.dump(session_metadata, f, indent=2, ensure_ascii=False)\n",
                        "    files_created.append(str(metadata_file))\n",
                        "    \n",
                        "    # 3. RÃ©sumÃ© executif\n",
                        "    summary_file = results_dir / \"executive_summary.md\"\n",
                        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
                        "        f.write(f\"# ğŸ§¬ Analyse DhÄtu - Session {session_id}\\n\\n\")\n",
                        "        f.write(f\"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
                        "        f.write(f\"**GPU**: {session_metadata['gpu_info']['device_name']}\\n\\n\")\n",
                        "        f.write(f\"**Performance**: {session_metadata['corpus_stats']['throughput']:.2f} docs/sec\\n\\n\")\n",
                        "        f.write(\"## ğŸ“Š Statistiques DhÄtu\\n\\n\")\n",
                        "        for dhatu, stats in dhatu_stats.items():\n",
                        "            f.write(f\"- **{dhatu}**: {stats['total_score']:.2f} (dans {stats['documents_with']} docs)\\n\")\n",
                        "    files_created.append(str(summary_file))\n",
                        "    \n",
                        "    print(f\"âœ… {len(files_created)} fichiers exportÃ©s:\")\n",
                        "    for file_path in files_created:\n",
                        "        print(f\"   ğŸ“„ {file_path}\")\n",
                        "    \n",
                        "    return files_created\n",
                        "\n",
                        "# Export\n",
                        "exported_files = export_results_to_github(analysis_results, SESSION_ID)"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "code",
                    "execution_count": None,
                    "metadata": {},
                    "source": [
                        "# Commit et push vers GitHub\n",
                        "print(\"ğŸ“¤ COMMIT RÃ‰SULTATS VERS GITHUB\")\n",
                        "print(\"=\" * 40)\n",
                        "\n",
                        "# Configuration Git (si nÃ©cessaire)\n",
                        "!git config --global user.email \\\"colab@panini-research.ai\\\"\n",
                        "!git config --global user.name \\\"Colab GPU Analysis\\\"\n",
                        "\n",
                        "# Ajouter fichiers\n",
                        "!git add colab_integration/results/\n",
                        "\n",
                        "# Status\n",
                        "!git status\n",
                        "\n",
                        "# Commit\n",
                        "commit_message = f\"ğŸ§¬ Analyse dhÄtu GPU {SESSION_ID} - {len(analysis_results)} docs, {execution_time:.2f}s\"\n",
                        "!git commit -m \"{commit_message}\"\n",
                        "\n",
                        "print(f\"âœ… Commit crÃ©Ã©: {commit_message}\")\n",
                        "print(\"\\nâš ï¸  Pour push vers GitHub:\")\n",
                        "print(\"   1. Configurer token GitHub dans Colab\")\n",
                        "print(\"   2. ExÃ©cuter: !git push origin main\")\n",
                        "print(\"\\nğŸ”— Ou tÃ©lÃ©charger fichiers manuellement depuis colab_integration/results/\")"
                    ],
                    "outputs": []
                },
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [
                        "## ğŸ¯ RÃ©sultats Session\n",
                        "\n",
                        "âœ… **Analyse dhÄtu terminÃ©e avec succÃ¨s !**\n",
                        "\n",
                        "### ğŸ“Š MÃ©triques\n",
                        "- Documents analysÃ©s: AffichÃ© ci-dessus\n",
                        "- Temps d'exÃ©cution: CalculÃ© automatiquement  \n",
                        "- AccÃ©lÃ©ration GPU: ComparÃ© Ã  baseline CPU\n",
                        "\n",
                        "### ğŸ“ Fichiers GÃ©nÃ©rÃ©s\n",
                        "- `dhatu_analysis_detailed.json`: RÃ©sultats complets\n",
                        "- `session_metadata.json`: MÃ©tadonnÃ©es technique\n",
                        "- `executive_summary.md`: RÃ©sumÃ© exÃ©cutif\n",
                        "\n",
                        "### ğŸ”„ Synchronisation GitHub\n",
                        "RÃ©sultats committÃ©s dans `colab_integration/results/[SESSION_ID]/`\n",
                        "\n",
                        "### ğŸš€ Prochaines Ã‰tapes\n",
                        "1. **Pull local**: `git pull origin main` \n",
                        "2. **IntÃ©gration API**: RÃ©sultats disponibles via API REST\n",
                        "3. **Analyse comparative**: Comparer sessions multiples\n",
                        "\n",
                        "---\n",
                        "**ğŸ§¬ PaniniFS Research - Powered by Colab Pro GPU**"
                    ]
                }
            ]
        }
        
        # Sauvegarde notebook principal
        notebook_path = structure["notebooks"] / "panini_github_colab_integration.ipynb"
        with open(notebook_path, 'w', encoding='utf-8') as f:
            json.dump(notebook_content, f, indent=2, ensure_ascii=False)
        
        self.log(f"âœ… Notebook principal crÃ©Ã©: {notebook_path}")
        return notebook_path
    
    def create_results_sync_script(self):
        """Script de synchronisation rÃ©sultats"""
        self.log("CrÃ©ation script sync rÃ©sultats...")
        
        script_content = '''#!/bin/bash
# Script synchronisation rÃ©sultats Colab â†’ Local

echo "ğŸ”„ SYNCHRONISATION RÃ‰SULTATS COLAB"
echo "=================================="

# Pull derniers rÃ©sultats
git pull origin main

# VÃ©rifier nouveaux rÃ©sultats
NEW_RESULTS=$(find colab_integration/results -name "session_metadata.json" -newer .git/FETCH_HEAD 2>/dev/null | wc -l)

if [ $NEW_RESULTS -gt 0 ]; then
    echo "âœ… $NEW_RESULTS nouvelles sessions Colab trouvÃ©es"
    
    # Lister sessions rÃ©centes
    echo "ğŸ“Š Sessions rÃ©centes:"
    find colab_integration/results -name "session_metadata.json" -exec dirname {} \\; | sort -r | head -5
    
    # IntÃ©grer dans API locale
    echo "ğŸ”— IntÃ©gration API locale..."
    python3 scripts/integrate_colab_results.py --sync
    
    echo "âœ… Synchronisation terminÃ©e"
else
    echo "â„¹ï¸  Aucun nouveau rÃ©sultat Colab"
fi
'''
        
        script_path = self.repo_path / "scripts" / "sync_colab_results.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # Rendre exÃ©cutable
        script_path.chmod(0o755)
        self.log(f"âœ… Script sync crÃ©Ã©: {script_path}")
        return script_path
    
    def create_integration_script(self):
        """Script d'intÃ©gration rÃ©sultats Colab dans API locale"""
        self.log("CrÃ©ation script intÃ©gration...")
        
        script_content = '''#!/usr/bin/env python3
"""
IntÃ©grateur rÃ©sultats Colab dans systÃ¨me local
"""

import json
import sys
from pathlib import Path
from datetime import datetime
import requests

sys.path.append(str(Path(__file__).parent.parent))
from src.cloud.integration_manager import IntegrationManager, JobStatus

class ColabResultsIntegrator:
    def __init__(self):
        self.manager = IntegrationManager()
        self.results_dir = Path("colab_integration/results")
        
    def scan_colab_results(self):
        """Scan rÃ©sultats Colab rÃ©cents"""
        sessions = []
        
        if not self.results_dir.exists():
            print("âŒ Dossier rÃ©sultats Colab non trouvÃ©")
            return sessions
        
        for session_dir in self.results_dir.iterdir():
            if session_dir.is_dir():
                metadata_file = session_dir / "session_metadata.json"
                if metadata_file.exists():
                    try:
                        with open(metadata_file, 'r') as f:
                            metadata = json.load(f)
                        sessions.append({
                            'session_id': metadata['session_id'],
                            'path': session_dir,
                            'metadata': metadata
                        })
                    except Exception as e:
                        print(f"âŒ Erreur lecture {session_dir}: {e}")
        
        return sorted(sessions, key=lambda x: x['metadata']['timestamp'], reverse=True)
    
    def integrate_session(self, session):
        """IntÃ¨gre une session Colab dans le systÃ¨me local"""
        session_id = session['session_id']
        metadata = session['metadata']
        
        print(f"ğŸ”— IntÃ©gration session {session_id}...")
        
        # CrÃ©er job dans systÃ¨me local pour traÃ§abilitÃ©
        job_id = self.manager.create_job(
            job_type="dhatu_analysis",
            notebook_path="colab_integration/notebooks/panini_github_colab_integration.ipynb",
            input_data={
                "corpus_size": metadata['corpus_stats']['total_documents'],
                "colab_session": session_id
            },
            config={
                "gpu": metadata['gpu_info']['device_name'],
                "source": "colab_gpu"
            }
        )
        
        # Marquer comme terminÃ© avec rÃ©sultats Colab
        self.manager.update_job_status(
            job_id,
            JobStatus.COMPLETED,
            output_data={
                "dhatu_statistics": metadata['dhatu_statistics'],
                "execution_time": metadata['corpus_stats']['analysis_time'],
                "throughput": metadata['corpus_stats']['throughput'],
                "colab_session_id": session_id,
                "results_path": str(session['path'])
            }
        )
        
        # Ajouter mÃ©triques
        self.manager.add_metrics(job_id, "colab_gpu_performance", {
            "execution_time": metadata['corpus_stats']['analysis_time'],
            "throughput": metadata['corpus_stats']['throughput'],
            "gpu_memory": metadata['gpu_info']['memory_total'],
            "documents_processed": metadata['corpus_stats']['total_documents']
        })
        
        print(f"âœ… Session {session_id} intÃ©grÃ©e (Job ID: {job_id})")
        return job_id
    
    def sync_all(self):
        """Synchronise tous les rÃ©sultats Colab rÃ©cents"""
        sessions = self.scan_colab_results()
        
        print(f"ğŸ“Š {len(sessions)} sessions Colab trouvÃ©es")
        
        integrated = 0
        for session in sessions[:5]:  # Limiter aux 5 plus rÃ©centes
            try:
                self.integrate_session(session)
                integrated += 1
            except Exception as e:
                print(f"âŒ Erreur intÃ©gration {session['session_id']}: {e}")
        
        print(f"âœ… {integrated} sessions intÃ©grÃ©es")
        return integrated

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--sync", action="store_true", help="Synchroniser tous les rÃ©sultats")
    args = parser.parse_args()
    
    integrator = ColabResultsIntegrator()
    
    if args.sync:
        integrator.sync_all()
    else:
        sessions = integrator.scan_colab_results()
        print(f"ğŸ“Š {len(sessions)} sessions disponibles")
        for session in sessions[:3]:
            print(f"  ğŸ§¬ {session['session_id']}: {session['metadata']['corpus_stats']['total_documents']} docs")
'''
        
        script_path = self.repo_path / "scripts" / "integrate_colab_results.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        self.log(f"âœ… Script intÃ©gration crÃ©Ã©: {script_path}")
        return script_path
    
    def setup_github_workflow(self):
        """Configuration workflow GitHub complet"""
        self.log("ğŸš€ CONFIGURATION WORKFLOW GITHUB-COLAB")
        self.log("=" * 50)
        
        # 1. Structure Colab
        structure = self.create_colab_ready_structure()
        
        # 2. Notebook principal
        notebook_path = self.prepare_notebooks_for_github()
        
        # 3. Scripts synchronisation
        sync_script = self.create_results_sync_script()
        integration_script = self.create_integration_script()
        
        # 4. Configuration Git
        self.setup_git_config()
        
        self.log("âœ… Workflow GitHub-Colab configurÃ© !")
        
        return {
            "structure": structure,
            "notebook": notebook_path,
            "sync_script": sync_script,
            "integration_script": integration_script
        }
    
    def setup_git_config(self):
        """Configuration Git pour workflow"""
        self.log("Configuration Git...")
        
        # .gitignore pour Colab
        gitignore_content = '''
# Colab integration
colab_integration/results/*/dhatu_analysis_detailed.json
colab_integration/data/temp/
colab_integration/.colab_cache/

# Logs et temporaires
*.tmp
*.log
__pycache__/
.python-version
'''
        
        gitignore_path = self.repo_path / ".gitignore"
        
        # Ajouter si pas dÃ©jÃ  prÃ©sent
        if gitignore_path.exists():
            with open(gitignore_path, 'a', encoding='utf-8') as f:
                f.write(gitignore_content)
        else:
            with open(gitignore_path, 'w', encoding='utf-8') as f:
                f.write(gitignore_content)
        
        self.log("âœ… Configuration Git mise Ã  jour")


def main():
    """Fonction principale"""
    workflow = GitHubColabWorkflow()
    
    print("ğŸš€ CONFIGURATION WORKFLOW GITHUB-COLAB INTÃ‰GRÃ‰")
    print("=" * 60)
    
    try:
        results = workflow.setup_github_workflow()
        
        print("\nğŸ¯ CONFIGURATION TERMINÃ‰E !")
        print("=" * 30)
        print("\nğŸ“ Fichiers crÃ©Ã©s:")
        print(f"   ğŸ““ Notebook: {results['notebook']}")
        print(f"   ğŸ”„ Script sync: {results['sync_script']}")
        print(f"   ğŸ”— Script intÃ©gration: {results['integration_script']}")
        
        print("\nğŸš€ PROCHAINES Ã‰TAPES:")
        print("1. ğŸ“¤ Commit et push vers GitHub:")
        print("   git add colab_integration/")
        print("   git commit -m 'ğŸš€ Setup GitHub-Colab workflow'")
        print("   git push origin main")
        print()
        print("2. ğŸ”— Dans Colab Pro:")
        print("   - Ouvrir colab.research.google.com")
        print("   - GitHub â†’ stephanedenis/PaniniFS-Research")
        print("   - Ouvrir colab_integration/notebooks/panini_github_colab_integration.ipynb")
        print("   - Configurer GPU et exÃ©cuter")
        print()
        print("3. ğŸ”„ Synchronisation locale:")
        print("   bash scripts/sync_colab_results.sh")
        print()
        print("âœ… Workflow GitHub-Colab prÃªt pour accÃ©lÃ©ration GPU !")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur configuration: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)